{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944f8007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA RTX A5000\n",
      "\n",
      "============================================================\n",
      "CIPS-Net PanNuke Evaluation with Panoptic Quality\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import label as scipy_label\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Add CIPS-Net to path\n",
    "sys.path.insert(0, 'CIPS-Net')\n",
    "from models.cips_net import CIPSNet\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CIPS-Net PanNuke Evaluation with Panoptic Quality\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260600de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using experiment: results/cipsnet_pannuke_cv3_balanced_VIT_B_16_distil_bert_uncased_20260103_201916\n",
      "âœ“ Loaded config from results/cipsnet_pannuke_cv3_balanced_VIT_B_16_distil_bert_uncased_20260103_201916/config.yaml\n",
      "\n",
      "Evaluation output: results/cipsnet_pannuke_cv3_balanced_VIT_B_16_distil_bert_uncased_20260103_201916/evaluation\n",
      "Classes: ['Neoplastic', 'Inflammatory', 'Connective_Soft_tissue', 'Dead', 'Epithelial']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Point to your trained experiment directory\n",
    "# Update this path to match your training output\n",
    "EXPERIMENT_DIR = \"results/cipsnet_pannuke_cv3_balanced_VIT_B_16_distil_bert_uncased_20260103_201916\"  # <-- UPDATE THIS\n",
    "\n",
    "# If experiment dir doesn't exist, list available experiments\n",
    "if not os.path.exists(EXPERIMENT_DIR):\n",
    "    print(\"âš ï¸  Experiment directory not found!\")\n",
    "    print(\"\\nAvailable experiments in results/:\")\n",
    "    if os.path.exists(\"results\"):\n",
    "        for exp in sorted(os.listdir(\"results\")):\n",
    "            if exp.startswith(\"cipsnet_pannuke\"):\n",
    "                print(f\"  - results/{exp}\")\n",
    "    print(\"\\nðŸ‘† Update EXPERIMENT_DIR above with one of these paths\")\n",
    "else:\n",
    "    print(f\"âœ“ Using experiment: {EXPERIMENT_DIR}\")\n",
    "\n",
    "# Load config from training\n",
    "config_path = f\"{EXPERIMENT_DIR}/config.yaml\"\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "    print(f\"âœ“ Loaded config from {config_path}\")\n",
    "else:\n",
    "    # Default config if not found\n",
    "    CONFIG = {\n",
    "        'dataset_path': 'PanNuke_Preprocess',\n",
    "        'num_folds': 3,\n",
    "        'img_size': 224,\n",
    "        'class_names': ['Neoplastic', 'Inflammatory', 'Connective_Soft_tissue', 'Dead', 'Epithelial'],\n",
    "        'num_classes': 5,\n",
    "        'img_encoder': 'vit_b_16',\n",
    "        'text_encoder': 'distilbert-base-uncased',\n",
    "        'embed_dim': 768,\n",
    "        'mean': [0.485, 0.456, 0.406],\n",
    "        'std': [0.229, 0.224, 0.225],\n",
    "        'batch_size': 16,\n",
    "    }\n",
    "    print(\"âš ï¸  Using default config (config.yaml not found)\")\n",
    "\n",
    "# Evaluation settings\n",
    "EVAL_CONFIG = {\n",
    "    'batch_size': 8,  # Lower batch size for evaluation\n",
    "    'num_workers': 0,\n",
    "    'iou_threshold': 0.5,  # PQ matching threshold (standard)\n",
    "}\n",
    "\n",
    "# Output directory for evaluation results\n",
    "EVAL_OUTPUT_DIR = f\"{EXPERIMENT_DIR}/evaluation\"\n",
    "os.makedirs(EVAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nEvaluation output: {EVAL_OUTPUT_DIR}\")\n",
    "print(f\"Classes: {CONFIG['class_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae5457",
   "metadata": {},
   "source": [
    "## 1. Dataset and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3cf6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset and transforms defined for evaluation (matching training format)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataset Class for Evaluation (Same as Training)\n",
    "\n",
    "class PanNukeEvalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for PanNuke evaluation - same format as training.\n",
    "    \n",
    "    - Images: PNG files in images/fold{n}/\n",
    "    - Masks: NPZ files in masks/fold{n}/ containing binary masks per class\n",
    "    - Annotations: annotations.csv with image_id, fold, classes_present, instruction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, folds, transform=None, img_size=224, class_names=None):\n",
    "        self.data_root = data_root\n",
    "        self.folds = folds if isinstance(folds, list) else [folds]\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.class_names = class_names or CONFIG['class_names']\n",
    "        self.num_classes = len(self.class_names)\n",
    "        \n",
    "        # Load annotations\n",
    "        annotations = pd.read_csv(os.path.join(data_root, 'annotations.csv'))\n",
    "        self.df = annotations[annotations['fold'].isin(self.folds)].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Loaded {len(self.df)} samples from folds {self.folds}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        fold = row['fold']\n",
    "        \n",
    "        # Load image (same as training)\n",
    "        img_path = os.path.join(self.data_root, 'images', f'fold{fold}', f'{image_id}.png')\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        # Load masks (same as training)\n",
    "        mask_path = os.path.join(self.data_root, 'masks', f'fold{fold}', f'{image_id}.npz')\n",
    "        mask_data = np.load(mask_path)\n",
    "        masks = mask_data['masks']  # [H, W, num_classes] binary masks\n",
    "        \n",
    "        # Create class index mask from binary masks (same as training)\n",
    "        # Priority: later classes override earlier ones if overlapping\n",
    "        class_index_mask = np.zeros((masks.shape[0], masks.shape[1]), dtype=np.int64)\n",
    "        for c in range(self.num_classes):\n",
    "            class_index_mask[masks[:, :, c] > 0] = c\n",
    "        \n",
    "        # Create instance mask from binary masks for PQ computation\n",
    "        # Each connected component gets a unique ID\n",
    "        instance_mask = self._create_instance_mask(masks)\n",
    "        \n",
    "        # Store original semantic mask before transforms (for GT in PQ)\n",
    "        original_semantic = class_index_mask.copy()\n",
    "        original_instance = instance_mask.copy()\n",
    "        \n",
    "        # Apply augmentations (only resize & normalize for eval)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=class_index_mask)\n",
    "            image = augmented['image']\n",
    "            class_index_mask = augmented['mask']\n",
    "        \n",
    "        # Convert to tensors (same as training)\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "        elif isinstance(image, torch.Tensor) and image.ndim == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1).float()\n",
    "        \n",
    "        if isinstance(class_index_mask, np.ndarray):\n",
    "            class_index_mask = torch.from_numpy(class_index_mask.astype(np.int64)).long()\n",
    "        else:\n",
    "            class_index_mask = class_index_mask.long()\n",
    "        \n",
    "        # Get instruction (same as training)\n",
    "        instruction = row['instruction'] if pd.notna(row['instruction']) else \"Segment all tissue types.\"\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': class_index_mask,\n",
    "            'semantic_mask': original_semantic,  # Original resolution for PQ\n",
    "            'instance_mask': original_instance,  # For PQ computation\n",
    "            'binary_masks': masks,  # Original [H,W,C] binary masks\n",
    "            'instruction': instruction,\n",
    "            'image_id': image_id,\n",
    "            'fold': fold\n",
    "        }\n",
    "    \n",
    "    def _create_instance_mask(self, binary_masks):\n",
    "        \"\"\"Create instance mask from binary masks using connected components.\"\"\"\n",
    "        H, W, C = binary_masks.shape\n",
    "        instance_mask = np.zeros((H, W), dtype=np.int32)\n",
    "        instance_id = 1\n",
    "        \n",
    "        for c in range(C):\n",
    "            class_mask = binary_masks[:, :, c].astype(np.uint8)\n",
    "            if class_mask.sum() > 0:\n",
    "                num_labels, labels = cv2.connectedComponents(class_mask, connectivity=8)\n",
    "                for label_id in range(1, num_labels):\n",
    "                    instance_mask[labels == label_id] = instance_id\n",
    "                    instance_id += 1\n",
    "        \n",
    "        return instance_mask\n",
    "\n",
    "\n",
    "def get_eval_transforms(img_size, mean, std):\n",
    "    \"\"\"Get evaluation transforms (no augmentation, same as training val).\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "\n",
    "def eval_collate_fn(batch):\n",
    "    \"\"\"Custom collate function for evaluation.\"\"\"\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    masks = torch.stack([item['mask'] for item in batch])\n",
    "    semantic_masks = [item['semantic_mask'] for item in batch]  # List of numpy arrays\n",
    "    instance_masks = [item['instance_mask'] for item in batch]  # List of numpy arrays\n",
    "    binary_masks = [item['binary_masks'] for item in batch]  # List of numpy arrays\n",
    "    instructions = [item['instruction'] for item in batch]\n",
    "    image_ids = [item['image_id'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'mask': masks,\n",
    "        'semantic_mask': semantic_masks,\n",
    "        'instance_mask': instance_masks,\n",
    "        'binary_masks': binary_masks,\n",
    "        'instruction': instructions,\n",
    "        'image_id': image_ids\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ“ Dataset and transforms defined for evaluation (matching training format)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f37ffbd",
   "metadata": {},
   "source": [
    "## 2. Official PanNuke Panoptic Quality Functions\n",
    "\n",
    "These functions are adapted from the official PanNuke evaluation code (`stats_utils.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44727fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Official PanNuke PQ functions defined:\n",
      "  - remap_label(): Relabel instances to contiguous IDs\n",
      "  - get_fast_pq(): Compute DQ, SQ, PQ with Hungarian matching\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Official PanNuke PQ Functions (from stats_utils.py)\n",
    "\n",
    "def remap_label(pred, by_size=False):\n",
    "    \"\"\"\n",
    "    Rename all instance IDs such that the ID is contiguous (i.e. 1, 2, 3, ...)\n",
    "    \n",
    "    Args:\n",
    "        pred: Input instance map (numpy array)\n",
    "        by_size: If True, relabel instances by their size (largest first)\n",
    "    \n",
    "    Returns:\n",
    "        Remapped instance map\n",
    "    \"\"\"\n",
    "    pred_id = list(np.unique(pred))\n",
    "    if 0 in pred_id:\n",
    "        pred_id.remove(0)\n",
    "    if len(pred_id) == 0:\n",
    "        return pred  # No instances\n",
    "    \n",
    "    if by_size:\n",
    "        pred_size = []\n",
    "        for inst_id in pred_id:\n",
    "            size = (pred == inst_id).sum()\n",
    "            pred_size.append(size)\n",
    "        # Sort by size (descending)\n",
    "        pair_list = zip(pred_id, pred_size)\n",
    "        pair_list = sorted(pair_list, key=lambda x: x[1], reverse=True)\n",
    "        pred_id, _ = zip(*pair_list)\n",
    "    \n",
    "    new_pred = np.zeros(pred.shape, np.int32)\n",
    "    for idx, inst_id in enumerate(pred_id):\n",
    "        new_pred[pred == inst_id] = idx + 1\n",
    "    \n",
    "    return new_pred\n",
    "\n",
    "\n",
    "def get_fast_pq(true, pred, match_iou=0.5):\n",
    "    \"\"\"\n",
    "    Compute Panoptic Quality (PQ) for instance segmentation.\n",
    "    \n",
    "    PQ = DQ Ã— SQ\n",
    "    - DQ (Detection Quality) = TP / (TP + 0.5*FP + 0.5*FN)\n",
    "    - SQ (Segmentation Quality) = Average IoU of matched (TP) pairs\n",
    "    \n",
    "    Uses Hungarian algorithm for optimal 1-to-1 matching between\n",
    "    ground truth and predicted instances.\n",
    "    \n",
    "    Args:\n",
    "        true: Ground truth instance map (H, W) - each unique value is an instance\n",
    "        pred: Predicted instance map (H, W) - each unique value is an instance\n",
    "        match_iou: IoU threshold for considering a match (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        [DQ, SQ, PQ] as numpy array\n",
    "    \"\"\"\n",
    "    assert match_iou >= 0.0, \"match_iou must be >= 0.0\"\n",
    "    \n",
    "    true = np.copy(true)\n",
    "    pred = np.copy(pred)\n",
    "    \n",
    "    # Remap labels to be contiguous\n",
    "    true = remap_label(true)\n",
    "    pred = remap_label(pred)\n",
    "    \n",
    "    # Get unique instance IDs (excluding background 0)\n",
    "    true_id_list = list(np.unique(true))\n",
    "    pred_id_list = list(np.unique(pred))\n",
    "    \n",
    "    if 0 in true_id_list:\n",
    "        true_id_list.remove(0)\n",
    "    if 0 in pred_id_list:\n",
    "        pred_id_list.remove(0)\n",
    "    \n",
    "    # Edge case: no instances\n",
    "    if len(true_id_list) == 0 and len(pred_id_list) == 0:\n",
    "        return np.array([1.0, 1.0, 1.0])  # Perfect score (nothing to predict, nothing predicted)\n",
    "    if len(true_id_list) == 0:\n",
    "        return np.array([0.0, 0.0, 0.0])  # All FP\n",
    "    if len(pred_id_list) == 0:\n",
    "        return np.array([0.0, 0.0, 0.0])  # All FN\n",
    "    \n",
    "    # Compute pairwise IoU matrix\n",
    "    num_true = len(true_id_list)\n",
    "    num_pred = len(pred_id_list)\n",
    "    \n",
    "    # Create IoU matrix\n",
    "    pairwise_iou = np.zeros((num_true, num_pred), dtype=np.float64)\n",
    "    \n",
    "    for t_idx, t_id in enumerate(true_id_list):\n",
    "        true_mask = (true == t_id)\n",
    "        for p_idx, p_id in enumerate(pred_id_list):\n",
    "            pred_mask = (pred == p_id)\n",
    "            \n",
    "            intersection = np.logical_and(true_mask, pred_mask).sum()\n",
    "            union = np.logical_or(true_mask, pred_mask).sum()\n",
    "            \n",
    "            if union > 0:\n",
    "                pairwise_iou[t_idx, p_idx] = intersection / union\n",
    "    \n",
    "    # Hungarian matching to find optimal 1-to-1 assignment\n",
    "    # We want to maximize IoU, but linear_sum_assignment minimizes cost\n",
    "    # So we use negative IoU as cost\n",
    "    \n",
    "    if num_true <= num_pred:\n",
    "        # More predictions than GT - match GT to predictions\n",
    "        cost_matrix = -pairwise_iou\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        matched_iou = pairwise_iou[row_ind, col_ind]\n",
    "    else:\n",
    "        # More GT than predictions - match predictions to GT\n",
    "        cost_matrix = -pairwise_iou.T\n",
    "        col_ind, row_ind = linear_sum_assignment(cost_matrix)\n",
    "        matched_iou = pairwise_iou[row_ind, col_ind]\n",
    "    \n",
    "    # Filter matches by IoU threshold\n",
    "    valid_matches = matched_iou >= match_iou\n",
    "    \n",
    "    # Count TP, FP, FN\n",
    "    tp = valid_matches.sum()\n",
    "    fp = num_pred - tp  # Predictions not matched to GT\n",
    "    fn = num_true - tp  # GT not matched to predictions\n",
    "    \n",
    "    # Compute metrics\n",
    "    # DQ = TP / (TP + 0.5*FP + 0.5*FN)\n",
    "    dq = tp / (tp + 0.5 * fp + 0.5 * fn + 1e-8)\n",
    "    \n",
    "    # SQ = average IoU of TPs\n",
    "    if tp > 0:\n",
    "        sq = matched_iou[valid_matches].sum() / tp\n",
    "    else:\n",
    "        sq = 0.0\n",
    "    \n",
    "    # PQ = DQ Ã— SQ\n",
    "    pq = dq * sq\n",
    "    \n",
    "    return np.array([dq, sq, pq])\n",
    "\n",
    "\n",
    "print(\"âœ“ Official PanNuke PQ functions defined:\")\n",
    "print(\"  - remap_label(): Relabel instances to contiguous IDs\")\n",
    "print(\"  - get_fast_pq(): Compute DQ, SQ, PQ with Hungarian matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8318792",
   "metadata": {},
   "source": [
    "## 3. Instance Extraction from Semantic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd2785ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Instance extraction functions defined:\n",
      "  - semantic_to_instance(): Convert semantic â†’ instance using connected components\n",
      "  - get_class_instance_mask(): Extract instances for a specific class\n",
      "  - compute_pq_metrics(): Compute bPQ, mPQ from predictions\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Instance Extraction from Semantic Segmentation\n",
    "\n",
    "def semantic_to_instance(semantic_mask, num_classes):\n",
    "    \"\"\"\n",
    "    Convert semantic segmentation to instance segmentation using connected components.\n",
    "    \n",
    "    For each class, find connected components and assign unique instance IDs.\n",
    "    \n",
    "    Args:\n",
    "        semantic_mask: (H, W) array with class labels (0 to num_classes-1)\n",
    "        num_classes: Number of classes\n",
    "    \n",
    "    Returns:\n",
    "        instance_mask: (H, W) array with unique instance IDs\n",
    "        class_instance_map: Dict mapping instance_id -> class_id\n",
    "    \"\"\"\n",
    "    H, W = semantic_mask.shape\n",
    "    instance_mask = np.zeros((H, W), dtype=np.int32)\n",
    "    class_instance_map = {}  # instance_id -> class_id\n",
    "    \n",
    "    instance_id = 1\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # Get binary mask for this class\n",
    "        class_mask = (semantic_mask == class_id).astype(np.uint8)\n",
    "        \n",
    "        if class_mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        # Find connected components\n",
    "        num_labels, labels = cv2.connectedComponents(class_mask, connectivity=8)\n",
    "        \n",
    "        # Assign unique instance IDs (skip background label 0)\n",
    "        for label_id in range(1, num_labels):\n",
    "            instance_mask[labels == label_id] = instance_id\n",
    "            class_instance_map[instance_id] = class_id\n",
    "            instance_id += 1\n",
    "    \n",
    "    return instance_mask, class_instance_map\n",
    "\n",
    "\n",
    "def get_class_instance_mask(instance_mask, class_instance_map, target_class):\n",
    "    \"\"\"\n",
    "    Extract instance mask for a specific class.\n",
    "    \n",
    "    Args:\n",
    "        instance_mask: Full instance mask (H, W)\n",
    "        class_instance_map: Dict mapping instance_id -> class_id\n",
    "        target_class: Class ID to extract\n",
    "    \n",
    "    Returns:\n",
    "        Class-specific instance mask (H, W) with relabeled instances\n",
    "    \"\"\"\n",
    "    class_mask = np.zeros_like(instance_mask)\n",
    "    new_id = 1\n",
    "    \n",
    "    for inst_id, class_id in class_instance_map.items():\n",
    "        if class_id == target_class:\n",
    "            class_mask[instance_mask == inst_id] = new_id\n",
    "            new_id += 1\n",
    "    \n",
    "    return class_mask\n",
    "\n",
    "\n",
    "def compute_pq_metrics(pred_semantic, gt_instance, num_classes, class_names, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute bPQ and mPQ from semantic predictions and instance ground truth.\n",
    "    \n",
    "    Args:\n",
    "        pred_semantic: Predicted semantic mask (H, W) with class labels\n",
    "        gt_instance: Ground truth instance mask (H, W) with unique instance IDs\n",
    "        num_classes: Number of classes\n",
    "        class_names: List of class names\n",
    "        iou_threshold: IoU threshold for PQ matching\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with bPQ, mPQ, and per-class PQ values\n",
    "    \"\"\"\n",
    "    # Convert prediction to instances\n",
    "    pred_instance, pred_class_map = semantic_to_instance(pred_semantic, num_classes)\n",
    "    \n",
    "    # Get GT class mapping (we need to know which class each GT instance belongs to)\n",
    "    # Assuming GT instance mask has class info encoded or we derive from overlap\n",
    "    gt_class_map = {}\n",
    "    for inst_id in np.unique(gt_instance):\n",
    "        if inst_id == 0:\n",
    "            continue\n",
    "        # Find most common class in the region (from semantic gt)\n",
    "        # For PanNuke, we can derive from the semantic mask\n",
    "        # Here we assume gt_instance encodes class info already\n",
    "        gt_class_map[inst_id] = 0  # Will be overridden below\n",
    "    \n",
    "    # Per-class PQ computation\n",
    "    pq_per_class = []\n",
    "    dq_per_class = []\n",
    "    sq_per_class = []\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # Get class-specific instance masks\n",
    "        pred_class_inst = get_class_instance_mask(pred_instance, pred_class_map, class_id)\n",
    "        \n",
    "        # For GT, we need to extract instances of this class\n",
    "        # This requires knowing the class of each GT instance\n",
    "        # Simplified: assume we have a way to filter GT by class\n",
    "        gt_class_inst = np.zeros_like(gt_instance)  # Placeholder\n",
    "        \n",
    "        # Compute PQ for this class\n",
    "        if pred_class_inst.max() == 0 and gt_class_inst.max() == 0:\n",
    "            # No instances of this class - skip or count as perfect\n",
    "            pq_per_class.append(1.0)\n",
    "            dq_per_class.append(1.0)\n",
    "            sq_per_class.append(1.0)\n",
    "        else:\n",
    "            dq, sq, pq = get_fast_pq(gt_class_inst, pred_class_inst, match_iou=iou_threshold)\n",
    "            pq_per_class.append(pq)\n",
    "            dq_per_class.append(dq)\n",
    "            sq_per_class.append(sq)\n",
    "    \n",
    "    # Binary PQ (treat all nuclei as one class)\n",
    "    pred_binary = (pred_instance > 0).astype(np.int32)\n",
    "    pred_binary = remap_label(pred_binary)\n",
    "    \n",
    "    gt_binary = (gt_instance > 0).astype(np.int32)\n",
    "    gt_binary = remap_label(gt_binary)\n",
    "    \n",
    "    b_dq, b_sq, b_pq = get_fast_pq(gt_binary, pred_binary, match_iou=iou_threshold)\n",
    "    \n",
    "    # Compile results\n",
    "    metrics = {\n",
    "        'bPQ': b_pq,\n",
    "        'bDQ': b_dq,\n",
    "        'bSQ': b_sq,\n",
    "        'mPQ': np.mean(pq_per_class),\n",
    "        'mDQ': np.mean(dq_per_class),\n",
    "        'mSQ': np.mean(sq_per_class),\n",
    "    }\n",
    "    \n",
    "    for i, name in enumerate(class_names):\n",
    "        metrics[f'pq_{name}'] = pq_per_class[i]\n",
    "        metrics[f'dq_{name}'] = dq_per_class[i]\n",
    "        metrics[f'sq_{name}'] = sq_per_class[i]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"âœ“ Instance extraction functions defined:\")\n",
    "print(\"  - semantic_to_instance(): Convert semantic â†’ instance using connected components\")\n",
    "print(\"  - get_class_instance_mask(): Extract instances for a specific class\")\n",
    "print(\"  - compute_pq_metrics(): Compute bPQ, mPQ from predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec68d2",
   "metadata": {},
   "source": [
    "## 4. Fold Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36187454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fold evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Evaluation Function for a Single Fold\n",
    "\n",
    "def evaluate_fold(fold_idx, test_fold, config, experiment_dir, eval_config):\n",
    "    \"\"\"\n",
    "    Evaluate a single fold: load model, run predictions, compute PQ metrics.\n",
    "    \n",
    "    Args:\n",
    "        fold_idx: Fold index (1, 2, or 3)\n",
    "        test_fold: Which fold to use as test data\n",
    "        config: Training configuration\n",
    "        experiment_dir: Directory containing trained models\n",
    "        eval_config: Evaluation configuration\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with PQ metrics for this fold\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"EVALUATING FOLD {fold_idx}: Testing on fold {test_fold}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load model checkpoint\n",
    "    model_path = f\"{experiment_dir}/fold{fold_idx}/best_model.pth\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"âš ï¸  Model not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    # weights_only=False for PyTorch 2.6+ compatibility (contains numpy arrays)\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    print(f\"âœ“ Loaded model from: {model_path}\")\n",
    "    print(f\"  Best epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Best Dice: {checkpoint.get('best_dice', 'N/A'):.4f}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = CIPSNet(\n",
    "        img_encoder_name=config['img_encoder'],\n",
    "        text_encoder_name=config['text_encoder'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        num_classes=config['num_classes'],\n",
    "        img_size=config['img_size'],\n",
    "        pretrained=False  # We're loading weights\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Create test dataset (matching training format)\n",
    "    test_transform = get_eval_transforms(\n",
    "        config['img_size'], \n",
    "        config['mean'], \n",
    "        config['std']\n",
    "    )\n",
    "    \n",
    "    test_dataset = PanNukeEvalDataset(\n",
    "        data_root=config['dataset_path'],\n",
    "        folds=[test_fold],\n",
    "        transform=test_transform,\n",
    "        img_size=config['img_size'],\n",
    "        class_names=config['class_names']\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=eval_config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=eval_config['num_workers'],\n",
    "        collate_fn=eval_collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Collect predictions and compute PQ\n",
    "    all_pq_binary = []\n",
    "    all_pq_per_class = {name: [] for name in config['class_names']}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(test_loader, desc=f'Evaluating Fold {fold_idx}')\n",
    "        \n",
    "        for batch in pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            gt_instance_list = batch['instance_mask']  # List of numpy arrays\n",
    "            gt_binary_masks_list = batch['binary_masks']  # List of [H,W,C] arrays\n",
    "            instructions = batch['instruction']\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, instructions)\n",
    "            logits = outputs['masks'][:, :config['num_classes'], :, :]\n",
    "            \n",
    "            # Resize predictions to original size if needed\n",
    "            # Note: GT is at original resolution, predictions at model resolution\n",
    "            pred_semantic = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            # Process each sample in batch\n",
    "            batch_size = images.shape[0]\n",
    "            for i in range(batch_size):\n",
    "                pred_sem = pred_semantic[i]  # [H_model, W_model]\n",
    "                gt_inst = gt_instance_list[i]  # [H_orig, W_orig]\n",
    "                gt_binary = gt_binary_masks_list[i]  # [H_orig, W_orig, C]\n",
    "                \n",
    "                # Resize prediction to original GT size for fair comparison\n",
    "                H_orig, W_orig = gt_inst.shape\n",
    "                pred_sem_resized = cv2.resize(\n",
    "                    pred_sem.astype(np.uint8), \n",
    "                    (W_orig, H_orig), \n",
    "                    interpolation=cv2.INTER_NEAREST\n",
    "                )\n",
    "                \n",
    "                # Convert prediction to instance mask\n",
    "                pred_inst, pred_class_map = semantic_to_instance(pred_sem_resized, config['num_classes'])\n",
    "                \n",
    "                # Create GT class map from binary masks\n",
    "                gt_class_map = {}\n",
    "                gt_inst_copy = gt_inst.copy()\n",
    "                instance_id = 1\n",
    "                for c in range(config['num_classes']):\n",
    "                    class_mask = gt_binary[:, :, c].astype(np.uint8)\n",
    "                    if class_mask.sum() > 0:\n",
    "                        num_labels, labels = cv2.connectedComponents(class_mask, connectivity=8)\n",
    "                        for label_id in range(1, num_labels):\n",
    "                            gt_class_map[instance_id] = c\n",
    "                            instance_id += 1\n",
    "                \n",
    "                # Binary PQ (all nuclei as one class)\n",
    "                dq, sq, pq = get_fast_pq(gt_inst, pred_inst, match_iou=eval_config['iou_threshold'])\n",
    "                all_pq_binary.append(pq)\n",
    "                \n",
    "                # Per-class PQ\n",
    "                for class_id, class_name in enumerate(config['class_names']):\n",
    "                    pred_class_inst = get_class_instance_mask(pred_inst, pred_class_map, class_id)\n",
    "                    gt_class_inst = get_class_instance_mask(gt_inst, gt_class_map, class_id)\n",
    "                    \n",
    "                    # Only compute if at least one has instances\n",
    "                    if pred_class_inst.max() > 0 or gt_class_inst.max() > 0:\n",
    "                        dq_c, sq_c, pq_c = get_fast_pq(gt_class_inst, pred_class_inst, \n",
    "                                                        match_iou=eval_config['iou_threshold'])\n",
    "                        all_pq_per_class[class_name].append(pq_c)\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    metrics = {\n",
    "        'fold': fold_idx,\n",
    "        'test_fold': test_fold,\n",
    "        'bPQ': np.mean(all_pq_binary) if all_pq_binary else 0.0,\n",
    "        'bPQ_std': np.std(all_pq_binary) if all_pq_binary else 0.0,\n",
    "    }\n",
    "    \n",
    "    # Per-class PQ\n",
    "    pq_per_class_mean = []\n",
    "    for class_name in config['class_names']:\n",
    "        values = all_pq_per_class[class_name]\n",
    "        if values:\n",
    "            metrics[f'pq_{class_name}'] = np.mean(values)\n",
    "            metrics[f'pq_{class_name}_std'] = np.std(values)\n",
    "            pq_per_class_mean.append(np.mean(values))\n",
    "        else:\n",
    "            metrics[f'pq_{class_name}'] = 0.0\n",
    "            metrics[f'pq_{class_name}_std'] = 0.0\n",
    "            pq_per_class_mean.append(0.0)\n",
    "    \n",
    "    # mPQ is average across classes\n",
    "    metrics['mPQ'] = np.mean(pq_per_class_mean)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nðŸ“Š Fold {fold_idx} Results:\")\n",
    "    print(f\"  bPQ (Binary PQ): {metrics['bPQ']:.4f} Â± {metrics['bPQ_std']:.4f}\")\n",
    "    print(f\"  mPQ (Multi-class PQ): {metrics['mPQ']:.4f}\")\n",
    "    print(\"  Per-class PQ:\")\n",
    "    for class_name in config['class_names']:\n",
    "        print(f\"    {class_name}: {metrics[f'pq_{class_name}']:.4f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"âœ“ Fold evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deef97da",
   "metadata": {},
   "source": [
    "## 5. Run 3-Fold Cross-Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2bec2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING 3-FOLD PANOPTIC QUALITY EVALUATION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "EVALUATING FOLD 1: Testing on fold 1\n",
      "======================================================================\n",
      "âœ“ Loaded model from: results/cipsnet_pannuke_cv3_balanced_VIT_B_16_distil_bert_uncased_20260103_201916/fold1/best_model.pth\n",
      "  Best epoch: 47\n",
      "  Best Dice: 0.6201\n",
      "Loaded 2656 samples from folds [1]\n",
      "Test samples: 2656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Fold 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332/332 [04:02<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Fold 1 Results:\n",
      "  bPQ (Binary PQ): 0.1503 Â± 0.1613\n",
      "  mPQ (Multi-class PQ): 0.1392\n",
      "  Per-class PQ:\n",
      "    Neoplastic: 0.0000\n",
      "    Inflammatory: 0.2729\n",
      "    Connective_Soft_tissue: 0.1483\n",
      "    Dead: 0.1418\n",
      "    Epithelial: 0.1330\n",
      "\n",
      "======================================================================\n",
      "EVALUATING FOLD 2: Testing on fold 2\n",
      "======================================================================\n",
      "âœ“ Loaded model from: results/cipsnet_pannuke_cv3_balanced_VIT_B_16_distil_bert_uncased_20260103_201916/fold2/best_model.pth\n",
      "  Best epoch: 49\n",
      "  Best Dice: 0.6070\n",
      "Loaded 2523 samples from folds [2]\n",
      "Test samples: 2523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Fold 2:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 294/316 [02:45<00:12,  1.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m all_fold_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m cv_splits:\n\u001b[0;32m---> 23\u001b[0m     fold_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfold_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_fold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_fold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEXPERIMENT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVAL_CONFIG\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fold_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m         all_fold_results\u001b[38;5;241m.\u001b[39mappend(fold_metrics)\n",
      "Cell \u001b[0;32mIn[13], line 124\u001b[0m, in \u001b[0;36mevaluate_fold\u001b[0;34m(fold_idx, test_fold, config, experiment_dir, eval_config)\u001b[0m\n\u001b[1;32m    121\u001b[0m             instance_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Binary PQ (all nuclei as one class)\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m dq, sq, pq \u001b[38;5;241m=\u001b[39m \u001b[43mget_fast_pq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_inst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_inst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_iou\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miou_threshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m all_pq_binary\u001b[38;5;241m.\u001b[39mappend(pq)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Per-class PQ\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 94\u001b[0m, in \u001b[0;36mget_fast_pq\u001b[0;34m(true, pred, match_iou)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p_idx, p_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pred_id_list):\n\u001b[1;32m     92\u001b[0m     pred_mask \u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m==\u001b[39m p_id)\n\u001b[0;32m---> 94\u001b[0m     intersection \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_and\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     95\u001b[0m     union \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(true_mask, pred_mask)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m union \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 7: Run 3-Fold Cross-Validation Evaluation\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING 3-FOLD PANOPTIC QUALITY EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Cross-validation splits (same as training)\n",
    "cv_splits = [\n",
    "    {'fold_idx': 1, 'test_fold': 1},  # Model trained on [2,3], test on 1\n",
    "    {'fold_idx': 2, 'test_fold': 2},  # Model trained on [1,3], test on 2\n",
    "    {'fold_idx': 3, 'test_fold': 3},  # Model trained on [1,2], test on 3\n",
    "]\n",
    "\n",
    "# Check if experiment directory exists\n",
    "if not os.path.exists(EXPERIMENT_DIR):\n",
    "    print(f\"\\nâŒ ERROR: Experiment directory not found: {EXPERIMENT_DIR}\")\n",
    "    print(\"Please update EXPERIMENT_DIR in Cell 2 to point to your training results.\")\n",
    "else:\n",
    "    # Store results for all folds\n",
    "    all_fold_results = []\n",
    "    \n",
    "    for split in cv_splits:\n",
    "        fold_metrics = evaluate_fold(\n",
    "            fold_idx=split['fold_idx'],\n",
    "            test_fold=split['test_fold'],\n",
    "            config=CONFIG,\n",
    "            experiment_dir=EXPERIMENT_DIR,\n",
    "            eval_config=EVAL_CONFIG\n",
    "        )\n",
    "        \n",
    "        if fold_metrics is not None:\n",
    "            all_fold_results.append(fold_metrics)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"3-FOLD EVALUATION COMPLETE!\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6257bf",
   "metadata": {},
   "source": [
    "## 6. Aggregate Results and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Aggregate Results and Save\n",
    "\n",
    "if len(all_fold_results) > 0:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PANOPTIC QUALITY SUMMARY (3-Fold Cross-Validation)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(all_fold_results)\n",
    "    \n",
    "    print(\"\\nPer-Fold Results:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"AGGREGATED RESULTS (Mean Â± Std across 3 folds):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Binary PQ\n",
    "    bpq_mean = results_df['bPQ'].mean()\n",
    "    bpq_std = results_df['bPQ'].std()\n",
    "    print(f\"\\n  bPQ (Binary Panoptic Quality): {bpq_mean:.4f} Â± {bpq_std:.4f}\")\n",
    "    \n",
    "    # Multi-class PQ\n",
    "    mpq_mean = results_df['mPQ'].mean()\n",
    "    mpq_std = results_df['mPQ'].std()\n",
    "    print(f\"  mPQ (Multi-class Panoptic Quality): {mpq_mean:.4f} Â± {mpq_std:.4f}\")\n",
    "    \n",
    "    # Per-class PQ\n",
    "    print(\"\\n  Per-Class PQ (Mean Â± Std):\")\n",
    "    for class_name in CONFIG['class_names']:\n",
    "        col = f'pq_{class_name}'\n",
    "        if col in results_df.columns:\n",
    "            mean_val = results_df[col].mean()\n",
    "            std_val = results_df[col].std()\n",
    "            print(f\"    {class_name}: {mean_val:.4f} Â± {std_val:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(f\"{EVAL_OUTPUT_DIR}/pq_per_fold.csv\", index=False)\n",
    "    \n",
    "    # Save aggregated summary\n",
    "    summary = {\n",
    "        'bPQ_mean': float(bpq_mean),\n",
    "        'bPQ_std': float(bpq_std),\n",
    "        'mPQ_mean': float(mpq_mean),\n",
    "        'mPQ_std': float(mpq_std),\n",
    "    }\n",
    "    for class_name in CONFIG['class_names']:\n",
    "        col = f'pq_{class_name}'\n",
    "        if col in results_df.columns:\n",
    "            summary[f'{col}_mean'] = float(results_df[col].mean())\n",
    "            summary[f'{col}_std'] = float(results_df[col].std())\n",
    "    \n",
    "    with open(f\"{EVAL_OUTPUT_DIR}/pq_summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ“ Results saved to: {EVAL_OUTPUT_DIR}\")\n",
    "    print(f\"  - pq_per_fold.csv: Per-fold PQ metrics\")\n",
    "    print(f\"  - pq_summary.json: Aggregated summary\")\n",
    "else:\n",
    "    print(\"âš ï¸  No evaluation results to aggregate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3678b",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualization of PQ Results\n",
    "\n",
    "if len(all_fold_results) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: bPQ and mPQ per fold\n",
    "    ax = axes[0]\n",
    "    folds = [r['fold'] for r in all_fold_results]\n",
    "    bpq_values = [r['bPQ'] for r in all_fold_results]\n",
    "    mpq_values = [r['mPQ'] for r in all_fold_results]\n",
    "    \n",
    "    x = np.arange(len(folds))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, bpq_values, width, label='bPQ', color='#1f77b4', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, mpq_values, width, label='mPQ', color='#ff7f0e', alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, bpq_values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    for bar, val in zip(bars2, mpq_values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Fold')\n",
    "    ax.set_ylabel('Panoptic Quality')\n",
    "    ax.set_title('bPQ and mPQ per Fold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'Fold {f}' for f in folds])\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Per-class PQ (averaged across folds)\n",
    "    ax = axes[1]\n",
    "    class_names = CONFIG['class_names']\n",
    "    pq_means = []\n",
    "    pq_stds = []\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        values = [r[f'pq_{class_name}'] for r in all_fold_results]\n",
    "        pq_means.append(np.mean(values))\n",
    "        pq_stds.append(np.std(values))\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    bars = ax.bar(x, pq_means, yerr=pq_stds, capsize=5, color='#2ca02c', alpha=0.8)\n",
    "    \n",
    "    for bar, mean in zip(bars, pq_means):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{mean:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Panoptic Quality')\n",
    "    ax.set_title('Per-Class PQ (Mean Â± Std across folds)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{EVAL_OUTPUT_DIR}/pq_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ“ Visualization saved to: {EVAL_OUTPUT_DIR}/pq_visualization.png\")\n",
    "else:\n",
    "    print(\"âš ï¸  No results to visualize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Final Summary Table (Paper-Ready Format)\n",
    "\n",
    "if len(all_fold_results) > 0:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL PANOPTIC QUALITY RESULTS (Paper-Ready Format)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_rows = []\n",
    "    \n",
    "    # bPQ\n",
    "    bpq_mean = results_df['bPQ'].mean()\n",
    "    bpq_std = results_df['bPQ'].std()\n",
    "    summary_rows.append({\n",
    "        'Metric': 'bPQ (Binary)',\n",
    "        'Mean': f'{bpq_mean:.4f}',\n",
    "        'Std': f'{bpq_std:.4f}',\n",
    "        'Result': f'{bpq_mean:.4f} Â± {bpq_std:.4f}'\n",
    "    })\n",
    "    \n",
    "    # mPQ\n",
    "    mpq_mean = results_df['mPQ'].mean()\n",
    "    mpq_std = results_df['mPQ'].std()\n",
    "    summary_rows.append({\n",
    "        'Metric': 'mPQ (Multi-class)',\n",
    "        'Mean': f'{mpq_mean:.4f}',\n",
    "        'Std': f'{mpq_std:.4f}',\n",
    "        'Result': f'{mpq_mean:.4f} Â± {mpq_std:.4f}'\n",
    "    })\n",
    "    \n",
    "    # Per-class PQ\n",
    "    for class_name in CONFIG['class_names']:\n",
    "        col = f'pq_{class_name}'\n",
    "        if col in results_df.columns:\n",
    "            mean_val = results_df[col].mean()\n",
    "            std_val = results_df[col].std()\n",
    "            summary_rows.append({\n",
    "                'Metric': f'PQ ({class_name})',\n",
    "                'Mean': f'{mean_val:.4f}',\n",
    "                'Std': f'{std_val:.4f}',\n",
    "                'Result': f'{mean_val:.4f} Â± {std_val:.4f}'\n",
    "            })\n",
    "    \n",
    "    summary_table = pd.DataFrame(summary_rows)\n",
    "    print(\"\\n\" + summary_table.to_string(index=False))\n",
    "    \n",
    "    # Save final summary\n",
    "    summary_table.to_csv(f\"{EVAL_OUTPUT_DIR}/final_pq_results.csv\", index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPARISON WITH PANNUKE PAPER BASELINES\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nReference values from PanNuke paper (different methods):\")\n",
    "    print(\"  - Micro-Net: bPQ=0.3866, mPQ=0.2291\")\n",
    "    print(\"  - DIST:      bPQ=0.4108, mPQ=0.2464\")\n",
    "    print(\"  - Mask-RCNN: bPQ=0.4011, mPQ=0.2484\")\n",
    "    print(\"  - HoVer-Net: bPQ=0.4724, mPQ=0.2958\")\n",
    "    print(f\"\\n  Your CIPS-Net: bPQ={bpq_mean:.4f}, mPQ={mpq_mean:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"All evaluation results saved to: {EVAL_OUTPUT_DIR}\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"âš ï¸  No results available for summary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
