{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b8fd42",
   "metadata": {},
   "source": [
    "# CIPS-Net Training with Permutation-Based Dataset\n",
    "## Language-Conditional Compositional Pathology Segmentation\n",
    "\n",
    "This notebook trains CIPS-Net with the **permutations-based dataset** where:\n",
    "- Each image has **2^N - 1** instances (N = number of classes in that image)\n",
    "- Each instance has a **different text query** asking for specific class(es)\n",
    "- The model learns to segment **ONLY** the classes mentioned in the query\n",
    "- This enables true **text-conditional compositional segmentation**\n",
    "\n",
    "### Key Difference from Unique Labels:\n",
    "- **Unique**: \"Segment Neoplastic and Inflammatory\" â†’ ALL present masks loaded\n",
    "- **Permutations**: Same image can have:\n",
    "  - \"Segment Neoplastic\" â†’ ONLY Neoplastic mask, others are ZERO\n",
    "  - \"Segment Inflammatory\" â†’ ONLY Inflammatory mask, others are ZERO  \n",
    "  - \"Segment Neoplastic and Inflammatory\" â†’ Both masks loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa5c6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Base imports loaded!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Path Setup\n",
    "import sys\n",
    "sys.path.append('CIPS-Net')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.cips_net import CIPSNet\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "print(\"âœ“ Base imports loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d9e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CIPS-Net modules reloaded!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Reload CIPS-Net modules\n",
    "import importlib\n",
    "\n",
    "# Remove cached modules to pick up any changes\n",
    "modules_to_reload = [\n",
    "    'CIPS-Net.models.image_encoder',\n",
    "    'CIPS-Net.models.text_encoder',\n",
    "    'CIPS-Net.models.instruction_grounding',\n",
    "    'CIPS-Net.models.decoder',\n",
    "    'CIPS-Net.models.cips_net',\n",
    "    'CIPS-Net.models'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "\n",
    "# Re-import\n",
    "from models.cips_net import CIPSNet\n",
    "print(\"âœ“ CIPS-Net modules reloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e8e392",
   "metadata": {},
   "source": [
    "## 1. Initialize CIPS-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c640756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Initializing CIPS-Net for Language-Conditional Compositional Segmentation...\n",
      "ðŸ“¦ ViT-B/16 + DistilBERT\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize CIPS-Net Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitializing CIPS-Net for Language-Conditional Compositional Segmentation...\")\n",
    "print(\"ðŸ“¦ ViT-B/16 + DistilBERT\")\n",
    "\n",
    "model = CIPSNet(\n",
    "    img_encoder_name='vit_b_16',\n",
    "    text_encoder_name=\"distilbert-base-uncased\",\n",
    "    embed_dim=768,\n",
    "    num_classes=5,\n",
    "    img_size=224,\n",
    "    num_graph_layers=3,\n",
    "    decoder_channels=[512, 256, 128, 64],\n",
    "    freeze_text_encoder=False,\n",
    "    pretrained=False\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0f615",
   "metadata": {},
   "source": [
    "## 2. Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb9e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CIPS-NET MODEL INFORMATION\n",
      "================================================================================\n",
      "model_name: CIPS-Net\n",
      "image_encoder: vit_b_16\n",
      "text_encoder: distilbert-base-uncased\n",
      "embed_dim: 768\n",
      "num_classes: 5\n",
      "img_size: 224\n",
      "total_parameters: 120,601,349\n",
      "trainable_parameters: 120,601,349\n",
      "\n",
      "class_names:\n",
      "  0. Neoplastic\n",
      "  1. Inflammatory\n",
      "  2. Connective_Soft_tissue\n",
      "  3. Epithelial\n",
      "  4. Dead\n",
      "  5. Background\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Model Information\n",
    "model_info = model.get_model_info()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CIPS-NET MODEL INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for key, value in model_info.items():\n",
    "    if key == 'class_names':\n",
    "        print(f\"\\n{key}:\")\n",
    "        for i, name in enumerate(value):\n",
    "            print(f\"  {i}. {name}\")\n",
    "    elif key in ['total_parameters', 'trainable_parameters']:\n",
    "        print(f\"{key}: {value:,}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c203740",
   "metadata": {},
   "source": [
    "## 3. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62485a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward pass with compositional queries...\n",
      "\n",
      "Input shape: torch.Size([2, 3, 224, 224])\n",
      "Output shape: torch.Size([2, 5, 224, 224])\n",
      "Expected: [B, 5, 224, 224] = [2, 5, 224, 224]\n",
      "\n",
      "âœ“ Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Test Forward Pass\n",
    "print(\"Testing forward pass with compositional queries...\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Test with different compositional queries\n",
    "test_queries = [\n",
    "    \"Segment Neoplastic tissue\",\n",
    "    \"Segment Inflammatory and Connective_Soft_tissue\",\n",
    "    \"Segment Neoplastic and Inflammatory and Dead\",\n",
    "]\n",
    "\n",
    "batch_size = 2\n",
    "test_images = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "test_instructions = test_queries[:batch_size]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_images, test_instructions)\n",
    "    pred_masks = outputs['masks']\n",
    "\n",
    "print(f\"\\nInput shape: {test_images.shape}\")\n",
    "print(f\"Output shape: {pred_masks.shape}\")\n",
    "print(f\"Expected: [B, 5, 224, 224] = [{batch_size}, 5, 224, 224]\")\n",
    "print(\"\\nâœ“ Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ed459",
   "metadata": {},
   "source": [
    "## 4. Load Permutation-Based Dataset\n",
    "\n",
    "**Key Change**: Loading `Images_With_Permutations_Labels_Refer_Segmentation_Task.csv` instead of unique labels.\n",
    "\n",
    "Each image now has **2^N - 1 instances** where N is the number of classes in that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training libraries imported!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Training Libraries\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Training libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb9a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 98 total instances (image + text query combinations)\n",
      "\n",
      "Columns: ['image_id', 'image_path', 'classes', 'organ', 'classes_clean', 'instruction']\n",
      "\n",
      "Unique images: 14\n",
      "Average instances per image: 7.0\n",
      "\n",
      "ðŸ“Š Class Query Distribution (how often each class is QUERIED):\n",
      "  Neoplastic: 56 queries (57.1%)\n",
      "  Inflammatory: 56 queries (57.1%)\n",
      "  Connective_Soft_tissue: 56 queries (57.1%)\n",
      "  Epithelial: 0 queries (0.0%)\n",
      "  Dead: 0 queries (0.0%)\n",
      "\n",
      "ðŸ“ Sample instances (same image, different queries):\n",
      "Image: 1_Breast_fold_1_0000_img.png\n",
      "Number of query instances: 7\n",
      "  1. Classes: Connective_Soft_tissue\n",
      "     Instruction: This Breast falls into Connective_Soft_tissue....\n",
      "  2. Classes: Inflammatory\n",
      "     Instruction: The Breast histopathology reveals Inflammatory....\n",
      "  3. Classes: Neoplastic\n",
      "     Instruction: Here we have a Breast with Neoplastic....\n",
      "  4. Classes: Connective_Soft_tissue;Inflammatory\n",
      "     Instruction: Outline pathological margins of Connective_Soft_tissue and Inflammatory within t...\n",
      "  5. Classes: Connective_Soft_tissue;Neoplastic\n",
      "     Instruction: Trace borders of the lesion within the Breast....\n",
      "  6. Classes: Inflammatory;Neoplastic\n",
      "     Instruction: Define exact site of Breast tissue degeneration from Inflammatory and Neoplastic...\n",
      "  7. Classes: Connective_Soft_tissue;Inflammatory;Neoplastic\n",
      "     Instruction: The Breast slide is labeled with Connective_Soft_tissue and Inflammatory and Neo...\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load Permutation-Based Dataset\n",
    "# ============================================\n",
    "# KEY CHANGE: Using permutations CSV instead of unique labels\n",
    "# ============================================\n",
    "\n",
    "df = pd.read_csv('Dataset/Images_With_Permutations_Labels_Refer_Segmentation_Task_Small.csv')\n",
    "print(f\"Dataset loaded: {len(df)} total instances (image + text query combinations)\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "# Count unique images\n",
    "unique_images = df['image_path'].nunique()\n",
    "print(f\"\\nUnique images: {unique_images}\")\n",
    "print(f\"Average instances per image: {len(df) / unique_images:.1f}\")\n",
    "\n",
    "# Class names\n",
    "class_names = ['Neoplastic', 'Inflammatory', 'Connective_Soft_tissue', 'Epithelial', 'Dead']\n",
    "CLASS_NAMES = class_names\n",
    "# Parse classes column (semicolon-separated) into binary columns\n",
    "# NOTE: This is for QUERIED classes in each instance, not ALL classes in image\n",
    "for class_name in class_names:\n",
    "    df[class_name] = df['classes'].apply(lambda x: 1 if class_name in str(x).split(';') else 0)\n",
    "\n",
    "# Add base_name for mask path construction\n",
    "df['base_name'] = df['image_path'].str.replace('_img.png', '', regex=False)\n",
    "\n",
    "# Show class query distribution\n",
    "print(f\"\\nðŸ“Š Class Query Distribution (how often each class is QUERIED):\")\n",
    "for col in class_names:\n",
    "    count = df[col].sum()\n",
    "    print(f\"  {col}: {count} queries ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show sample rows\n",
    "print(f\"\\nðŸ“ Sample instances (same image, different queries):\")\n",
    "sample_image = df['image_path'].iloc[0]\n",
    "sample_df = df[df['image_path'] == sample_image]\n",
    "print(f\"Image: {sample_image}\")\n",
    "print(f\"Number of query instances: {len(sample_df)}\")\n",
    "for i, (_, row) in enumerate(sample_df.iterrows()):\n",
    "    print(f\"  {i+1}. Classes: {row['classes']}\")\n",
    "    print(f\"     Instruction: {row['instruction'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7309d969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ HistopathologyPermutationDataset class defined!\n",
      "  Key feature: Masks are loaded ONLY for queried classes\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Permutation-Aware Dataset Class\n",
    "# ============================================\n",
    "# KEY CHANGE: Load masks ONLY for classes mentioned in the query\n",
    "# Other class masks are forced to be ZERO (even if mask file exists)\n",
    "# ============================================\n",
    "\n",
    "class HistopathologyPermutationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Language-Conditional Compositional Segmentation.\n",
    "    \n",
    "    Each instance has:\n",
    "    - An image\n",
    "    - A text query asking for specific class(es)\n",
    "    - Masks ONLY for the queried classes (other channels are ZERO)\n",
    "    \n",
    "    This forces the model to learn text-conditional segmentation:\n",
    "    - Same image with \"Segment Neoplastic\" -> only channel 0 has mask\n",
    "    - Same image with \"Segment Inflammatory\" -> only channel 1 has mask\n",
    "    - Same image with \"Segment Neoplastic and Inflammatory\" -> channels 0,1 have masks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, img_dir='Dataset/multi_images', mask_dir='Dataset/multi_masks', \n",
    "                 transform=None, img_size=224):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        self.class_names = ['Neoplastic', 'Inflammatory', 'Connective_Soft_tissue', 'Epithelial', 'Dead']\n",
    "        self.channel_mapping = {\n",
    "            'Neoplastic': 0,\n",
    "            'Inflammatory': 1,\n",
    "            'Connective_Soft_tissue': 2,\n",
    "            'Dead': 3,\n",
    "            'Epithelial': 4\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, row['image_path'])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # ============================================\n",
    "        # KEY CHANGE: Parse QUERIED classes from semicolon-separated string\n",
    "        # ============================================\n",
    "        queried_classes = set(str(row['classes']).split(';'))\n",
    "        \n",
    "        # Load masks ONLY for queried classes\n",
    "        base_name = row['base_name']\n",
    "        masks = []\n",
    "        \n",
    "        for class_name in self.class_names:\n",
    "            channel_idx = self.channel_mapping[class_name]\n",
    "            \n",
    "            # ============================================\n",
    "            # KEY CHANGE: Only load mask if class is in the query\n",
    "            # Otherwise, use empty mask (zeros)\n",
    "            # ============================================\n",
    "            if class_name in queried_classes:\n",
    "                # This class is queried - load the actual mask\n",
    "                mask_filename = f\"{base_name}_channel_{channel_idx}_{class_name}.png\"\n",
    "                mask_path = os.path.join(self.mask_dir, mask_filename)\n",
    "                \n",
    "                if os.path.exists(mask_path):\n",
    "                    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    mask = (mask > 127).astype(np.uint8)  # Binarize\n",
    "                else:\n",
    "                    # Mask file doesn't exist (rare case)\n",
    "                    mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "            else:\n",
    "                # This class is NOT queried - FORCE empty mask\n",
    "                # Even if the mask file exists, we don't load it\n",
    "                mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "            \n",
    "            masks.append(mask)\n",
    "        \n",
    "        # Stack masks: [H, W, num_classes]\n",
    "        masks = np.stack(masks, axis=-1)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=masks)\n",
    "            image = augmented['image']\n",
    "            masks = augmented['mask']\n",
    "        \n",
    "        # Convert to tensors\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "        elif isinstance(image, torch.Tensor) and image.ndim == 3 and image.shape[-1] == 3:\n",
    "            image = image.permute(2, 0, 1).float()\n",
    "        \n",
    "        if isinstance(masks, np.ndarray):\n",
    "            masks = torch.from_numpy(masks).permute(2, 0, 1).float()\n",
    "        elif isinstance(masks, torch.Tensor) and masks.ndim == 3 and masks.shape[-1] == len(self.class_names):\n",
    "            masks = masks.permute(2, 0, 1).float()\n",
    "        \n",
    "        # Get instruction from the 'instruction' column\n",
    "        instruction = row['instruction']\n",
    "        \n",
    "        # Get class presence (which classes are QUERIED in this instance)\n",
    "        class_presence = torch.tensor([row[cls] for cls in self.class_names], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'masks': masks,\n",
    "            'instruction': instruction,\n",
    "            'class_presence': class_presence,\n",
    "            'image_name': row['image_path'],\n",
    "            'queried_classes': row['classes']  # For debugging\n",
    "        }\n",
    "\n",
    "print(\"âœ“ HistopathologyPermutationDataset class defined!\")\n",
    "print(\"  Key feature: Masks are loaded ONLY for queried classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce15425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Verifying permutation-aware mask loading...\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Instance 1:\n",
      "   Queried classes: Connective_Soft_tissue\n",
      "   Instruction: This Breast falls into Connective_Soft_tissue....\n",
      "   Mask channel sums (should be >0 only for queried classes):\n",
      "      Neoplastic: EMPTY (âœ— NOT queried)\n",
      "      Inflammatory: EMPTY (âœ— NOT queried)\n",
      "      Connective_Soft_tissue: sum=892 (âœ“ QUERIED)\n",
      "      Epithelial: EMPTY (âœ— NOT queried)\n",
      "      Dead: EMPTY (âœ— NOT queried)\n",
      "\n",
      "ðŸ“ Instance 2:\n",
      "   Queried classes: Inflammatory\n",
      "   Instruction: The Breast histopathology reveals Inflammatory....\n",
      "   Mask channel sums (should be >0 only for queried classes):\n",
      "      Neoplastic: EMPTY (âœ— NOT queried)\n",
      "      Inflammatory: sum=602 (âœ“ QUERIED)\n",
      "      Connective_Soft_tissue: EMPTY (âœ— NOT queried)\n",
      "      Epithelial: EMPTY (âœ— NOT queried)\n",
      "      Dead: EMPTY (âœ— NOT queried)\n",
      "\n",
      "ðŸ“ Instance 3:\n",
      "   Queried classes: Neoplastic\n",
      "   Instruction: Here we have a Breast with Neoplastic....\n",
      "   Mask channel sums (should be >0 only for queried classes):\n",
      "      Neoplastic: sum=3325 (âœ“ QUERIED)\n",
      "      Inflammatory: EMPTY (âœ— NOT queried)\n",
      "      Connective_Soft_tissue: EMPTY (âœ— NOT queried)\n",
      "      Epithelial: EMPTY (âœ— NOT queried)\n",
      "      Dead: EMPTY (âœ— NOT queried)\n",
      "\n",
      "================================================================================\n",
      "âœ“ Verification complete! Masks are loaded correctly based on query.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Verify Dataset Loading (Sanity Check)\n",
    "# Test that masks are correctly loaded only for queried classes\n",
    "\n",
    "print(\"ðŸ” Verifying permutation-aware mask loading...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find instances of the same image with different queries\n",
    "sample_image = df['image_path'].iloc[0]\n",
    "sample_instances = df[df['image_path'] == sample_image].head(3)\n",
    "\n",
    "temp_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "])\n",
    "\n",
    "temp_dataset = HistopathologyPermutationDataset(sample_instances, transform=temp_transform)\n",
    "\n",
    "for i in range(len(temp_dataset)):\n",
    "    sample = temp_dataset[i]\n",
    "    masks = sample['masks']\n",
    "    queried = sample['queried_classes']\n",
    "    instruction = sample['instruction']\n",
    "    \n",
    "    print(f\"\\nðŸ“ Instance {i+1}:\")\n",
    "    print(f\"   Queried classes: {queried}\")\n",
    "    print(f\"   Instruction: {instruction[:60]}...\")\n",
    "    print(f\"   Mask channel sums (should be >0 only for queried classes):\")\n",
    "    \n",
    "    for j, class_name in enumerate(class_names):\n",
    "        mask_sum = masks[j].sum().item()\n",
    "        is_queried = class_name in queried\n",
    "        status = \"âœ“ QUERIED\" if is_queried else \"âœ— NOT queried\"\n",
    "        expected = \"should have mask\" if is_queried else \"should be ZERO\"\n",
    "        actual = f\"sum={mask_sum:.0f}\" if mask_sum > 0 else \"EMPTY\"\n",
    "        print(f\"      {class_name}: {actual} ({status})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ Verification complete! Masks are loaded correctly based on query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Val Split (by unique images):\n",
      "  Train images: 11 unique images\n",
      "  Train instances: 77 total instances\n",
      "  Val images: 3 unique images\n",
      "  Val instances: 21 total instances\n",
      "\n",
      "âœ“ DataLoaders created!\n",
      "  Batch size: 4\n",
      "  Train batches: 20\n",
      "  Val batches: 6\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Augmentations and DataLoaders\n",
    "# Define augmentations\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    # A.HorizontalFlip(p=0.5),\n",
    "    # A.VerticalFlip(p=0.5),\n",
    "    # A.RandomRotate90(p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate(batch):\n",
    "    images = []\n",
    "    masks = []\n",
    "    instructions = []\n",
    "    class_presence = []\n",
    "    image_names = []\n",
    "    \n",
    "    for item in batch:\n",
    "        images.append(item['image'])\n",
    "        mask = item['masks']\n",
    "        if mask.shape[0] != 5:\n",
    "            mask = mask.permute(2, 0, 1)\n",
    "        masks.append(mask)\n",
    "        instructions.append(item['instruction'])\n",
    "        class_presence.append(item['class_presence'])\n",
    "        image_names.append(item['image_name'])\n",
    "    \n",
    "    return {\n",
    "        'image': torch.stack(images),\n",
    "        'masks': torch.stack(masks),\n",
    "        'instruction': instructions,\n",
    "        'class_presence': torch.stack(class_presence),\n",
    "        'image_name': image_names\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# Train-Val Split: Split by UNIQUE IMAGES, not instances\n",
    "# This ensures same image doesn't appear in both train and val\n",
    "# ============================================\n",
    "unique_images = df['image_path'].unique()\n",
    "train_images, val_images = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = df[df['image_path'].isin(train_images)]\n",
    "val_df = df[df['image_path'].isin(val_images)]\n",
    "\n",
    "print(f\"Train-Val Split (by unique images):\")\n",
    "print(f\"  Train images: {len(train_images)} unique images\")\n",
    "print(f\"  Train instances: {len(train_df)} total instances\")\n",
    "print(f\"  Val images: {len(val_images)} unique images\")\n",
    "print(f\"  Val instances: {len(val_df)} total instances\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HistopathologyPermutationDataset(train_df, transform=train_transform)\n",
    "val_dataset = HistopathologyPermutationDataset(val_df, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, \n",
    "                          pin_memory=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, \n",
    "                        pin_memory=True, collate_fn=custom_collate)\n",
    "\n",
    "print(f\"\\nâœ“ DataLoaders created!\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fed2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Metrics and loss functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Metrics and Loss Functions\n",
    "def dice_score(pred, target, smooth=1e-6, per_class=False):\n",
    "    \"\"\"\n",
    "    Calculate Dice score for binary masks\n",
    "    pred: [B, C, H, W] - predicted masks (after sigmoid)\n",
    "    target: [B, C, H, W] - ground truth masks\n",
    "    per_class: If True, return per-class scores, else return mean\n",
    "    \"\"\"\n",
    "    pred = (pred > 0.5).float()\n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    if per_class:\n",
    "        return dice.mean(dim=0)\n",
    "    else:\n",
    "        return dice.mean()\n",
    "\n",
    "def iou_score(pred, target, smooth=1e-6, per_class=False):\n",
    "    \"\"\"\n",
    "    Calculate IoU (Intersection over Union)\n",
    "    \"\"\"\n",
    "    pred = (pred > 0.5).float()\n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3)) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    if per_class:\n",
    "        return iou.mean(dim=0)\n",
    "    else:\n",
    "        return iou.mean()\n",
    "\n",
    "# Combined Loss: BCE + Dice Loss\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def dice_loss(self, pred, target, smooth=1e-6):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        intersection = (pred * target).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "        dice = (2. * intersection + smooth) / (union + smooth)\n",
    "        return 1 - dice.mean()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        bce = self.bce(pred, target)\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        return self.bce_weight * bce + self.dice_weight * dice\n",
    "\n",
    "print(\"âœ“ Metrics and loss functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810e546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 120,601,349\n",
      "âœ“ Training configuration set!\n",
      "  Device: cuda\n",
      "  Epochs: 10\n",
      "  Learning rate: 0.0001\n",
      "  Model save path: checkpoints/best_cipsnet_permutations.pth\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Training Configuration\n",
    "config = {\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'device': device,\n",
    "    'save_dir': 'checkpoints',\n",
    "    'best_model_path': 'checkpoints/best_cipsnet_permutations.pth',\n",
    "    'log_interval': 10,\n",
    "}\n",
    "\n",
    "os.makedirs(config['save_dir'], exist_ok=True)\n",
    "\n",
    "# Setup training - ensure model is in train mode and gradients are enabled\n",
    "model = model.to(config['device'])\n",
    "model.train()  # Ensure model is in training mode\n",
    "\n",
    "# Verify model parameters require gradients\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5)\n",
    "optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "\n",
    "# Use ReduceLROnPlateau instead of CosineAnnealingLR (since we want to step based on val_dice)\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "print(\"âœ“ Training configuration set!\")\n",
    "print(f\"  Device: {config['device']}\")\n",
    "print(f\"  Epochs: {config['num_epochs']}\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "print(f\"  Model save path: {config['best_model_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6974e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training and validation functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Training and Validation Functions\n",
    "def train_epoch(model, loader, criterion, optimizer, device, class_names):\n",
    "    model.train()\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_dice = torch.zeros(num_classes)\n",
    "    running_iou = torch.zeros(num_classes)\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['masks'].to(device)\n",
    "        instructions = batch['instruction']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass (CIPS-Net takes images AND instructions)\n",
    "        outputs = model(images, instructions)\n",
    "        pred_masks = outputs['masks']\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(pred_masks, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        with torch.no_grad():\n",
    "            pred_sigmoid = torch.sigmoid(pred_masks)\n",
    "            dice_per_class = dice_score(pred_sigmoid, masks, per_class=True)\n",
    "            iou_per_class = iou_score(pred_sigmoid, masks, per_class=True)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_dice += dice_per_class.cpu()\n",
    "        running_iou += iou_per_class.cpu()\n",
    "        num_batches += 1\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'dice': f'{dice_per_class.mean().item():.4f}'\n",
    "        })\n",
    "    \n",
    "    # Return mean loss and mean of per-class metrics\n",
    "    avg_dice = running_dice / num_batches\n",
    "    avg_iou = running_iou / num_batches\n",
    "    return running_loss / num_batches, avg_dice.mean().item(), avg_iou.mean().item()\n",
    "\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device, class_names):\n",
    "    model.eval()\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_dice = torch.zeros(num_classes)\n",
    "    running_iou = torch.zeros(num_classes)\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation', leave=False)\n",
    "        for batch in pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            masks = batch['masks'].to(device)\n",
    "            instructions = batch['instruction']\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images, instructions)\n",
    "            pred_masks = outputs['masks']\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(pred_masks, masks)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            pred_sigmoid = torch.sigmoid(pred_masks)\n",
    "            dice_per_class = dice_score(pred_sigmoid, masks, per_class=True)\n",
    "            iou_per_class = iou_score(pred_sigmoid, masks, per_class=True)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_dice += dice_per_class.cpu()\n",
    "            running_iou += iou_per_class.cpu()\n",
    "            num_batches += 1\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'dice': f'{dice_per_class.mean().item():.4f}'\n",
    "            })\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_dice = running_dice / num_batches\n",
    "    avg_iou = running_iou / num_batches\n",
    "    \n",
    "    # Create per-class dictionaries\n",
    "    per_class_dice = {class_names[i]: avg_dice[i].item() for i in range(num_classes)}\n",
    "    per_class_iou = {class_names[i]: avg_iou[i].item() for i in range(num_classes)}\n",
    "    \n",
    "    return running_loss / num_batches, avg_dice.mean().item(), avg_iou.mean().item(), per_class_dice, per_class_iou\n",
    "\n",
    "print(\"âœ“ Training and validation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5df17",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "Training with **compositional text queries** - the model must learn to segment ONLY the classes mentioned in each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Starting Compositional VLM Training\n",
      "======================================================================\n",
      "Training instances: 77\n",
      "Validation instances: 21\n",
      "Unique training images: 11\n",
      "Unique validation images: 3\n",
      "Classes: ['Neoplastic', 'Inflammatory', 'Connective_Soft_tissue', 'Epithelial', 'Dead']\n",
      "Approach: Each text query triggers segmentation of ONLY the mentioned classes\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e3a5f0b3dd4b4db8e99ad43b943876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9ad78cbeb34933b62ac482c78224f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Epoch [1/50]\n",
      "======================================================================\n",
      "  Train Loss: 0.8273 | Val Loss: 0.8240\n",
      "  Train Dice: 0.4748 | Val Dice: 0.4229\n",
      "  Train IoU:  0.4631 | Val IoU:  0.4123\n",
      "  Learning Rate: 1.00e-04\n",
      "\n",
      "  Per-Class Dice Scores:\n",
      "    Neoplastic               : Dice=0.0912, IoU=0.0496\n",
      "    Inflammatory             : Dice=0.0041, IoU=0.0021\n",
      "    Connective_Soft_tissue   : Dice=0.0194, IoU=0.0099\n",
      "    Epithelial               : Dice=1.0000, IoU=1.0000\n",
      "    Dead                     : Dice=1.0000, IoU=1.0000\n",
      "\n",
      "  *** New Best Model Saved! Dice: 0.4229 ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b61fafbc5a549a5865542326af1957a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10a181b6afe4829ba2baa7cb3980eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Epoch [2/50]\n",
      "======================================================================\n",
      "  Train Loss: 0.8245 | Val Loss: 0.8165\n",
      "  Train Dice: 0.4729 | Val Dice: 0.4233\n",
      "  Train IoU:  0.4619 | Val IoU:  0.4125\n",
      "  Learning Rate: 1.00e-04\n",
      "\n",
      "  Per-Class Dice Scores:\n",
      "    Neoplastic               : Dice=0.0907, IoU=0.0493\n",
      "    Inflammatory             : Dice=0.0041, IoU=0.0021\n",
      "    Connective_Soft_tissue   : Dice=0.0216, IoU=0.0110\n",
      "    Epithelial               : Dice=1.0000, IoU=1.0000\n",
      "    Dead                     : Dice=1.0000, IoU=1.0000\n",
      "\n",
      "  *** New Best Model Saved! Dice: 0.4233 ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a3f0a309d3444c84a8e791683d3abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a1dd51afd14374bff267e0920ff9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Epoch [3/50]\n",
      "======================================================================\n",
      "  Train Loss: 0.8204 | Val Loss: 0.8111\n",
      "  Train Dice: 0.4764 | Val Dice: 0.4190\n",
      "  Train IoU:  0.4661 | Val IoU:  0.4103\n",
      "  Learning Rate: 1.00e-04\n",
      "\n",
      "  Per-Class Dice Scores:\n",
      "    Neoplastic               : Dice=0.0910, IoU=0.0494\n",
      "    Inflammatory             : Dice=0.0039, IoU=0.0020\n",
      "    Connective_Soft_tissue   : Dice=0.0002, IoU=0.0001\n",
      "    Epithelial               : Dice=1.0000, IoU=1.0000\n",
      "    Dead                     : Dice=1.0000, IoU=1.0000\n",
      "\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2493bb997a540a4af21be3ba93a6d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c78420e4b942668e04b2709dea44e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Epoch [4/50]\n",
      "======================================================================\n",
      "  Train Loss: 0.8143 | Val Loss: 0.7915\n",
      "  Train Dice: 0.4530 | Val Dice: 0.4937\n",
      "  Train IoU:  0.4421 | Val IoU:  0.4851\n",
      "  Learning Rate: 1.00e-04\n",
      "\n",
      "  Per-Class Dice Scores:\n",
      "    Neoplastic               : Dice=0.0906, IoU=0.0492\n",
      "    Inflammatory             : Dice=0.0026, IoU=0.0013\n",
      "    Connective_Soft_tissue   : Dice=0.3750, IoU=0.3750\n",
      "    Epithelial               : Dice=1.0000, IoU=1.0000\n",
      "    Dead                     : Dice=1.0000, IoU=1.0000\n",
      "\n",
      "  *** New Best Model Saved! Dice: 0.4937 ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd861de834e4c2a870175b61fd14162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89973bbbc81e465b96a1345eb3460cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Epoch [5/50]\n",
      "======================================================================\n",
      "  Train Loss: 0.8058 | Val Loss: 0.7631\n",
      "  Train Dice: 0.5050 | Val Dice: 0.4902\n",
      "  Train IoU:  0.4954 | Val IoU:  0.4831\n",
      "  Learning Rate: 1.00e-04\n",
      "\n",
      "  Per-Class Dice Scores:\n",
      "    Neoplastic               : Dice=0.0761, IoU=0.0407\n",
      "    Inflammatory             : Dice=0.0000, IoU=0.0000\n",
      "    Connective_Soft_tissue   : Dice=0.3750, IoU=0.3750\n",
      "    Epithelial               : Dice=1.0000, IoU=1.0000\n",
      "    Dead                     : Dice=1.0000, IoU=1.0000\n",
      "\n",
      "  No improvement. Patience: 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c88be2ecfc84bbd80e9014e88638092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     train_loss, train_dice, train_iou \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLASS_NAMES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Validation (returns 5 values now)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     val_loss, val_dice, val_iou, per_class_dice, per_class_iou \u001b[38;5;241m=\u001b[39m validate_epoch(\n\u001b[0;32m     36\u001b[0m         model, val_loader, criterion, device, CLASS_NAMES\n\u001b[0;32m     37\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[16], line 27\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, loader, criterion, optimizer, device, class_names)\u001b[0m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred_masks, masks)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nikhi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main Training Loop\n",
    "num_epochs = 20\n",
    "best_dice = 0.0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "# Ensure model is in training mode with gradients enabled\n",
    "model.train()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_dice': [], 'val_dice': [],\n",
    "    'train_iou': [], 'val_iou': [],\n",
    "    'per_class_dice': [], 'per_class_iou': []\n",
    "}\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Starting Compositional VLM Training\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Training instances: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation instances: {len(val_loader.dataset)}\")\n",
    "print(f\"Unique training images: {train_df['image_path'].nunique()}\")\n",
    "print(f\"Unique validation images: {val_df['image_path'].nunique()}\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")\n",
    "print(f\"Approach: Each text query triggers segmentation of ONLY the mentioned classes\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss, train_dice, train_iou = train_epoch(model, train_loader, criterion, optimizer, device, CLASS_NAMES)\n",
    "    \n",
    "    # Validation (returns 5 values now)\n",
    "    val_loss, val_dice, val_iou, per_class_dice, per_class_iou = validate_epoch(\n",
    "        model, val_loader, criterion, device, CLASS_NAMES\n",
    "    )\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_dice'].append(train_dice)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    history['train_iou'].append(train_iou)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['per_class_dice'].append(per_class_dice)\n",
    "    history['per_class_iou'].append(per_class_iou)\n",
    "    \n",
    "    # Scheduler step (ReduceLROnPlateau takes the metric)\n",
    "    scheduler.step(val_dice)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Train Dice: {train_dice:.4f} | Val Dice: {val_dice:.4f}\")\n",
    "    print(f\"  Train IoU:  {train_iou:.4f} | Val IoU:  {val_iou:.4f}\")\n",
    "    print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(f\"\\n  Per-Class Dice Scores:\")\n",
    "    for class_name in CLASS_NAMES:\n",
    "        dice_val = per_class_dice.get(class_name, 0.0)\n",
    "        iou_val = per_class_iou.get(class_name, 0.0)\n",
    "        print(f\"    {class_name:25s}: Dice={dice_val:.4f}, IoU={iou_val:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_dice > best_dice:\n",
    "        best_dice = val_dice\n",
    "        patience_counter = 0\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_dice': best_dice,\n",
    "            'history': history,\n",
    "            'class_names': CLASS_NAMES,\n",
    "            'approach': 'compositional_text_conditional'\n",
    "        }\n",
    "        torch.save(checkpoint, 'best_cipsnet_compositional.pth')\n",
    "        print(f\"\\n  *** New Best Model Saved! Dice: {best_dice:.4f} ***\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"\\n  No improvement. Patience: {patience_counter}/{patience}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Best Validation Dice: {best_dice:.4f}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5014ac0",
   "metadata": {},
   "source": [
    "## 6. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0fed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['train_loss'], label='Train Loss', color='blue', linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Val Loss', color='red', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Dice curves\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['train_dice'], label='Train Dice', color='blue', linewidth=2)\n",
    "ax2.plot(history['val_dice'], label='Val Dice', color='red', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Dice Score')\n",
    "ax2.set_title('Training and Validation Dice')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# IoU curves\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history['train_iou'], label='Train IoU', color='blue', linewidth=2)\n",
    "ax3.plot(history['val_iou'], label='Val IoU', color='red', linewidth=2)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('IoU Score')\n",
    "ax3.set_title('Training and Validation IoU')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Per-class Dice evolution\n",
    "ax4 = axes[1, 1]\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(CLASS_NAMES)))\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    class_dice_history = [epoch_dice.get(class_name, 0) for epoch_dice in history['per_class_dice']]\n",
    "    ax4.plot(class_dice_history, label=class_name, color=colors[i], linewidth=2)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Dice Score')\n",
    "ax4.set_title('Per-Class Validation Dice')\n",
    "ax4.legend(loc='lower right', fontsize=8)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('compositional_training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final per-class summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Per-Class Performance (Best Epoch)\")\n",
    "print(\"=\"*60)\n",
    "best_epoch_idx = np.argmax(history['val_dice'])\n",
    "best_per_class_dice = history['per_class_dice'][best_epoch_idx]\n",
    "best_per_class_iou = history['per_class_iou'][best_epoch_idx]\n",
    "\n",
    "for class_name in CLASS_NAMES:\n",
    "    dice = best_per_class_dice.get(class_name, 0.0)\n",
    "    iou = best_per_class_iou.get(class_name, 0.0)\n",
    "    print(f\"  {class_name:25s}: Dice={dice:.4f}, IoU={iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016fe2f8",
   "metadata": {},
   "source": [
    "## 7. Compositional Query Visualization\n",
    "\n",
    "This is the **key test** - the same image should produce different segmentation outputs based on which classes are mentioned in the text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_compositional_segmentation(model, image_path, queries, class_names, device, img_size=224):\n",
    "    \"\"\"\n",
    "    Demonstrate compositional text-conditional segmentation.\n",
    "    Same image + different queries = different segmentation outputs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    original_image = image.copy()\n",
    "    \n",
    "    # Resize\n",
    "    image_resized = cv2.resize(image, (img_size, img_size))\n",
    "    \n",
    "    # Normalize\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image_normalized = (image_resized / 255.0 - mean) / std\n",
    "    image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).float().unsqueeze(0).to(device)\n",
    "    \n",
    "    # Color map for visualization\n",
    "    colors = {\n",
    "        'Neoplastic': [255, 0, 0],          # Red\n",
    "        'Inflammatory': [0, 255, 0],         # Green\n",
    "        'Connective_Soft_tissue': [0, 0, 255], # Blue\n",
    "        'Dead': [255, 255, 0],               # Yellow\n",
    "        'Epithelial': [255, 0, 255]          # Magenta\n",
    "    }\n",
    "    \n",
    "    num_queries = len(queries)\n",
    "    fig, axes = plt.subplots(2, num_queries + 1, figsize=(5 * (num_queries + 1), 10))\n",
    "    \n",
    "    # Show original image in first column\n",
    "    axes[0, 0].imshow(cv2.resize(original_image, (img_size, img_size)))\n",
    "    axes[0, 0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for q_idx, query in enumerate(queries):\n",
    "            # Get prediction for this query\n",
    "            output = model(image_tensor, [query])\n",
    "            pred = torch.sigmoid(output).cpu().numpy()[0]\n",
    "            \n",
    "            # Create overlay visualization\n",
    "            overlay = cv2.resize(original_image, (img_size, img_size)).astype(np.float32)\n",
    "            mask_combined = np.zeros((img_size, img_size, 3), dtype=np.float32)\n",
    "            \n",
    "            # Parse which classes are in this query\n",
    "            query_classes = set()\n",
    "            for class_name in class_names:\n",
    "                if class_name.lower().replace('_', ' ') in query.lower() or class_name.lower() in query.lower():\n",
    "                    query_classes.add(class_name)\n",
    "            \n",
    "            # Overlay each channel\n",
    "            active_classes = []\n",
    "            for ch_idx, class_name in enumerate(class_names):\n",
    "                channel_pred = pred[ch_idx]\n",
    "                binary_mask = (channel_pred > 0.5).astype(np.float32)\n",
    "                \n",
    "                if binary_mask.sum() > 0:\n",
    "                    active_classes.append(class_name)\n",
    "                    color = np.array(colors[class_name]) / 255.0\n",
    "                    for c in range(3):\n",
    "                        mask_combined[:, :, c] += binary_mask * color[c]\n",
    "            \n",
    "            # Blend with original\n",
    "            alpha = 0.5\n",
    "            mask_combined = np.clip(mask_combined, 0, 1)\n",
    "            blended = overlay / 255.0 * (1 - alpha) + mask_combined * alpha\n",
    "            blended = np.clip(blended, 0, 1)\n",
    "            \n",
    "            # Top row: overlay\n",
    "            axes[0, q_idx + 1].imshow(blended)\n",
    "            axes[0, q_idx + 1].set_title(f'Query: {query[:40]}...', fontsize=10, fontweight='bold')\n",
    "            axes[0, q_idx + 1].axis('off')\n",
    "            \n",
    "            # Bottom row: individual channel predictions\n",
    "            ax_bottom = axes[1, q_idx + 1]\n",
    "            \n",
    "            # Create a multi-channel visualization\n",
    "            channel_vis = np.zeros((img_size, img_size * len(class_names), 3))\n",
    "            for ch_idx, class_name in enumerate(class_names):\n",
    "                channel_pred = pred[ch_idx]\n",
    "                start_x = ch_idx * img_size\n",
    "                # Grayscale to RGB\n",
    "                for c in range(3):\n",
    "                    channel_vis[:, start_x:start_x+img_size, c] = channel_pred\n",
    "            \n",
    "            ax_bottom.imshow(channel_vis, cmap='gray')\n",
    "            ax_bottom.set_title(f'Active: {\", \".join(active_classes) if active_classes else \"None\"}', fontsize=9)\n",
    "            ax_bottom.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('compositional_query_demo.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('best_cipsnet_compositional.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']} with Dice: {checkpoint['best_dice']:.4f}\")\n",
    "\n",
    "# Get a sample image that has multiple classes\n",
    "sample_image_id = val_df['image_id'].iloc[0]\n",
    "sample_image_path = os.path.join(BASE_PATH, 'multi_images', f'{sample_image_id}.png')\n",
    "\n",
    "# Define different compositional queries for the SAME image\n",
    "test_queries = [\n",
    "    \"Segment Neoplastic tissue\",\n",
    "    \"Segment Inflammatory cells\",\n",
    "    \"Segment Neoplastic and Inflammatory\",\n",
    "    \"Segment Connective Soft tissue and Epithelial\",\n",
    "    \"Segment all: Neoplastic, Inflammatory, Connective Soft tissue, Dead, Epithelial\"\n",
    "]\n",
    "\n",
    "print(f\"\\nTesting compositional queries on image: {sample_image_id}\")\n",
    "print(\"=\"*60)\n",
    "for q in test_queries:\n",
    "    print(f\"  - {q}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize\n",
    "visualize_compositional_segmentation(model, sample_image_path, test_queries, CLASS_NAMES, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1ee03",
   "metadata": {},
   "source": [
    "## 8. Detailed Compositional Testing\n",
    "\n",
    "Test the model's ability to understand different text queries and produce appropriate segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_compositional_response(model, image_tensor, queries, class_names, device):\n",
    "    \"\"\"\n",
    "    Quantitatively analyze how the model responds to different compositional queries.\n",
    "    \n",
    "    For true text-conditional segmentation:\n",
    "    - Query for class A â†’ high activation in channel A, low in others\n",
    "    - Query for class A+B â†’ high activation in channels A and B, low in others\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for query in queries:\n",
    "            output = model(image_tensor, [query])\n",
    "            pred = torch.sigmoid(output).cpu().numpy()[0]\n",
    "            \n",
    "            # Parse expected classes from query\n",
    "            expected_classes = set()\n",
    "            for class_name in class_names:\n",
    "                if class_name.lower().replace('_', ' ') in query.lower() or class_name.lower() in query.lower():\n",
    "                    expected_classes.add(class_name)\n",
    "            \n",
    "            # Calculate mean activation per channel\n",
    "            activations = {}\n",
    "            for ch_idx, class_name in enumerate(class_names):\n",
    "                activations[class_name] = pred[ch_idx].mean()\n",
    "            \n",
    "            results.append({\n",
    "                'query': query,\n",
    "                'expected': expected_classes,\n",
    "                'activations': activations\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Get a validation sample\n",
    "val_dataset_for_test = HistopathologyPermutationDataset(\n",
    "    val_df, BASE_PATH, img_size=224, transform=None\n",
    ")\n",
    "\n",
    "# Pick a sample with multiple classes present\n",
    "test_idx = 0\n",
    "test_sample = val_dataset_for_test[test_idx]\n",
    "test_image = test_sample['image'].unsqueeze(0).to(device)\n",
    "test_text = test_sample['text']\n",
    "\n",
    "print(f\"Original query from dataset: {test_text}\")\n",
    "print(f\"\\nTesting compositional understanding...\\n\")\n",
    "\n",
    "# Test different compositional queries\n",
    "compositional_queries = [\n",
    "    \"Segment Neoplastic tissue only\",\n",
    "    \"Segment Inflammatory cells only\",\n",
    "    \"Segment Neoplastic and Inflammatory\",\n",
    "    \"Segment Connective Soft tissue only\",\n",
    "    \"Segment Dead cells only\",\n",
    "    \"Segment Epithelial tissue only\",\n",
    "    \"Segment Neoplastic, Inflammatory, and Connective Soft tissue\",\n",
    "    \"Segment all tissue types: Neoplastic, Inflammatory, Connective Soft tissue, Dead, Epithelial\"\n",
    "]\n",
    "\n",
    "results = analyze_compositional_response(model, test_image, compositional_queries, CLASS_NAMES, device)\n",
    "\n",
    "# Display results as a table\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Query':<55} | {'Expected Classes':<30} | Channel Activations\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for r in results:\n",
    "    query_short = r['query'][:52] + \"...\" if len(r['query']) > 55 else r['query']\n",
    "    expected_str = \", \".join(sorted(r['expected']))[:28] if r['expected'] else \"None\"\n",
    "    \n",
    "    # Format activations\n",
    "    act_parts = []\n",
    "    for class_name in CLASS_NAMES:\n",
    "        act = r['activations'][class_name]\n",
    "        # Highlight if expected vs not expected\n",
    "        if class_name in r['expected']:\n",
    "            act_parts.append(f\"{class_name[:3]}:{act:.3f}*\")\n",
    "        else:\n",
    "            act_parts.append(f\"{class_name[:3]}:{act:.3f}\")\n",
    "    \n",
    "    print(f\"{query_short:<55} | {expected_str:<30} | {' '.join(act_parts)}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"\\n* = Expected to be high based on query\")\n",
    "print(\"\\nIdeal behavior: Marked channels (*) should have higher activations than unmarked ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75567b",
   "metadata": {},
   "source": [
    "## 9. Side-by-Side Comparison: Query vs Ground Truth\n",
    "\n",
    "Visualize the model's predictions alongside ground truth for specific compositional queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63906bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction_vs_ground_truth(model, dataset, indices, device, class_names):\n",
    "    \"\"\"\n",
    "    Compare model predictions with ground truth masks for compositional queries.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    colors = {\n",
    "        'Neoplastic': [255, 0, 0],\n",
    "        'Inflammatory': [0, 255, 0],\n",
    "        'Connective_Soft_tissue': [0, 0, 255],\n",
    "        'Dead': [255, 255, 0],\n",
    "        'Epithelial': [255, 0, 255]\n",
    "    }\n",
    "    \n",
    "    num_samples = len(indices)\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4 * num_samples))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row, idx in enumerate(indices):\n",
    "            sample = dataset[idx]\n",
    "            image = sample['image'].unsqueeze(0).to(device)\n",
    "            mask_gt = sample['mask'].numpy()\n",
    "            query = sample['text']\n",
    "            \n",
    "            # Get prediction\n",
    "            output = model(image, [query])\n",
    "            pred = torch.sigmoid(output).cpu().numpy()[0]\n",
    "            \n",
    "            # Convert image for display\n",
    "            img_display = sample['image'].permute(1, 2, 0).numpy()\n",
    "            img_display = img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            img_display = np.clip(img_display, 0, 1)\n",
    "            \n",
    "            # Create overlays\n",
    "            gt_overlay = np.zeros((*mask_gt.shape[1:], 3))\n",
    "            pred_overlay = np.zeros((*pred.shape[1:], 3))\n",
    "            \n",
    "            for ch_idx, class_name in enumerate(class_names):\n",
    "                color = np.array(colors[class_name]) / 255.0\n",
    "                \n",
    "                # Ground truth\n",
    "                gt_mask = mask_gt[ch_idx] > 0.5\n",
    "                for c in range(3):\n",
    "                    gt_overlay[:, :, c] += gt_mask * color[c]\n",
    "                \n",
    "                # Prediction\n",
    "                pred_mask = pred[ch_idx] > 0.5\n",
    "                for c in range(3):\n",
    "                    pred_overlay[:, :, c] += pred_mask * color[c]\n",
    "            \n",
    "            gt_overlay = np.clip(gt_overlay, 0, 1)\n",
    "            pred_overlay = np.clip(pred_overlay, 0, 1)\n",
    "            \n",
    "            # Plot\n",
    "            axes[row, 0].imshow(img_display)\n",
    "            axes[row, 0].set_title('Input Image', fontsize=10)\n",
    "            axes[row, 0].axis('off')\n",
    "            \n",
    "            axes[row, 1].imshow(gt_overlay)\n",
    "            axes[row, 1].set_title('Ground Truth', fontsize=10)\n",
    "            axes[row, 1].axis('off')\n",
    "            \n",
    "            axes[row, 2].imshow(pred_overlay)\n",
    "            axes[row, 2].set_title('Prediction', fontsize=10)\n",
    "            axes[row, 2].axis('off')\n",
    "            \n",
    "            # Difference map\n",
    "            diff = np.abs(gt_overlay - pred_overlay)\n",
    "            axes[row, 3].imshow(diff)\n",
    "            axes[row, 3].set_title('Difference', fontsize=10)\n",
    "            axes[row, 3].axis('off')\n",
    "            \n",
    "            # Add query as row label\n",
    "            fig.text(0.02, (num_samples - row - 0.5) / num_samples, \n",
    "                     f'Query: {query[:60]}...', fontsize=9, va='center', ha='left',\n",
    "                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [plt.Rectangle((0, 0), 1, 1, facecolor=np.array(colors[c])/255.0, label=c) \n",
    "                       for c in class_names]\n",
    "    fig.legend(handles=legend_elements, loc='upper center', ncol=5, fontsize=9, \n",
    "               bbox_to_anchor=(0.5, 0.02))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.08, left=0.15)\n",
    "    plt.savefig('prediction_vs_ground_truth.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few validation samples\n",
    "test_indices = [0, 10, 20, 30]  # Adjust based on dataset size\n",
    "test_indices = [i for i in test_indices if i < len(val_dataset_for_test)]\n",
    "\n",
    "print(f\"Comparing predictions vs ground truth for {len(test_indices)} samples...\")\n",
    "visualize_prediction_vs_ground_truth(model, val_dataset_for_test, test_indices, device, CLASS_NAMES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
