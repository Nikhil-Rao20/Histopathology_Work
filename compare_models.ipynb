{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5cfafd",
   "metadata": {},
   "source": [
    "# Model Comparison: Text-Guided Segmentation vs CIPS-Net\n",
    "\n",
    "This notebook compares all trained text-guided segmentation models against the CIPS-Net baseline.\n",
    "\n",
    "## Evaluation Metrics\n",
    "- **mIoU**: Mean Intersection over Union\n",
    "- **mDice**: Mean Dice coefficient  \n",
    "- **Per-class IoU/Dice**: For each cell type\n",
    "\n",
    "## Datasets\n",
    "- **PanNuke**: 3-fold cross-validation (training)\n",
    "- **CoNSeP**: Zero-shot evaluation\n",
    "- **MoNuSAC**: Zero-shot evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f338460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work\"\n",
    "sys.path.insert(0, WORKSPACE)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('tab10')\n",
    "\n",
    "CHECKPOINT_DIR = Path(f\"{WORKSPACE}/checkpoints/text_guided\")\n",
    "\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"Exists: {CHECKPOINT_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e77586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model information\n",
    "MODEL_INFO = {\n",
    "    'clipseg': {'name': 'CLIPSeg', 'venue': 'CVPR 2022', 'type': 'Decoder'},\n",
    "    'clipseg_rd64': {'name': 'CLIPSeg-RD64', 'venue': 'CVPR 2022', 'type': 'Decoder'},\n",
    "    'clipseg_rd128': {'name': 'CLIPSeg-RD128', 'venue': 'CVPR 2022', 'type': 'Decoder'},\n",
    "    'lseg': {'name': 'LSeg', 'venue': 'ICLR 2022', 'type': 'Dense Prediction'},\n",
    "    'groupvit': {'name': 'GroupViT', 'venue': 'CVPR 2022', 'type': 'Grouping'},\n",
    "    'san': {'name': 'SAN', 'venue': 'CVPR 2023', 'type': 'Side Adapter'},\n",
    "    'fc_clip': {'name': 'FC-CLIP', 'venue': 'NeurIPS 2023', 'type': 'Frozen CLIP'},\n",
    "    'fc_clip_convnext': {'name': 'FC-CLIP-ConvNext', 'venue': 'NeurIPS 2023', 'type': 'Frozen CLIP'},\n",
    "    'ovseg': {'name': 'OVSeg', 'venue': 'CVPR 2023', 'type': 'Open Vocabulary'},\n",
    "    'cat_seg': {'name': 'CAT-Seg', 'venue': 'CVPR 2024', 'type': 'Cost Aggregation'},\n",
    "    'sed': {'name': 'SED', 'venue': 'CVPR 2024', 'type': 'Decoupled'},\n",
    "    'openseed': {'name': 'OpenSeeD', 'venue': 'ICCV 2023', 'type': 'Universal'},\n",
    "    'odise': {'name': 'ODISE', 'venue': 'CVPR 2023', 'type': 'Diffusion'},\n",
    "    'semantic_sam': {'name': 'Semantic-SAM', 'venue': 'ECCV 2024', 'type': 'SAM-based'},\n",
    "    'cips_net': {'name': 'CIPS-Net', 'venue': 'Baseline', 'type': 'Instruction-guided'},\n",
    "}\n",
    "\n",
    "CLASS_NAMES = ['Neoplastic', 'Inflammatory', 'Connective', 'Dead', 'Epithelial']\n",
    "\n",
    "print(f\"Models to compare: {len(MODEL_INFO)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30347935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results\n",
    "def load_results():\n",
    "    \"\"\"Load results from all trained models.\"\"\"\n",
    "    results = defaultdict(dict)\n",
    "    \n",
    "    # Load text-guided model results\n",
    "    if CHECKPOINT_DIR.exists():\n",
    "        for result_file in CHECKPOINT_DIR.glob('results_*.json'):\n",
    "            with open(result_file) as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            model_name = data.get('model_name', result_file.stem.replace('results_', ''))\n",
    "            fold = data.get('fold', 0)\n",
    "            \n",
    "            if model_name not in results:\n",
    "                results[model_name] = {'folds': {}}\n",
    "            \n",
    "            results[model_name]['folds'][fold] = {\n",
    "                'iou': data.get('best_iou', 0),\n",
    "                'dice': data.get('final_dice', 0),\n",
    "                'history': data.get('history', {}),\n",
    "            }\n",
    "    \n",
    "    # Load CIPS-Net baseline results (if available)\n",
    "    cips_checkpoint = Path(f\"{WORKSPACE}/checkpoints/best_cipsnet_binary.pth\")\n",
    "    if cips_checkpoint.exists():\n",
    "        # Placeholder - update with actual CIPS-Net results\n",
    "        results['cips_net'] = {\n",
    "            'folds': {\n",
    "                0: {'iou': 0.0, 'dice': 0.0},\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = load_results()\n",
    "print(f\"Loaded results for {len(results)} models:\")\n",
    "for model, data in results.items():\n",
    "    folds = list(data['folds'].keys())\n",
    "    print(f\"  {model}: {len(folds)} fold(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e920a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "def aggregate_results(results):\n",
    "    \"\"\"Calculate mean and std across folds.\"\"\"\n",
    "    summary = []\n",
    "    \n",
    "    for model_name, data in results.items():\n",
    "        folds = data['folds']\n",
    "        if not folds:\n",
    "            continue\n",
    "        \n",
    "        ious = [f['iou'] for f in folds.values()]\n",
    "        dices = [f['dice'] for f in folds.values()]\n",
    "        \n",
    "        info = MODEL_INFO.get(model_name, {'name': model_name, 'venue': 'Unknown', 'type': 'Unknown'})\n",
    "        \n",
    "        summary.append({\n",
    "            'model_key': model_name,\n",
    "            'Model': info['name'],\n",
    "            'Venue': info['venue'],\n",
    "            'Type': info['type'],\n",
    "            'mIoU': np.mean(ious),\n",
    "            'IoU_std': np.std(ious),\n",
    "            'mDice': np.mean(dices),\n",
    "            'Dice_std': np.std(dices),\n",
    "            'n_folds': len(folds),\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(summary)\n",
    "    df = df.sort_values('mIoU', ascending=False)\n",
    "    return df\n",
    "\n",
    "summary_df = aggregate_results(results)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d27b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table (LaTeX-ready)\n",
    "if not summary_df.empty:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL COMPARISON TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Format for display\n",
    "    display_df = summary_df[['Model', 'Venue', 'Type', 'mIoU', 'IoU_std', 'mDice', 'Dice_std']].copy()\n",
    "    display_df['mIoU'] = display_df.apply(lambda x: f\"{x['mIoU']:.4f} ± {x['IoU_std']:.4f}\", axis=1)\n",
    "    display_df['mDice'] = display_df.apply(lambda x: f\"{x['mDice']:.4f} ± {x['Dice_std']:.4f}\", axis=1)\n",
    "    display_df = display_df.drop(columns=['IoU_std', 'Dice_std'])\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = CHECKPOINT_DIR / 'comparison_table.csv'\n",
    "    summary_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nTable saved to {output_path}\")\n",
    "else:\n",
    "    print(\"No results to compare yet. Run training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecf6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "if not summary_df.empty and len(summary_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # mIoU comparison\n",
    "    ax1 = axes[0]\n",
    "    x = np.arange(len(summary_df))\n",
    "    bars1 = ax1.barh(x, summary_df['mIoU'], xerr=summary_df['IoU_std'], \n",
    "                     color='steelblue', alpha=0.8, capsize=3)\n",
    "    ax1.set_yticks(x)\n",
    "    ax1.set_yticklabels(summary_df['Model'])\n",
    "    ax1.set_xlabel('mIoU')\n",
    "    ax1.set_title('Mean IoU Comparison')\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (v, std) in enumerate(zip(summary_df['mIoU'], summary_df['IoU_std'])):\n",
    "        ax1.text(v + std + 0.02, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "    \n",
    "    # mDice comparison\n",
    "    ax2 = axes[1]\n",
    "    bars2 = ax2.barh(x, summary_df['mDice'], xerr=summary_df['Dice_std'],\n",
    "                     color='coral', alpha=0.8, capsize=3)\n",
    "    ax2.set_yticks(x)\n",
    "    ax2.set_yticklabels(summary_df['Model'])\n",
    "    ax2.set_xlabel('mDice')\n",
    "    ax2.set_title('Mean Dice Comparison')\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    for i, (v, std) in enumerate(zip(summary_df['mDice'], summary_df['Dice_std'])):\n",
    "        ax2.text(v + std + 0.02, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CHECKPOINT_DIR / 'comparison_bar_chart.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to visualize yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves comparison\n",
    "def plot_training_curves(results, metric='val_iou'):\n",
    "    \"\"\"Plot training curves for all models.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for model_name, data in results.items():\n",
    "        folds = data.get('folds', {})\n",
    "        if not folds:\n",
    "            continue\n",
    "        \n",
    "        # Use first fold's history\n",
    "        first_fold = list(folds.values())[0]\n",
    "        history = first_fold.get('history', {})\n",
    "        \n",
    "        if metric in history:\n",
    "            values = history[metric]\n",
    "            info = MODEL_INFO.get(model_name, {'name': model_name})\n",
    "            ax.plot(values, label=info['name'], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "    ax.set_title(f'Training Curves - {metric.replace(\"_\", \" \").title()}')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "if results:\n",
    "    fig = plot_training_curves(results, 'val_iou')\n",
    "    fig.savefig(CHECKPOINT_DIR / 'training_curves_iou.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a22ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart for multi-metric comparison\n",
    "def radar_chart(summary_df, metrics=['mIoU', 'mDice']):\n",
    "    \"\"\"Create radar chart comparing models across metrics.\"\"\"\n",
    "    if summary_df.empty or len(summary_df) < 2:\n",
    "        print(\"Need at least 2 models for radar chart\")\n",
    "        return\n",
    "    \n",
    "    categories = CLASS_NAMES\n",
    "    n_cats = len(categories)\n",
    "    \n",
    "    # This would need per-class metrics\n",
    "    # For now, create a simple comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    angles = [n / float(len(summary_df)) * 2 * np.pi for n in range(len(summary_df))]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # IoU\n",
    "    values = summary_df['mIoU'].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label='mIoU', color='steelblue')\n",
    "    ax.fill(angles, values, alpha=0.25, color='steelblue')\n",
    "    \n",
    "    # Dice\n",
    "    values = summary_df['mDice'].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label='mDice', color='coral')\n",
    "    ax.fill(angles, values, alpha=0.25, color='coral')\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(summary_df['Model'], size=10)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "    ax.set_title('Model Performance Comparison', size=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "if len(summary_df) >= 2:\n",
    "    fig = radar_chart(summary_df)\n",
    "    if fig:\n",
    "        fig.savefig(CHECKPOINT_DIR / 'radar_comparison.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "def generate_report(summary_df):\n",
    "    \"\"\"Generate markdown report.\"\"\"\n",
    "    report = []\n",
    "    report.append(\"# Text-Guided Segmentation Experiment Report\\n\")\n",
    "    report.append(\"## Overview\\n\")\n",
    "    report.append(f\"- **Total models evaluated**: {len(summary_df)}\\n\")\n",
    "    \n",
    "    if not summary_df.empty:\n",
    "        best_model = summary_df.iloc[0]\n",
    "        report.append(f\"- **Best model**: {best_model['Model']}\\n\")\n",
    "        report.append(f\"- **Best mIoU**: {best_model['mIoU']:.4f} ± {best_model['IoU_std']:.4f}\\n\")\n",
    "        report.append(f\"- **Best mDice**: {best_model['mDice']:.4f} ± {best_model['Dice_std']:.4f}\\n\")\n",
    "    \n",
    "    report.append(\"\\n## Results Table\\n\")\n",
    "    report.append(\"| Rank | Model | Venue | mIoU | mDice |\\n\")\n",
    "    report.append(\"|------|-------|-------|------|-------|\\n\")\n",
    "    \n",
    "    for i, row in summary_df.iterrows():\n",
    "        rank = summary_df.index.get_loc(i) + 1\n",
    "        report.append(f\"| {rank} | {row['Model']} | {row['Venue']} | \"\n",
    "                     f\"{row['mIoU']:.4f}±{row['IoU_std']:.4f} | \"\n",
    "                     f\"{row['mDice']:.4f}±{row['Dice_std']:.4f} |\\n\")\n",
    "    \n",
    "    report.append(\"\\n## Conclusions\\n\")\n",
    "    report.append(\"(Add your analysis here)\\n\")\n",
    "    \n",
    "    return ''.join(report)\n",
    "\n",
    "report = generate_report(summary_df)\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open(CHECKPOINT_DIR / 'experiment_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"\\nReport saved to {CHECKPOINT_DIR / 'experiment_report.md'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31f4fa",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Train remaining models**: Use `run_all_experiments.py` or the unified notebook\n",
    "2. **Zero-shot evaluation**: Test on CoNSeP and MoNuSAC\n",
    "3. **Statistical analysis**: Paired t-tests for significance\n",
    "4. **Qualitative analysis**: Visualize segmentation outputs\n",
    "5. **Paper-ready figures**: Generate publication-quality plots"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
