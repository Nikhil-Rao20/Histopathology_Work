{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f596cac",
   "metadata": {},
   "source": [
    "# Unified Text-Guided Segmentation Training\n",
    "\n",
    "This notebook provides a unified interface for training all 14 text-guided segmentation models on histopathology datasets.\n",
    "\n",
    "## Experiment Setup\n",
    "- **Training**: PanNuke 3-fold cross-validation\n",
    "- **Zero-shot Evaluation**: CoNSeP, MoNuSAC\n",
    "- **Fine-tuning**: Optional fine-tuning on target datasets\n",
    "- **Comparison**: Against CIPS-Net baseline\n",
    "\n",
    "## Available Models (14 total)\n",
    "| # | Model | Venue | Status |\n",
    "|---|-------|-------|--------|\n",
    "| 1 | CLIPSeg | CVPR 2022 | ✅ Working |\n",
    "| 2 | LSeg | ICLR 2022 | ✅ Working |\n",
    "| 3 | GroupViT | CVPR 2022 | ✅ Working |\n",
    "| 4 | SAN | CVPR 2023 | ✅ Working |\n",
    "| 5 | FC-CLIP | NeurIPS 2023 | ✅ Working |\n",
    "| 6 | OVSeg | CVPR 2023 | ✅ Working |\n",
    "| 7 | CAT-Seg | CVPR 2024 | ✅ Working |\n",
    "| 8 | SED | CVPR 2024 | ✅ Working |\n",
    "| 9 | MAFT+ | ECCV 2024 Oral | ⚠️ Needs fix |\n",
    "| 10 | X-Decoder | CVPR 2023 | ⚠️ Needs fix |\n",
    "| 11 | OpenSeeD | ICCV 2023 | ✅ Working |\n",
    "| 12 | ODISE | CVPR 2023 | ✅ Working |\n",
    "| 13 | TagAlign | arXiv 2023 | ⚠️ Needs fix |\n",
    "| 14 | Semantic-SAM | ECCV 2024 | ✅ Working |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005b23ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT CONFIGURATION\n",
      "======================================================================\n",
      "Model: groupvit\n",
      "Experiment Name: groupvit_20260128_150847\n",
      "Results: /mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/results/groupvit_20260128_150847\n",
      "\n",
      "Training Config:\n",
      "  Image Size: 224x224 (SAME AS CIPS-NET)\n",
      "  Epochs: 20 (Early Stopping: 5)\n",
      "  Batch Size: 8\n",
      "  3-Fold CV on PanNuke\n",
      "\n",
      "Text Variants (CIPS-Net Protocol):\n",
      "  1. Per-Image Text: From annotations.csv 'instruction' column\n",
      "  2. Common Text: 'Segment all Neoplastic, Inflammatory, Connective, ...'\n",
      "  3. No Text: Empty string\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - CHANGE THIS TO SELECT MODEL\n",
    "# ============================================================================\n",
    "from datetime import datetime\n",
    "\n",
    "# Model selection - change this to train different models\n",
    "MODEL_NAME = \"groupvit\"  # Options: clipseg, lseg, groupvit, san, fc_clip, ovseg, \n",
    "                        #          cat_seg, sed, openseed, odise, semantic_sam\n",
    "\n",
    "# Experiment timestamp for unique naming\n",
    "EXPERIMENT_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXPERIMENT_NAME = f\"{MODEL_NAME}_{EXPERIMENT_TIMESTAMP}\"\n",
    "\n",
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Dataset\n",
    "    \"num_classes\": 5,\n",
    "    \"image_size\": 224,  # SAME AS CIPS-NET (224x224) for fair comparison\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 8,\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    # Cross-validation\n",
    "    \"num_folds\": 3,\n",
    "    \n",
    "    # Model\n",
    "    \"clip_model\": \"ViT-B/16\",\n",
    "    \"freeze_clip\": True,\n",
    "    \n",
    "    # Paths\n",
    "    \"data_root\": \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/Dataset\",\n",
    "    \"pannuke_preprocess_root\": \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/PanNuke_Preprocess\",\n",
    "    \"results_dir\": f\"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/results/{EXPERIMENT_NAME}\",\n",
    "    \n",
    "    # External datasets for zero-shot evaluation\n",
    "    \"external_datasets_root\": \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/Histopathology_Datasets_Official\",\n",
    "    \n",
    "    # Device\n",
    "    \"device\": \"cuda\",\n",
    "    \n",
    "    # Early stopping\n",
    "    \"early_stopping_patience\": 5,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT VARIANTS - MATCHING CIPS-NET EVALUATION PROTOCOL EXACTLY\n",
    "# ============================================================================\n",
    "# 3 Text Variants (same as CIPS-Net):\n",
    "#   1. Per-Image Text: Read from annotations.csv 'instruction' column (varies per image)\n",
    "#   2. Common Text: Single generic instruction for ALL images\n",
    "#   3. No Text: Empty string (pure visual features)\n",
    "# ============================================================================\n",
    "\n",
    "# Class names (SAME AS CIPS-NET)\n",
    "CLASS_NAMES = ['Neoplastic', 'Inflammatory', 'Connective_Soft_tissue', 'Dead', 'Epithelial']\n",
    "\n",
    "# Per-Image Text: Will be read from annotations.csv during evaluation\n",
    "# (Each image has its own unique instruction based on classes present)\n",
    "# Example: \"Identify all neoplastic, inflammatory and connective soft tissue regions.\"\n",
    "\n",
    "# Common Text: Single sentence mentioning ALL classes (EXACTLY SAME AS CIPS-NET)\n",
    "COMMON_TEXT = \"Segment all Neoplastic, Inflammatory, Connective, Dead, and Epithelial cells in the image.\"\n",
    "\n",
    "# No Text: Empty string (tests pure visual features) (SAME AS CIPS-NET)\n",
    "NO_TEXT = \"\"\n",
    "\n",
    "# For training, we use class-specific prompts (one per class for VLM models)\n",
    "# This is the standard approach for CLIP-based segmentation models\n",
    "TRAINING_TEXT_PROMPTS = [\n",
    "    \"neoplastic cells\",           # Class 0\n",
    "    \"inflammatory cells\",          # Class 1\n",
    "    \"connective tissue cells\",     # Class 2\n",
    "    \"dead cells\",                  # Class 3\n",
    "    \"epithelial cells\",            # Class 4\n",
    "]\n",
    "\n",
    "# Backwards-compatible dictionary of text variants used by older cells\n",
    "# - 'per_image_text' is represented by `None` because it requires reading\n",
    "#   per-image instructions from `annotations.csv` at evaluation time.\n",
    "# - 'common_text' and 'no_text' are represented as a list repeated per class\n",
    "#   to match VLMs that expect one prompt per class.\n",
    "TEXT_PROMPTS_VARIANTS = {\n",
    "    'per_image_text': None,\n",
    "    'common_text': [COMMON_TEXT] * CONFIG['num_classes'],\n",
    "    'no_text': [NO_TEXT] * CONFIG['num_classes'],\n",
    "}\n",
    "\n",
    "# Default prompts for training\n",
    "TEXT_PROMPTS = TRAINING_TEXT_PROMPTS\n",
    "\n",
    "print(f\"=\" * 70)\n",
    "print(f\"EXPERIMENT CONFIGURATION\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Experiment Name: {EXPERIMENT_NAME}\")\n",
    "print(f\"Results: {CONFIG['results_dir']}\")\n",
    "print(f\"\\nTraining Config:\")\n",
    "print(f\"  Image Size: {CONFIG['image_size']}x{CONFIG['image_size']} (SAME AS CIPS-NET)\")\n",
    "print(f\"  Epochs: {CONFIG['num_epochs']} (Early Stopping: {CONFIG['early_stopping_patience']})\")\n",
    "print(f\"  Batch Size: {CONFIG['batch_size']}\")\n",
    "print(f\"  3-Fold CV on PanNuke\")\n",
    "print(f\"\\nText Variants (CIPS-Net Protocol):\")\n",
    "print(f\"  1. Per-Image Text: From annotations.csv 'instruction' column\")\n",
    "print(f\"  2. Common Text: '{COMMON_TEXT[:50]}...'\")\n",
    "print(f\"  3. No Text: Empty string\")\n",
    "print(f\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a5c4d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miglab/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA RTX A5000\n",
      "\n",
      "Available models:\n",
      "\n",
      "================================================================================\n",
      "Text-Guided Segmentation Models for Histopathology\n",
      "================================================================================\n",
      "\n",
      "#   Model           Venue                Description                             \n",
      "--------------------------------------------------------------------------------\n",
      "1   CLIPSeg         CVPR 2022            Uses CLIP features with FiLM condition..\n",
      "2   LSeg            ICLR 2022            Dense Prediction Transformer with CLIP..\n",
      "3   GroupViT        CVPR 2022            Hierarchical grouping mechanism with c..\n",
      "4   SAN             CVPR 2023            Side adapter network preserving CLIP c..\n",
      "5   FC-CLIP         NeurIPS 2023         Fully convolutional CLIP for dense pre..\n",
      "6   OVSeg           CVPR 2023            Mask-adapted CLIP with region-level cl..\n",
      "7   CAT-Seg         CVPR 2024            Cost aggregation with spatial semantic..\n",
      "8   SED             CVPR 2024            Simple encoder-decoder with category-g..\n",
      "9   MAFT+           ECCV 2024 Oral       Multi-modal adapters with cross-modal ..\n",
      "10  X-Decoder       CVPR 2023            Unified decoder supporting multiple vi..\n",
      "11  OpenSeeD        ICCV 2023            Unified framework with decoupled seman..\n",
      "12  ODISE           CVPR 2023            Leverages diffusion model features for..\n",
      "13  TagAlign        arXiv 2023           Tag-guided alignment with multi-granul..\n",
      "14  Semantic-SAM    ECCV 2024            Multi-granularity segmentation with se..\n",
      "--------------------------------------------------------------------------------\n",
      "Total: 14 models\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "WORKSPACE = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work\"\n",
    "sys.path.insert(0, WORKSPACE)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Clear cached imports\n",
    "modules_to_remove = [m for m in sys.modules.keys() if 'TextGuidedSegmentation' in m]\n",
    "for m in modules_to_remove:\n",
    "    del sys.modules[m]\n",
    "\n",
    "# Import our package\n",
    "from TextGuidedSegmentation import (\n",
    "    get_model,\n",
    "    list_models,\n",
    "    print_model_summary,\n",
    "    DEFAULT_TEXT_PROMPTS,\n",
    "    PanNukeDataset,\n",
    "    DiceLoss,\n",
    "    FocalLoss,\n",
    "    CombinedSegmentationLoss,\n",
    "    compute_iou,\n",
    "    compute_dice,\n",
    "    compute_f1,\n",
    "    ConfusionMatrix,\n",
    "    MetricTracker,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "print(f\"\\nAvailable models:\")\n",
    "print_model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2809462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results directory: /mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/results/groupvit_20260128_150847\n",
      "\n",
      "Class names: ['Neoplastic', 'Inflammatory', 'Connective_Soft_tissue', 'Dead', 'Epithelial']\n",
      "\n",
      "Training prompts: ['neoplastic cells', 'inflammatory cells', 'connective tissue cells', 'dead cells', 'epithelial cells']\n",
      "\n",
      "Text Variants for Evaluation (CIPS-Net Protocol):\n",
      "  1. Per-Image Text: From annotations.csv 'instruction' column\n",
      "  2. Common Text: 'Segment all Neoplastic, Inflammatory, Connective, Dead, and Epithelial cells in the image.'\n",
      "  3. No Text: '' (empty string)\n"
     ]
    }
   ],
   "source": [
    "# Create results directory structure\n",
    "import os\n",
    "results_dir = Path(CONFIG['results_dir'])\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sub-directories for different experiment phases\n",
    "(results_dir / \"models\").mkdir(exist_ok=True)           # Trained model checkpoints\n",
    "(results_dir / \"pannuke_3fold\").mkdir(exist_ok=True)    # 3-fold CV results\n",
    "(results_dir / \"text_variants\").mkdir(exist_ok=True)    # Text prompt evaluation\n",
    "(results_dir / \"zero_shot\").mkdir(exist_ok=True)        # Zero-shot evaluation\n",
    "(results_dir / \"fine_tuned\").mkdir(exist_ok=True)       # Fine-tuned model results\n",
    "\n",
    "print(\"Results directory:\", results_dir)\n",
    "print(f\"\\nClass names: {CLASS_NAMES}\")\n",
    "print(f\"\\nTraining prompts: {TRAINING_TEXT_PROMPTS}\")\n",
    "print(f\"\\nText Variants for Evaluation (CIPS-Net Protocol):\")\n",
    "print(f\"  1. Per-Image Text: From annotations.csv 'instruction' column\")\n",
    "print(f\"  2. Common Text: '{COMMON_TEXT}'\")\n",
    "print(f\"  3. No Text: '' (empty string)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2606346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset [train]: 5268 images\n",
      "Dataset [val]: 2633 images\n",
      "\n",
      "Verifying mask loading...\n",
      "Sample mask shape: torch.Size([224, 224])\n",
      "Sample mask unique values: tensor([-100,    1])\n",
      "  Class 1 (Inflammatory): 768 pixels\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "class SimplePanNukeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple PanNuke dataset loader.\n",
    "    \n",
    "    IMPORTANT: The masks are stored as separate per-class binary masks:\n",
    "    - {base_name}_channel_0_Neoplastic.png\n",
    "    - {base_name}_channel_1_Inflammatory.png\n",
    "    - {base_name}_channel_2_Connective_Soft_tissue.png\n",
    "    - {base_name}_channel_3_Dead.png\n",
    "    - {base_name}_channel_4_Epithelial.png\n",
    "    - {base_name}_channel_5_Background.png\n",
    "    \n",
    "    We need to combine them into a single mask with class indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class channel mapping\n",
    "    CHANNEL_NAMES = [\n",
    "        \"Neoplastic\",           # Channel 0 -> Class 0\n",
    "        \"Inflammatory\",          # Channel 1 -> Class 1\n",
    "        \"Connective_Soft_tissue\",# Channel 2 -> Class 2\n",
    "        \"Dead\",                  # Channel 3 -> Class 3\n",
    "        \"Epithelial\",           # Channel 4 -> Class 4\n",
    "        \"Background\",           # Channel 5 -> Background (not used)\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        mask_dir: str,\n",
    "        csv_file: str = None,\n",
    "        image_size: int = 224,\n",
    "        split: str = \"train\",\n",
    "        fold: int = 0,\n",
    "    ):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Get image files\n",
    "        self.image_files = sorted(list(self.image_dir.glob(\"*.png\")) + \n",
    "                                  list(self.image_dir.glob(\"*.jpg\")))\n",
    "        \n",
    "        # Simple split based on fold\n",
    "        n = len(self.image_files)\n",
    "        fold_size = n // 3\n",
    "        \n",
    "        if split == \"train\":\n",
    "            # Use 2 folds for training\n",
    "            indices = list(range(n))\n",
    "            test_start = fold * fold_size\n",
    "            test_end = (fold + 1) * fold_size if fold < 2 else n\n",
    "            indices = [i for i in indices if not (test_start <= i < test_end)]\n",
    "            self.image_files = [self.image_files[i] for i in indices]\n",
    "        elif split == \"val\":\n",
    "            # Use 1 fold for validation\n",
    "            test_start = fold * fold_size\n",
    "            test_end = (fold + 1) * fold_size if fold < 2 else n\n",
    "            self.image_files = self.image_files[test_start:test_end]\n",
    "        \n",
    "        # CLIP normalization\n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "            std=[0.26862954, 0.26130258, 0.27577711]\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset [{split}]: {len(self.image_files)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def _get_base_name(self, img_path: Path) -> str:\n",
    "        \"\"\"Extract base name from image path (remove _img suffix).\"\"\"\n",
    "        stem = img_path.stem\n",
    "        if stem.endswith(\"_img\"):\n",
    "            return stem[:-4]  # Remove \"_img\"\n",
    "        return stem\n",
    "    \n",
    "    def _load_combined_mask(self, base_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Load and combine per-channel masks into a single class mask.\n",
    "        \n",
    "        Returns mask with class indices 0-4 (ignoring background channel 5).\n",
    "        Priority: If multiple classes present at a pixel, use lowest class index.\n",
    "        \"\"\"\n",
    "        H, W = self.image_size, self.image_size\n",
    "        combined_mask = np.full((H, W), fill_value=255, dtype=np.uint8)  # 255 = ignore\n",
    "        \n",
    "        # Load each class channel (0-4, skip background)\n",
    "        for class_idx in range(5):  # Only 5 classes (0-4)\n",
    "            channel_name = self.CHANNEL_NAMES[class_idx]\n",
    "            mask_filename = f\"{base_name}_channel_{class_idx}_{channel_name}.png\"\n",
    "            mask_path = self.mask_dir / mask_filename\n",
    "            \n",
    "            if mask_path.exists():\n",
    "                channel_mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "                if channel_mask is not None:\n",
    "                    channel_mask = cv2.resize(channel_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "                    # Set pixels where this class is present (threshold > 127)\n",
    "                    class_pixels = channel_mask > 127\n",
    "                    # Only set if not already assigned (lower class index has priority)\n",
    "                    combined_mask[class_pixels & (combined_mask == 255)] = class_idx\n",
    "        \n",
    "        # Any remaining 255 (ignore) pixels that have no class -> set to background (class 0)\n",
    "        # Actually, let's check if there's a background channel\n",
    "        bg_mask_path = self.mask_dir / f\"{base_name}_channel_5_Background.png\"\n",
    "        if bg_mask_path.exists():\n",
    "            bg_mask = cv2.imread(str(bg_mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "            if bg_mask is not None:\n",
    "                bg_mask = cv2.resize(bg_mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "                # Background pixels remain as ignore_index or set to a specific value\n",
    "                # For now, keep them as 255 (ignore in loss)\n",
    "                pass\n",
    "        \n",
    "        # If still some unassigned pixels, they will be ignored (255)\n",
    "        # Convert 255 to -100 for ignore_index compatibility\n",
    "        combined_mask = combined_mask.astype(np.int64)\n",
    "        combined_mask[combined_mask == 255] = -100  # ignore_index\n",
    "        \n",
    "        return combined_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = image.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        # Load combined mask\n",
    "        base_name = self._get_base_name(img_path)\n",
    "        mask = self._load_combined_mask(base_name)\n",
    "        mask = torch.from_numpy(mask)\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"mask\": mask,\n",
    "            \"image_path\": str(img_path),\n",
    "        }\n",
    "\n",
    "# Create initial datasets for testing (fold 0)\n",
    "# The actual 3-fold CV will create datasets per fold in the training cell\n",
    "train_dataset = SimplePanNukeDataset(\n",
    "    image_dir=f\"{CONFIG['data_root']}/multi_images\",\n",
    "    mask_dir=f\"{CONFIG['data_root']}/multi_masks\",\n",
    "    image_size=CONFIG['image_size'],\n",
    "    split=\"train\",\n",
    "    fold=0,  # Default to fold 0 for initial testing\n",
    ")\n",
    "\n",
    "val_dataset = SimplePanNukeDataset(\n",
    "    image_dir=f\"{CONFIG['data_root']}/multi_images\",\n",
    "    mask_dir=f\"{CONFIG['data_root']}/multi_masks\",\n",
    "    image_size=CONFIG['image_size'],\n",
    "    split=\"val\",\n",
    "    fold=0,  # Default to fold 0 for initial testing\n",
    ")\n",
    "\n",
    "# Quick verification\n",
    "print(\"\\nVerifying mask loading...\")\n",
    "sample = train_dataset[0]\n",
    "sample_mask = sample['mask']\n",
    "print(f\"Sample mask shape: {sample_mask.shape}\")\n",
    "print(f\"Sample mask unique values: {torch.unique(sample_mask)}\")\n",
    "for i in range(5):\n",
    "    count = (sample_mask == i).sum().item()\n",
    "    if count > 0:\n",
    "        print(f\"  Class {i} ({CLASS_NAMES[i]}): {count} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5915c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 658\n",
      "Val batches: 330\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 to avoid multiprocessing warnings\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Set to 0 to avoid multiprocessing warnings\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236b596e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating model: groupvit\n",
      "Total parameters: 175,941,762\n",
      "Trainable parameters: 26,321,025\n",
      "Frozen parameters: 149,620,737\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "print(f\"\\nCreating model: {MODEL_NAME}\")\n",
    "\n",
    "model = get_model(\n",
    "    MODEL_NAME,\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    image_size=CONFIG['image_size'],\n",
    "    clip_model=CONFIG['clip_model'],\n",
    "    freeze_clip=CONFIG['freeze_clip'],\n",
    "    device=CONFIG['device'],\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504cf3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete!\n",
      "Early stopping patience: 8 epochs\n"
     ]
    }
   ],
   "source": [
    "# Setup training\n",
    "\n",
    "# Loss function\n",
    "criterion = CombinedSegmentationLoss(\n",
    "    ce_weight=1.0,\n",
    "    dice_weight=1.0,\n",
    "    focal_weight=0.0,\n",
    "    ignore_index=-100,\n",
    ")\n",
    "\n",
    "# Optimizer (only trainable parameters)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=CONFIG['num_epochs'],\n",
    "    eta_min=1e-6,\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Early stopping config\n",
    "EARLY_STOPPING_PATIENCE = 8\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8430b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, scaler, text_prompts, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(images, text_prompts)\n",
    "            logits = outputs['logits']\n",
    "            loss = criterion(logits, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, text_prompts, device, num_classes=5):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    confusion = torch.zeros(num_classes, num_classes, device=device)\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Validation\")\n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        outputs = model(images, text_prompts)\n",
    "        logits = outputs['logits']\n",
    "        loss = criterion(logits, masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        preds = logits.argmax(dim=1).flatten()\n",
    "        targets = masks.flatten()\n",
    "        valid_mask = (targets >= 0) & (targets < num_classes)\n",
    "        preds = preds[valid_mask]\n",
    "        targets = targets[valid_mask]\n",
    "        \n",
    "        for p, t in zip(preds, targets):\n",
    "            confusion[t, p] += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    ious, dices = [], []\n",
    "    for c in range(num_classes):\n",
    "        tp = confusion[c, c]\n",
    "        fp = confusion[:, c].sum() - tp\n",
    "        fn = confusion[c, :].sum() - tp\n",
    "        ious.append((tp / (tp + fp + fn + 1e-8)).item())\n",
    "        dices.append((2 * tp / (2 * tp + fp + fn + 1e-8)).item())\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'mean_iou': np.mean(ious),\n",
    "        'mean_dice': np.mean(dices),\n",
    "        'per_class_iou': ious,\n",
    "        'per_class_dice': dices,\n",
    "    }\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "230a852f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: 3-FOLD CROSS-VALIDATION TRAINING ON PANNUKE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "FOLD 1/3\n",
      "======================================================================\n",
      "Dataset [train]: 5268 images\n",
      "Dataset [val]: 2633 images\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a29258f5d64287ab58eba37b2bac50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3fd716a1684208a731a27949c6eb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 2.4642 | Val Loss: 2.4907\n",
      "  Val mIoU: 0.1228 | Val mDice: 0.1749\n",
      "  ✓ Saved best model (mIoU: 0.1228)\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6325fc5ecaff4913a0ee4d2458637f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc4ed9389574dddaabffeb2aeae7f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: nan | Val Loss: nan\n",
      "  Val mIoU: 0.1052 | Val mDice: 0.1379\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4bd2447b5c443386b2934fa158be3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f21c96526442ae838426e6b1c0e29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 76\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(\n\u001b[1;32m     72\u001b[0m     model, train_loader, optimizer, criterion, scaler,\n\u001b[1;32m     73\u001b[0m     TEXT_PROMPTS, CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     74\u001b[0m )\n\u001b[0;32m---> 76\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEXT_PROMPTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_classes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     83\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 59\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, loader, criterion, text_prompts, device, num_classes)\u001b[0m\n\u001b[1;32m     56\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets[valid_mask]\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(preds, targets):\n\u001b[0;32m---> 59\u001b[0m         confusion[t, p] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     61\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m num_batches\n\u001b[1;32m     63\u001b[0m ious, dices \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PHASE 1: 3-FOLD CROSS-VALIDATION TRAINING ON PANNUKE\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: 3-FOLD CROSS-VALIDATION TRAINING ON PANNUKE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_fold_results = {}\n",
    "\n",
    "for fold in range(CONFIG['num_folds']):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FOLD {fold + 1}/{CONFIG['num_folds']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create datasets for this fold\n",
    "    train_dataset = SimplePanNukeDataset(\n",
    "        image_dir=f\"{CONFIG['data_root']}/multi_images\",\n",
    "        mask_dir=f\"{CONFIG['data_root']}/multi_masks\",\n",
    "        image_size=CONFIG['image_size'],\n",
    "        split=\"train\",\n",
    "        fold=fold,\n",
    "    )\n",
    "    \n",
    "    val_dataset = SimplePanNukeDataset(\n",
    "        image_dir=f\"{CONFIG['data_root']}/multi_images\",\n",
    "        mask_dir=f\"{CONFIG['data_root']}/multi_masks\",\n",
    "        image_size=CONFIG['image_size'],\n",
    "        split=\"val\",\n",
    "        fold=fold,\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "        num_workers=0, pin_memory=True, drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
    "        num_workers=0, pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    # Create fresh model for each fold\n",
    "    model = get_model(\n",
    "        MODEL_NAME,\n",
    "        num_classes=CONFIG['num_classes'],\n",
    "        image_size=CONFIG['image_size'],\n",
    "        clip_model=CONFIG['clip_model'],\n",
    "        freeze_clip=CONFIG['freeze_clip'],\n",
    "        device=CONFIG['device'],\n",
    "    )\n",
    "    \n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay'],\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=CONFIG['num_epochs'], eta_min=1e-6,\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    \n",
    "    # Training loop\n",
    "    best_metric = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        \n",
    "        train_loss = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, scaler,\n",
    "            TEXT_PROMPTS, CONFIG['device']\n",
    "        )\n",
    "        \n",
    "        val_metrics = validate(\n",
    "            model, val_loader, criterion, TEXT_PROMPTS,\n",
    "            CONFIG['device'], CONFIG['num_classes']\n",
    "        )\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_iou'].append(val_metrics['mean_iou'])\n",
    "        history['val_dice'].append(val_metrics['mean_dice'])\n",
    "        \n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f}\")\n",
    "        print(f\"  Val mIoU: {val_metrics['mean_iou']:.4f} | Val mDice: {val_metrics['mean_dice']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['mean_iou'] > best_metric:\n",
    "            best_metric = val_metrics['mean_iou']\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "            checkpoint_path = results_dir / \"models\" / f\"best_{MODEL_NAME}_fold{fold}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_metric': best_metric,\n",
    "                'val_metrics': val_metrics,\n",
    "                'config': CONFIG,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  ✓ Saved best model (mIoU: {best_metric:.4f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= CONFIG['early_stopping_patience']:\n",
    "                print(f\"\\n⚠️ Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Store fold results\n",
    "    all_fold_results[f'fold_{fold}'] = {\n",
    "        'best_iou': best_metric,\n",
    "        'final_dice': history['val_dice'][-1] if history['val_dice'] else 0,\n",
    "        'history': dict(history),\n",
    "        'epochs_trained': len(history['train_loss']),\n",
    "    }\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del model, optimizer, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nFold {fold} complete! Best mIoU: {best_metric:.4f}\")\n",
    "\n",
    "# Aggregate and save 3-fold CV results\n",
    "mean_iou = np.mean([r['best_iou'] for r in all_fold_results.values()])\n",
    "std_iou = np.std([r['best_iou'] for r in all_fold_results.values()])\n",
    "mean_dice = np.mean([r['final_dice'] for r in all_fold_results.values()])\n",
    "std_dice = np.std([r['final_dice'] for r in all_fold_results.values()])\n",
    "\n",
    "cv_summary = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'experiment_name': EXPERIMENT_NAME,\n",
    "    'mean_iou': mean_iou,\n",
    "    'std_iou': std_iou,\n",
    "    'mean_dice': mean_dice,\n",
    "    'std_dice': std_dice,\n",
    "    'fold_results': all_fold_results,\n",
    "    'config': CONFIG,\n",
    "}\n",
    "\n",
    "cv_results_path = results_dir / \"pannuke_3fold\" / \"cv_results.json\"\n",
    "with open(cv_results_path, 'w') as f:\n",
    "    json.dump(cv_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3-FOLD CV TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Mean mIoU: {mean_iou:.4f} ± {std_iou:.4f}\")\n",
    "print(f\"Mean mDice: {mean_dice:.4f} ± {std_dice:.4f}\")\n",
    "print(f\"Results saved to: {cv_results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 1.5: POST-HOC mPQ EVALUATION ON PANNUKE 3-FOLD\n",
      "======================================================================\n",
      "\n",
      "Evaluating fold 0 with saved best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c63d736f5440e7b8872106f278627a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fold 0 Test Eval:   0%|          | 0/2656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 0: mDice=0.6905, mPQ=0.4993\n",
      "\n",
      "Evaluating fold 1 with saved best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b83cd92936f402cbf053181e4efff39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fold 1 Test Eval:   0%|          | 0/2523 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1: mDice=0.6806, mPQ=0.4897\n",
      "\n",
      "Evaluating fold 2 with saved best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f7dbddf1284976bcda5e34e37dd7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fold 2 Test Eval:   0%|          | 0/2722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2: mDice=0.6882, mPQ=0.4969\n",
      "\n",
      "✓ Saved PQ summary to: /mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/results/lseg_20260127_165728/pannuke_3fold/pq_results.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PHASE 1.5: POST-HOC EVALUATION - mPQ ON PANNUKE TEST FOLDS\n",
    "# ============================================================================\n",
    "# Loads the best model from each fold and evaluates mPQ (and mDice) on the\n",
    "# corresponding test fold using the same PQ implementation as Phase 2.\n",
    "# No retraining performed — pure post-hoc evaluation of saved checkpoints.\n",
    "# ============================================================================\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 1.5: POST-HOC mPQ EVALUATION ON PANNUKE 3-FOLD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pq_summary = {}\n",
    "\n",
    "# Define metrics functions (same as Phase 2)\n",
    "def compute_dice_per_class(pred_mask, gt_mask, num_classes):\n",
    "    \"\"\"Compute per-class Dice score.\"\"\"\n",
    "    dice_scores = {}\n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_mask == c).float()\n",
    "        gt_c = (gt_mask == c).float()\n",
    "        intersection = (pred_c * gt_c).sum()\n",
    "        union = pred_c.sum() + gt_c.sum()\n",
    "        if union > 0:\n",
    "            dice = (2 * intersection / (union + 1e-8)).item()\n",
    "        else:\n",
    "            dice = 1.0 if intersection == 0 else 0.0\n",
    "        dice_scores[c] = dice\n",
    "    return dice_scores\n",
    "\n",
    "def compute_pq_per_class(pred_semantic, gt_semantic, gt_instance, gt_class_map, num_classes):\n",
    "    \"\"\"\n",
    "    Compute Panoptic Quality per class (SAME AS CIPS-NET implementation).\n",
    "    Returns dict per class with PQ, DQ, SQ and counts.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for c in range(num_classes):\n",
    "        gt_mask_c = (gt_semantic == c)\n",
    "        pred_mask_c = (pred_semantic == c)\n",
    "        gt_instances_c = np.unique(gt_instance[gt_mask_c.cpu().numpy()])\n",
    "        gt_instances_c = gt_instances_c[gt_instances_c > 0]\n",
    "        pred_labeled, num_pred = ndimage.label(pred_mask_c.cpu().numpy())\n",
    "        pred_instances_c = list(range(1, num_pred + 1))\n",
    "        if len(gt_instances_c) == 0 and len(pred_instances_c) == 0:\n",
    "            results[c] = {'PQ': 1.0, 'DQ': 1.0, 'SQ': 1.0, 'TP': 0, 'FP': 0, 'FN': 0}\n",
    "            continue\n",
    "        if len(gt_instances_c) == 0:\n",
    "            results[c] = {'PQ': 0.0, 'DQ': 0.0, 'SQ': 0.0, 'TP': 0, 'FP': len(pred_instances_c), 'FN': 0}\n",
    "            continue\n",
    "        if len(pred_instances_c) == 0:\n",
    "            results[c] = {'PQ': 0.0, 'DQ': 0.0, 'SQ': 0.0, 'TP': 0, 'FP': 0, 'FN': len(gt_instances_c)}\n",
    "            continue\n",
    "        iou_matrix = np.zeros((len(gt_instances_c), len(pred_instances_c)))\n",
    "        for i, gt_id in enumerate(gt_instances_c):\n",
    "            gt_inst_mask = (gt_instance.numpy() == gt_id)\n",
    "            for j, pred_id in enumerate(pred_instances_c):\n",
    "                pred_inst_mask = (pred_labeled == pred_id)\n",
    "                intersection = np.logical_and(gt_inst_mask, pred_inst_mask).sum()\n",
    "                union = np.logical_or(gt_inst_mask, pred_inst_mask).sum()\n",
    "                if union > 0:\n",
    "                    iou_matrix[i, j] = intersection / union\n",
    "        row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
    "        tp = 0\n",
    "        matched_iou_sum = 0\n",
    "        matched_gt = set()\n",
    "        matched_pred = set()\n",
    "        for r, c_idx in zip(row_ind, col_ind):\n",
    "            if iou_matrix[r, c_idx] > 0.5:\n",
    "                tp += 1\n",
    "                matched_iou_sum += iou_matrix[r, c_idx]\n",
    "                matched_gt.add(r)\n",
    "                matched_pred.add(c_idx)\n",
    "        fp = len(pred_instances_c) - len(matched_pred)\n",
    "        fn = len(gt_instances_c) - len(matched_gt)\n",
    "        if tp > 0:\n",
    "            sq = matched_iou_sum / tp\n",
    "            dq = tp / (tp + 0.5 * fp + 0.5 * fn)\n",
    "            pq = sq * dq\n",
    "        else:\n",
    "            sq = 0.0; dq = 0.0; pq = 0.0\n",
    "        results[c] = {'PQ': pq, 'DQ': dq, 'SQ': sq, 'TP': tp, 'FP': fp, 'FN': fn}\n",
    "    return results\n",
    "\n",
    "# Load annotations.csv from PanNuke_Preprocess\n",
    "ann_path = Path(CONFIG['pannuke_preprocess_root']) / 'annotations.csv'\n",
    "if not ann_path.exists():\n",
    "    print(f\"⚠️ annotations.csv not found at {ann_path}. Cannot run PQ evaluation.\")\n",
    "else:\n",
    "    ann_df = pd.read_csv(ann_path)\n",
    "    for fold in range(CONFIG['num_folds']):\n",
    "        print(f\"\\nEvaluating fold {fold} with saved best model...\")\n",
    "        ckpt = results_dir / 'models' / f\"best_{MODEL_NAME}_fold{fold}.pth\"\n",
    "        if not ckpt.exists():\n",
    "            print(f\"  ⚠️ Checkpoint not found: {ckpt} — skipping fold {fold}\")\n",
    "            continue\n",
    "        # Load model weights\n",
    "        fold_model = get_model(\n",
    "            MODEL_NAME,\n",
    "            num_classes=CONFIG['num_classes'],\n",
    "            image_size=CONFIG['image_size'],\n",
    "            clip_model=CONFIG['clip_model'],\n",
    "            freeze_clip=True,\n",
    "            device=CONFIG['device'],\n",
    "        )\n",
    "        cp = torch.load(ckpt, map_location=CONFIG['device'], weights_only=False)\n",
    "        fold_model.load_state_dict(cp['model_state_dict'])\n",
    "        fold_model.eval()\n",
    "        \n",
    "        fold_rows = ann_df[ann_df['fold'] == (fold+1) if 'fold' in ann_df.columns and ann_df['fold'].max()>=1 else ann_df['fold'] == fold]\n",
    "        # Some preprocess use 1-indexed folds, handle both: try fold, then fold+1\n",
    "        if len(fold_rows) == 0:\n",
    "            fold_rows = ann_df[ann_df['fold'] == fold]\n",
    "        if len(fold_rows) == 0:\n",
    "            print(f\"  ⚠️ No annotations found for fold {fold}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        all_dice = {c: [] for c in range(CONFIG['num_classes'])}\n",
    "        all_pq = {c: [] for c in range(CONFIG['num_classes'])}\n",
    "        normalize = transforms.Normalize(mean=[0.48145466,0.4578275,0.40821073], std=[0.26862954,0.26130258,0.27577711])\n",
    "        \n",
    "        for idx, row in tqdm(fold_rows.reset_index(drop=True).iterrows(), total=len(fold_rows), desc=f\"Fold {fold} Test Eval\"):\n",
    "            image_id = row['image_id']\n",
    "            img_path = Path(CONFIG['pannuke_preprocess_root']) / 'images' / f'fold{row[\"fold\"]}' / f'{image_id}.png'\n",
    "            mask_path = Path(CONFIG['pannuke_preprocess_root']) / 'masks' / f'fold{row[\"fold\"]}' / f'{image_id}.npz'\n",
    "            if not img_path.exists() or not mask_path.exists():\n",
    "                continue\n",
    "            image = Image.open(img_path).convert('RGB').resize((CONFIG['image_size'], CONFIG['image_size']), Image.BILINEAR)\n",
    "            img_t = transforms.ToTensor()(image)\n",
    "            img_t = normalize(img_t).unsqueeze(0).to(CONFIG['device'])\n",
    "            mask_data = np.load(mask_path)\n",
    "            masks = mask_data['masks']\n",
    "            gt_semantic = np.zeros((masks.shape[0], masks.shape[1]), dtype=np.int64)\n",
    "            for c in range(min(masks.shape[2], CONFIG['num_classes'])):\n",
    "                gt_semantic[masks[:,:,c] > 0] = c\n",
    "            gt_instance = np.zeros((masks.shape[0], masks.shape[1]), dtype=np.int32)\n",
    "            inst_id = 1\n",
    "            gt_class_map = {}\n",
    "            for c in range(min(masks.shape[2], CONFIG['num_classes'])):\n",
    "                binary_mask = masks[:,:,c] > 0\n",
    "                if binary_mask.sum() > 0:\n",
    "                    labeled, ninst = ndimage.label(binary_mask)\n",
    "                    for i in range(1, ninst+1):\n",
    "                        gt_instance[labeled == i] = inst_id\n",
    "                        gt_class_map[inst_id] = c\n",
    "                        inst_id += 1\n",
    "            # resize masks\n",
    "            gt_semantic = torch.from_numpy(np.array(Image.fromarray(gt_semantic.astype(np.uint8)).resize((CONFIG['image_size'],CONFIG['image_size']), Image.NEAREST))).long()\n",
    "            gt_instance = torch.from_numpy(np.array(Image.fromarray(gt_instance.astype(np.int32)).resize((CONFIG['image_size'],CONFIG['image_size']), Image.NEAREST)))\n",
    "            \n",
    "            # forward with per-image instruction from annotations\n",
    "            text_instruction = row['instruction'] if pd.notna(row['instruction']) else \"Segment all tissue types.\"\n",
    "            text_prompts = [text_instruction] * CONFIG['num_classes']\n",
    "            with torch.no_grad():\n",
    "                out = fold_model(img_t, text_prompts)\n",
    "                logits = out['logits']\n",
    "                pred_semantic = logits.argmax(dim=1).squeeze(0).cpu()\n",
    "            # metrics\n",
    "            dice_scores = compute_dice_per_class(pred_semantic, gt_semantic, CONFIG['num_classes'])\n",
    "            pq_scores = compute_pq_per_class(pred_semantic, gt_semantic, gt_instance, gt_class_map, CONFIG['num_classes'])\n",
    "            for c in range(CONFIG['num_classes']):\n",
    "                all_dice[c].append(dice_scores[c])\n",
    "                all_pq[c].append(pq_scores[c]['PQ'])\n",
    "        # aggregate\n",
    "        per_class_dice = [np.mean(all_dice[c]) if len(all_dice[c])>0 else 0.0 for c in range(CONFIG['num_classes'])]\n",
    "        per_class_pq = [np.mean(all_pq[c]) if len(all_pq[c])>0 else 0.0 for c in range(CONFIG['num_classes'])]\n",
    "        fold_result = {\n",
    "            'fold': fold,\n",
    "            'num_images': len(fold_rows),\n",
    "            'per_class_dice': per_class_dice,\n",
    "            'per_class_pq': per_class_pq,\n",
    "            'mean_dice': float(np.mean(per_class_dice)),\n",
    "            'mean_pq': float(np.mean(per_class_pq)),\n",
    "        }\n",
    "        pq_summary[f'fold_{fold}'] = fold_result\n",
    "        print(f\"  Fold {fold}: mDice={fold_result['mean_dice']:.4f}, mPQ={fold_result['mean_pq']:.4f}\")\n",
    "\n",
    "    # Save PQ summary\n",
    "    pq_path = results_dir / 'pannuke_3fold' / 'pq_results.json'\n",
    "    with open(pq_path, 'w') as f:\n",
    "        json.dump(pq_summary, f, indent=2)\n",
    "    print(f\"\\n✓ Saved PQ summary to: {pq_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bffb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6684d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 2: TEXT VARIANTS EVALUATION (CIPS-NET PROTOCOL)\n",
      "======================================================================\n",
      "✓ Loaded best model from fold 0 (epoch 7)\n",
      "✓ Loaded annotations.csv: 7901 entries\n",
      "✓ Metrics functions defined (mDice + mPQ)\n",
      "\n",
      "============================================================\n",
      "Variant 1: Per-Image Text (from annotations.csv)\n",
      "============================================================\n",
      "\n",
      "  Testing on fold 1: 2656 images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1646233362746f884970d0625a3e039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Per-Image Text:   0%|          | 0/2656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mDice: 0.6905\n",
      "  mPQ:   0.4993\n",
      "\n",
      "============================================================\n",
      "Variant 2: Common Text\n",
      "  Text: 'Segment all Neoplastic, Inflammatory, Connective, Dead, and Epithelial cells in the image.'\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf88afea66454d55a9d4e5e8e1819643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Common Text:   0%|          | 0/2656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mDice: 0.6905\n",
      "  mPQ:   0.4993\n",
      "\n",
      "============================================================\n",
      "Variant 3: No Text (empty string)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4207180f6be4bd3950d73e10c0fe7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "No Text:   0%|          | 0/2656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mDice: 0.6905\n",
      "  mPQ:   0.4993\n",
      "\n",
      "======================================================================\n",
      "PANNUKE TEXT VARIANT RESULTS (CIPS-Net Format)\n",
      "======================================================================\n",
      "Dataset      Variant            Setting      mDice        mPQ         \n",
      "------------------------------------------------------------------\n",
      "PanNuke      Per-Image Text     Zero-Shot    0.6905       0.4993      \n",
      "PanNuke      Common Text        Zero-Shot    0.6905       0.4993      \n",
      "PanNuke      No Text            Zero-Shot    0.6905       0.4993      \n",
      "------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "COMPARISON WITH CIPS-NET\n",
      "======================================================================\n",
      "Variant            CIPS-Net mDice   CIPS-Net mPQ   clipseg mDice    clipseg mPQ   \n",
      "------------------------------------------------------------------------------\n",
      "Per-Image Text     0.7661           0.5356         0.6905           0.4993        \n",
      "Common Text        0.3558           0.1091         0.6905           0.4993        \n",
      "No Text            0.691            0.4998         0.6905           0.4993        \n",
      "\n",
      "✓ Text variant results saved to: /mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/results/clipseg_20260126_145532/text_variants/pannuke_text_variants.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PHASE 2: TEXT VARIANTS EVALUATION (CIPS-NET PROTOCOL - WITH mPQ)\n",
    "# ============================================================================\n",
    "# Evaluates 3 text variants on PanNuke test set (same as CIPS-Net):\n",
    "#   1. Per-Image Text: Read from annotations.csv 'instruction' column\n",
    "#   2. Common Text: Fixed sentence for ALL images\n",
    "#   3. No Text: Empty string (pure visual features)\n",
    "#\n",
    "# Metrics: mDice, mPQ (same as CIPS-Net)\n",
    "# ============================================================================\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 2: TEXT VARIANTS EVALUATION (CIPS-NET PROTOCOL)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load best model from fold 0\n",
    "model = get_model(\n",
    "    MODEL_NAME,\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    image_size=CONFIG['image_size'],\n",
    "    clip_model=CONFIG['clip_model'],\n",
    "    freeze_clip=True,\n",
    "    device=CONFIG['device'],\n",
    ")\n",
    "\n",
    "checkpoint_path = results_dir / \"models\" / f\"best_{MODEL_NAME}_fold0.pth\"\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=CONFIG['device'], weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✓ Loaded best model from fold 0 (epoch {checkpoint['epoch']})\")\n",
    "else:\n",
    "    print(\"⚠️ No checkpoint found for fold 0, using current model\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load annotations.csv for per-image text instructions\n",
    "# ============================================================================\n",
    "annotations_path = Path(CONFIG['pannuke_preprocess_root']) / \"annotations.csv\"\n",
    "if annotations_path.exists():\n",
    "    annotations_df = pd.read_csv(annotations_path)\n",
    "    print(f\"✓ Loaded annotations.csv: {len(annotations_df)} entries\")\n",
    "else:\n",
    "    print(f\"⚠️ annotations.csv not found at {annotations_path}\")\n",
    "    annotations_df = None\n",
    "\n",
    "# ============================================================================\n",
    "# Metrics Functions (same as CIPS-Net)\n",
    "# ============================================================================\n",
    "def compute_dice_per_class(pred_mask, gt_mask, num_classes):\n",
    "    \"\"\"Compute per-class Dice score.\"\"\"\n",
    "    dice_scores = {}\n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_mask == c).float()\n",
    "        gt_c = (gt_mask == c).float()\n",
    "        \n",
    "        intersection = (pred_c * gt_c).sum()\n",
    "        union = pred_c.sum() + gt_c.sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            dice = (2 * intersection / (union + 1e-8)).item()\n",
    "        else:\n",
    "            dice = 1.0 if intersection == 0 else 0.0\n",
    "        \n",
    "        dice_scores[c] = dice\n",
    "    return dice_scores\n",
    "\n",
    "\n",
    "def compute_pq_per_class(pred_semantic, gt_semantic, gt_instance, gt_class_map, num_classes):\n",
    "    \"\"\"\n",
    "    Compute Panoptic Quality per class (SAME AS CIPS-NET).\n",
    "    \n",
    "    PQ = SQ × DQ\n",
    "    - SQ (Segmentation Quality) = average IoU of matched pairs\n",
    "    - DQ (Detection Quality) = TP / (TP + 0.5*FP + 0.5*FN)\n",
    "    \n",
    "    Predicted instances are created from connected components of semantic output.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        # Get masks for this class\n",
    "        gt_mask_c = (gt_semantic == c)\n",
    "        pred_mask_c = (pred_semantic == c)\n",
    "        \n",
    "        # Get GT instances of this class\n",
    "        gt_instances_c = np.unique(gt_instance[gt_mask_c.cpu().numpy()])\n",
    "        gt_instances_c = gt_instances_c[gt_instances_c > 0]\n",
    "        \n",
    "        # Create predicted instances from connected components (same as CIPS-Net)\n",
    "        pred_labeled, num_pred = ndimage.label(pred_mask_c.cpu().numpy())\n",
    "        pred_instances_c = list(range(1, num_pred + 1))\n",
    "        \n",
    "        if len(gt_instances_c) == 0 and len(pred_instances_c) == 0:\n",
    "            results[c] = {'PQ': 1.0, 'DQ': 1.0, 'SQ': 1.0, 'TP': 0, 'FP': 0, 'FN': 0}\n",
    "            continue\n",
    "        \n",
    "        if len(gt_instances_c) == 0:\n",
    "            results[c] = {'PQ': 0.0, 'DQ': 0.0, 'SQ': 0.0, 'TP': 0, 'FP': len(pred_instances_c), 'FN': 0}\n",
    "            continue\n",
    "        \n",
    "        if len(pred_instances_c) == 0:\n",
    "            results[c] = {'PQ': 0.0, 'DQ': 0.0, 'SQ': 0.0, 'TP': 0, 'FP': 0, 'FN': len(gt_instances_c)}\n",
    "            continue\n",
    "        \n",
    "        # Compute IoU matrix between GT and predicted instances\n",
    "        iou_matrix = np.zeros((len(gt_instances_c), len(pred_instances_c)))\n",
    "        \n",
    "        for i, gt_id in enumerate(gt_instances_c):\n",
    "            gt_inst_mask = (gt_instance.numpy() == gt_id)\n",
    "            for j, pred_id in enumerate(pred_instances_c):\n",
    "                pred_inst_mask = (pred_labeled == pred_id)\n",
    "                \n",
    "                intersection = np.logical_and(gt_inst_mask, pred_inst_mask).sum()\n",
    "                union = np.logical_or(gt_inst_mask, pred_inst_mask).sum()\n",
    "                \n",
    "                if union > 0:\n",
    "                    iou_matrix[i, j] = intersection / union\n",
    "        \n",
    "        # Hungarian matching with IoU > 0.5 threshold\n",
    "        row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
    "        \n",
    "        tp = 0\n",
    "        matched_iou_sum = 0\n",
    "        matched_gt = set()\n",
    "        matched_pred = set()\n",
    "        \n",
    "        for r, c_idx in zip(row_ind, col_ind):\n",
    "            if iou_matrix[r, c_idx] > 0.5:\n",
    "                tp += 1\n",
    "                matched_iou_sum += iou_matrix[r, c_idx]\n",
    "                matched_gt.add(r)\n",
    "                matched_pred.add(c_idx)\n",
    "        \n",
    "        fp = len(pred_instances_c) - len(matched_pred)\n",
    "        fn = len(gt_instances_c) - len(matched_gt)\n",
    "        \n",
    "        # Compute PQ, DQ, SQ\n",
    "        if tp > 0:\n",
    "            sq = matched_iou_sum / tp\n",
    "            dq = tp / (tp + 0.5 * fp + 0.5 * fn)\n",
    "            pq = sq * dq\n",
    "        else:\n",
    "            sq = 0.0\n",
    "            dq = 0.0\n",
    "            pq = 0.0\n",
    "        \n",
    "        results[c] = {'PQ': pq, 'DQ': dq, 'SQ': sq, 'TP': tp, 'FP': fp, 'FN': fn}\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Metrics functions defined (mDice + mPQ)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation function for per-image text (like CIPS-Net)\n",
    "# ============================================================================\n",
    "@torch.no_grad()\n",
    "def evaluate_per_image_text(model, annotations_df, fold, data_root, img_size, device, num_classes=5):\n",
    "    \"\"\"\n",
    "    Evaluate with per-image text from annotations.csv (CIPS-Net protocol).\n",
    "    Returns: mDice, mPQ (same metrics as CIPS-Net)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fold_df = annotations_df[annotations_df['fold'] == fold].reset_index(drop=True)\n",
    "    print(f\"\\n  Testing on fold {fold}: {len(fold_df)} images\")\n",
    "    \n",
    "    # Accumulators for per-class metrics\n",
    "    all_dice = {c: [] for c in range(num_classes)}\n",
    "    all_pq = {c: [] for c in range(num_classes)}\n",
    "    \n",
    "    # CLIP normalization\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "        std=[0.26862954, 0.26130258, 0.27577711]\n",
    "    )\n",
    "    \n",
    "    for idx in tqdm(range(len(fold_df)), desc=\"Per-Image Text\"):\n",
    "        row = fold_df.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        \n",
    "        # Get per-image instruction from annotations.csv\n",
    "        text_instruction = row['instruction'] if pd.notna(row['instruction']) else \"Segment all tissue types.\"\n",
    "        \n",
    "        # Load image\n",
    "        img_path = Path(data_root) / 'images' / f'fold{fold}' / f'{image_id}.png'\n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "            \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = image.resize((img_size, img_size), Image.BILINEAR)\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "        image_tensor = normalize(image_tensor).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Load mask (npz with per-class binary masks)\n",
    "        mask_path = Path(data_root) / 'masks' / f'fold{fold}' / f'{image_id}.npz'\n",
    "        if not mask_path.exists():\n",
    "            continue\n",
    "            \n",
    "        mask_data = np.load(mask_path)\n",
    "        masks = mask_data['masks']  # [H, W, num_classes]\n",
    "        \n",
    "        # Create semantic mask\n",
    "        gt_semantic = np.zeros((masks.shape[0], masks.shape[1]), dtype=np.int64)\n",
    "        for c in range(min(masks.shape[2], num_classes)):\n",
    "            gt_semantic[masks[:, :, c] > 0] = c\n",
    "        \n",
    "        # Create instance mask (for PQ calculation)\n",
    "        gt_instance = np.zeros((masks.shape[0], masks.shape[1]), dtype=np.int32)\n",
    "        gt_class_map = {}\n",
    "        inst_id = 1\n",
    "        for c in range(min(masks.shape[2], num_classes)):\n",
    "            binary_mask = masks[:, :, c] > 0\n",
    "            if binary_mask.sum() > 0:\n",
    "                labeled, num_instances = ndimage.label(binary_mask)\n",
    "                for i in range(1, num_instances + 1):\n",
    "                    gt_instance[labeled == i] = inst_id\n",
    "                    gt_class_map[inst_id] = c\n",
    "                    inst_id += 1\n",
    "        \n",
    "        # Resize masks\n",
    "        gt_semantic = np.array(Image.fromarray(gt_semantic.astype(np.uint8)).resize(\n",
    "            (img_size, img_size), Image.NEAREST))\n",
    "        gt_instance = np.array(Image.fromarray(gt_instance.astype(np.int32)).resize(\n",
    "            (img_size, img_size), Image.NEAREST))\n",
    "        \n",
    "        gt_semantic = torch.from_numpy(gt_semantic).long()\n",
    "        gt_instance = torch.from_numpy(gt_instance)\n",
    "        \n",
    "        # Forward pass with per-image text\n",
    "        text_prompts = [text_instruction] * num_classes\n",
    "        outputs = model(image_tensor, text_prompts)\n",
    "        logits = outputs['logits']\n",
    "        pred_semantic = logits.argmax(dim=1).squeeze(0).cpu()\n",
    "        \n",
    "        # Compute Dice per class\n",
    "        dice_scores = compute_dice_per_class(pred_semantic, gt_semantic, num_classes)\n",
    "        for c in range(num_classes):\n",
    "            all_dice[c].append(dice_scores[c])\n",
    "        \n",
    "        # Compute PQ per class\n",
    "        pq_scores = compute_pq_per_class(pred_semantic, gt_semantic, gt_instance, gt_class_map, num_classes)\n",
    "        for c in range(num_classes):\n",
    "            all_pq[c].append(pq_scores[c]['PQ'])\n",
    "    \n",
    "    # Aggregate results\n",
    "    dice_macro = [np.mean(all_dice[c]) for c in range(num_classes)]\n",
    "    pq_macro = [np.mean(all_pq[c]) for c in range(num_classes)]\n",
    "    \n",
    "    return {\n",
    "        'mean_dice': np.mean(dice_macro),\n",
    "        'mean_pq': np.mean(pq_macro),\n",
    "        'per_class_dice': dice_macro,\n",
    "        'per_class_pq': pq_macro,\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation function for fixed text (Common Text / No Text)\n",
    "# ============================================================================\n",
    "@torch.no_grad()\n",
    "def evaluate_fixed_text(model, annotations_df, fold, data_root, img_size, device, text_instruction, num_classes=5):\n",
    "    \"\"\"\n",
    "    Evaluate with fixed text instruction for all images.\n",
    "    Returns: mDice, mPQ (same metrics as CIPS-Net)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fold_df = annotations_df[annotations_df['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    all_dice = {c: [] for c in range(num_classes)}\n",
    "    all_pq = {c: [] for c in range(num_classes)}\n",
    "    \n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "        std=[0.26862954, 0.26130258, 0.27577711]\n",
    "    )\n",
    "    \n",
    "    variant_name = \"Common Text\" if text_instruction else \"No Text\"\n",
    "    \n",
    "    for idx in tqdm(range(len(fold_df)), desc=variant_name):\n",
    "        row = fold_df.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        \n",
    "        img_path = Path(data_root) / 'images' / f'fold{fold}' / f'{image_id}.png'\n",
    "        if not img_path.exists():\n",
    "            continue\n",
    "            \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = image.resize((img_size, img_size), Image.BILINEAR)\n",
    "        image_tensor = transforms.ToTensor()(image)\n",
    "        image_tensor = normalize(image_tensor).unsqueeze(0).to(device)\n",
    "        \n",
    "        mask_path = Path(data_root) / 'masks' / f'fold{fold}' / f'{image_id}.npz'\n",
    "        if not mask_path.exists():\n",
    "            continue\n",
    "            \n",
    "        mask_data = np.load(mask_path)\n",
    "        masks = mask_data['masks']\n",
    "        \n",
    "        gt_semantic = np.zeros((masks.shape[0], masks.shape[1]), dtype=np.int64)\n",
    "        for c in range(min(masks.shape[2], num_classes)):\n",
    "            gt_semantic[masks[:, :, c] > 0] = c\n",
    "        \n",
    "        gt_instance = np.zeros((masks.shape[0], masks.shape[1]), dtype=np.int32)\n",
    "        gt_class_map = {}\n",
    "        inst_id = 1\n",
    "        for c in range(min(masks.shape[2], num_classes)):\n",
    "            binary_mask = masks[:, :, c] > 0\n",
    "            if binary_mask.sum() > 0:\n",
    "                labeled, num_instances = ndimage.label(binary_mask)\n",
    "                for i in range(1, num_instances + 1):\n",
    "                    gt_instance[labeled == i] = inst_id\n",
    "                    gt_class_map[inst_id] = c\n",
    "                    inst_id += 1\n",
    "        \n",
    "        gt_semantic = np.array(Image.fromarray(gt_semantic.astype(np.uint8)).resize(\n",
    "            (img_size, img_size), Image.NEAREST))\n",
    "        gt_instance = np.array(Image.fromarray(gt_instance.astype(np.int32)).resize(\n",
    "            (img_size, img_size), Image.NEAREST))\n",
    "        \n",
    "        gt_semantic = torch.from_numpy(gt_semantic).long()\n",
    "        gt_instance = torch.from_numpy(gt_instance)\n",
    "        \n",
    "        text_prompts = [text_instruction] * num_classes\n",
    "        outputs = model(image_tensor, text_prompts)\n",
    "        logits = outputs['logits']\n",
    "        pred_semantic = logits.argmax(dim=1).squeeze(0).cpu()\n",
    "        \n",
    "        dice_scores = compute_dice_per_class(pred_semantic, gt_semantic, num_classes)\n",
    "        for c in range(num_classes):\n",
    "            all_dice[c].append(dice_scores[c])\n",
    "        \n",
    "        pq_scores = compute_pq_per_class(pred_semantic, gt_semantic, gt_instance, gt_class_map, num_classes)\n",
    "        for c in range(num_classes):\n",
    "            all_pq[c].append(pq_scores[c]['PQ'])\n",
    "    \n",
    "    dice_macro = [np.mean(all_dice[c]) for c in range(num_classes)]\n",
    "    pq_macro = [np.mean(all_pq[c]) for c in range(num_classes)]\n",
    "    \n",
    "    return {\n",
    "        'mean_dice': np.mean(dice_macro),\n",
    "        'mean_pq': np.mean(pq_macro),\n",
    "        'per_class_dice': dice_macro,\n",
    "        'per_class_pq': pq_macro,\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# Run all 3 text variant evaluations (EXACTLY like CIPS-Net)\n",
    "# ============================================================================\n",
    "text_variant_results = {}\n",
    "all_results_rows = []\n",
    "\n",
    "test_fold = 1  # Use fold 1 as test (same as CIPS-Net)\n",
    "\n",
    "if annotations_df is not None:\n",
    "    # 1. Per-Image Text (from annotations.csv)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Variant 1: Per-Image Text (from annotations.csv)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    per_image_metrics = evaluate_per_image_text(\n",
    "        model, annotations_df, test_fold, \n",
    "        CONFIG['pannuke_preprocess_root'], CONFIG['image_size'],\n",
    "        CONFIG['device'], CONFIG['num_classes']\n",
    "    )\n",
    "    text_variant_results['per_image_text'] = {\n",
    "        'display_name': 'Per-Image Text',\n",
    "        'source': 'annotations.csv instruction column',\n",
    "        **per_image_metrics\n",
    "    }\n",
    "    all_results_rows.append({\n",
    "        'Dataset': 'PanNuke', 'Variant': 'Per-Image Text', 'Setting': 'Zero-Shot',\n",
    "        'mDice': per_image_metrics['mean_dice'], 'mPQ': per_image_metrics['mean_pq'],\n",
    "    })\n",
    "    print(f\"  mDice: {per_image_metrics['mean_dice']:.4f}\")\n",
    "    print(f\"  mPQ:   {per_image_metrics['mean_pq']:.4f}\")\n",
    "    \n",
    "    # 2. Common Text (fixed sentence for all images)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Variant 2: Common Text\")\n",
    "    print(f\"  Text: '{COMMON_TEXT}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    common_metrics = evaluate_fixed_text(\n",
    "        model, annotations_df, test_fold,\n",
    "        CONFIG['pannuke_preprocess_root'], CONFIG['image_size'],\n",
    "        CONFIG['device'], COMMON_TEXT, CONFIG['num_classes']\n",
    "    )\n",
    "    text_variant_results['common_text'] = {\n",
    "        'display_name': 'Common Text',\n",
    "        'text': COMMON_TEXT,\n",
    "        **common_metrics\n",
    "    }\n",
    "    all_results_rows.append({\n",
    "        'Dataset': 'PanNuke', 'Variant': 'Common Text', 'Setting': 'Zero-Shot',\n",
    "        'mDice': common_metrics['mean_dice'], 'mPQ': common_metrics['mean_pq'],\n",
    "    })\n",
    "    print(f\"  mDice: {common_metrics['mean_dice']:.4f}\")\n",
    "    print(f\"  mPQ:   {common_metrics['mean_pq']:.4f}\")\n",
    "    \n",
    "    # 3. No Text (empty string)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Variant 3: No Text (empty string)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    no_text_metrics = evaluate_fixed_text(\n",
    "        model, annotations_df, test_fold,\n",
    "        CONFIG['pannuke_preprocess_root'], CONFIG['image_size'],\n",
    "        CONFIG['device'], NO_TEXT, CONFIG['num_classes']\n",
    "    )\n",
    "    text_variant_results['no_text'] = {\n",
    "        'display_name': 'No Text',\n",
    "        'text': '',\n",
    "        **no_text_metrics\n",
    "    }\n",
    "    all_results_rows.append({\n",
    "        'Dataset': 'PanNuke', 'Variant': 'No Text', 'Setting': 'Zero-Shot',\n",
    "        'mDice': no_text_metrics['mean_dice'], 'mPQ': no_text_metrics['mean_pq'],\n",
    "    })\n",
    "    print(f\"  mDice: {no_text_metrics['mean_dice']:.4f}\")\n",
    "    print(f\"  mPQ:   {no_text_metrics['mean_pq']:.4f}\")\n",
    "\n",
    "# Save results\n",
    "text_variant_path = results_dir / \"text_variants\" / \"pannuke_text_variants.json\"\n",
    "with open(text_variant_path, 'w') as f:\n",
    "    json.dump(text_variant_results, f, indent=2)\n",
    "\n",
    "# ============================================================================\n",
    "# Summary Table (matching CIPS-Net format exactly)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PANNUKE TEXT VARIANT RESULTS (CIPS-Net Format)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Dataset':<12} {'Variant':<18} {'Setting':<12} {'mDice':<12} {'mPQ':<12}\")\n",
    "print(\"-\" * 66)\n",
    "for row in all_results_rows:\n",
    "    print(f\"{row['Dataset']:<12} {row['Variant']:<18} {row['Setting']:<12} \"\n",
    "          f\"{row['mDice']:<12.4f} {row['mPQ']:<12.4f}\")\n",
    "print(\"-\" * 66)\n",
    "\n",
    "# Comparison with CIPS-Net results\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON WITH CIPS-NET\")\n",
    "print(\"=\" * 70)\n",
    "cipsnet_reference = {\n",
    "    'Per-Image Text': {'mDice': 0.7661, 'mPQ': 0.5356},\n",
    "    'Common Text': {'mDice': 0.3558, 'mPQ': 0.1091},\n",
    "    'No Text': {'mDice': 0.6910, 'mPQ': 0.4998},\n",
    "}\n",
    "print(f\"{'Variant':<18} {'CIPS-Net mDice':<16} {'CIPS-Net mPQ':<14} {f'{MODEL_NAME} mDice':<16} {f'{MODEL_NAME} mPQ':<14}\")\n",
    "print(\"-\" * 78)\n",
    "for row in all_results_rows:\n",
    "    variant = row['Variant']\n",
    "    cips = cipsnet_reference.get(variant, {})\n",
    "    print(f\"{variant:<18} {cips.get('mDice', 'N/A'):<16} {cips.get('mPQ', 'N/A'):<14} {row['mDice']:<16.4f} {row['mPQ']:<14.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Text variant results saved to: {text_variant_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90e52c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 3: ZERO-SHOT EVALUATION ON EXTERNAL DATASETS\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Zero-Shot Evaluation: CoNSeP\n",
      "============================================================\n",
      "  Path: /mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/Histopathology_Datasets_Official/CoNSeP\n",
      "  Class Mapping: {2: 1, 3: 4, 4: 0, 5: 2}\n",
      "  ⚠️ External dataset loader needed for full evaluation\n",
      "\n",
      "============================================================\n",
      "Zero-Shot Evaluation: MoNuSAC\n",
      "============================================================\n",
      "  Path: /mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/Histopathology_Datasets_Official/MoNuSAC\n",
      "  Class Mapping: {'Epithelial': 4, 'Lymphocyte': 1, 'Macrophage': 1, 'Neutrophil': 1}\n",
      "  ⚠️ External dataset loader needed for full evaluation\n",
      "\n",
      "✓ Zero-shot config saved to: /mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/results/clipseg_20260126_145532/zero_shot/zero_shot_results.json\n",
      "\n",
      "======================================================================\n",
      "ZERO-SHOT EVALUATION STATUS\n",
      "======================================================================\n",
      "  CoNSeP: requires_dataset_loader\n",
      "  MoNuSAC: requires_dataset_loader\n",
      "\n",
      "Note: Full zero-shot evaluation on CoNSeP and MoNuSAC requires\n",
      "      proper dataset loaders with class mapping (see CIPS-Net notebook).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PHASE 3: ZERO-SHOT EVALUATION ON CONSEP AND MONUSAC\n",
    "# ============================================================================\n",
    "# Same 3 text variants as PanNuke:\n",
    "#   1. Per-Image Text\n",
    "#   2. Common Text\n",
    "#   3. No Text\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 3: ZERO-SHOT EVALUATION ON EXTERNAL DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# External dataset paths (Official datasets)\n",
    "EXTERNAL_DATASETS_ROOT = Path(CONFIG.get('external_datasets_root', \n",
    "    '/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/Histopathology_Datasets_Official'))\n",
    "\n",
    "external_datasets_config = {\n",
    "    'CoNSeP': {\n",
    "        'path': EXTERNAL_DATASETS_ROOT / 'CoNSeP',\n",
    "        # CoNSeP → PanNuke class mapping\n",
    "        'class_mapping': {\n",
    "            2: 1,  # Inflammatory → Inflammatory\n",
    "            3: 4,  # Healthy Epithelial → Epithelial\n",
    "            4: 0,  # Dysplastic/Malignant → Neoplastic\n",
    "            5: 2,  # Fibroblast → Connective\n",
    "        },\n",
    "    },\n",
    "    'MoNuSAC': {\n",
    "        'path': EXTERNAL_DATASETS_ROOT / 'MoNuSAC',\n",
    "        # MoNuSAC → PanNuke class mapping\n",
    "        'class_mapping': {\n",
    "            'Epithelial': 4,   # Epithelial → Epithelial\n",
    "            'Lymphocyte': 1,   # Lymphocyte → Inflammatory\n",
    "            'Macrophage': 1,   # Macrophage → Inflammatory\n",
    "            'Neutrophil': 1,   # Neutrophil → Inflammatory\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "zero_shot_results = {}\n",
    "all_zero_shot_rows = []\n",
    "\n",
    "# Use the model loaded from Phase 2\n",
    "model.eval()\n",
    "\n",
    "# Display name mapping for variants (ensure available here)\n",
    "variant_display_names = {\n",
    "    \"per_image_text\": \"Per-Image Text\",\n",
    "    \"common_text\": \"Common Text\",\n",
    "    \"no_text\": \"No Text\",\n",
    "}\n",
    "\n",
    "for dataset_name, config in external_datasets_config.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Zero-Shot Evaluation: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    dataset_path = config['path']\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not dataset_path.exists():\n",
    "        print(f\"⚠️ Dataset not found at: {dataset_path}\")\n",
    "        print(f\"   Skipping {dataset_name}...\")\n",
    "        zero_shot_results[dataset_name] = {'status': 'dataset_not_found', 'path': str(dataset_path)}\n",
    "        continue\n",
    "    \n",
    "    # For now, we'll note that external dataset evaluation requires\n",
    "    # proper dataset loaders matching the CIPS-Net format\n",
    "    # This is a placeholder that shows the expected output format\n",
    "    \n",
    "    print(f\"  Path: {dataset_path}\")\n",
    "    print(f\"  Class Mapping: {config['class_mapping']}\")\n",
    "    \n",
    "    # Placeholder results (actual evaluation would need proper dataset loaders)\n",
    "    dataset_results = {\n",
    "        'status': 'requires_dataset_loader',\n",
    "        'class_mapping': config['class_mapping'],\n",
    "        'text_variants': {},\n",
    "    }\n",
    "    \n",
    "    # For each text variant\n",
    "    for variant_name, prompts in TEXT_PROMPTS_VARIANTS.items():\n",
    "        display_name = variant_display_names[variant_name]\n",
    "        \n",
    "        # Placeholder - would need actual dataset loader\n",
    "        dataset_results['text_variants'][variant_name] = {\n",
    "            'display_name': display_name,\n",
    "            'status': 'pending',\n",
    "        }\n",
    "        \n",
    "        all_zero_shot_rows.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Variant': display_name,\n",
    "            'Setting': 'Zero-Shot',\n",
    "            'mDice': 'N/A',\n",
    "            'mIoU': 'N/A',\n",
    "        })\n",
    "    \n",
    "    zero_shot_results[dataset_name] = dataset_results\n",
    "    print(f\"  ⚠️ External dataset loader needed for full evaluation\")\n",
    "\n",
    "# Save zero-shot results\n",
    "zero_shot_path = results_dir / \"zero_shot\" / \"zero_shot_results.json\"\n",
    "with open(zero_shot_path, 'w') as f:\n",
    "    json.dump(zero_shot_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✓ Zero-shot config saved to: {zero_shot_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ZERO-SHOT EVALUATION STATUS\")\n",
    "print(\"=\" * 70)\n",
    "for dataset_name, result in zero_shot_results.items():\n",
    "    status = result.get('status', 'unknown')\n",
    "    print(f\"  {dataset_name}: {status}\")\n",
    "\n",
    "print(f\"\\nNote: Full zero-shot evaluation on CoNSeP and MoNuSAC requires\")\n",
    "print(f\"      proper dataset loaders with class mapping (see CIPS-Net notebook).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3310b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 4: FINE-TUNING ON EXTERNAL DATASETS\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "Fine-tuning on CoNSeP\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'image_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Check if dataset exists\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ Dataset not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     fine_tune_results[dataset_name] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_not_found\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_dir'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PHASE 4: FINE-TUNING ON CONSEP AND MONUSAC\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 4: FINE-TUNING ON EXTERNAL DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fine_tune_results = {}\n",
    "\n",
    "for dataset_name, paths in external_datasets_config.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fine-tuning on {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not Path(paths['image_dir']).exists():\n",
    "        print(f\"⚠️ Dataset not found at {paths['image_dir']}\")\n",
    "        fine_tune_results[dataset_name] = {'status': 'dataset_not_found'}\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Create train/val split for this dataset\n",
    "        all_images = sorted(\n",
    "            list(Path(paths['image_dir']).glob(\"*.png\")) +\n",
    "            list(Path(paths['image_dir']).glob(\"*.jpg\"))\n",
    "        )\n",
    "        \n",
    "        if len(all_images) < 10:\n",
    "            print(f\"⚠️ Too few images ({len(all_images)}), skipping...\")\n",
    "            fine_tune_results[dataset_name] = {'status': 'too_few_images'}\n",
    "            continue\n",
    "        \n",
    "        # 80/20 train/val split\n",
    "        n_train = int(len(all_images) * 0.8)\n",
    "        \n",
    "        # Create datasets using our SimplePanNukeDataset structure\n",
    "        # (assuming masks follow same naming convention)\n",
    "        train_ext_dataset = SimplePanNukeDataset(\n",
    "            image_dir=paths['image_dir'],\n",
    "            mask_dir=paths['mask_dir'],\n",
    "            image_size=CONFIG['image_size'],\n",
    "            split=\"train\",\n",
    "            fold=0,\n",
    "        )\n",
    "        \n",
    "        val_ext_dataset = SimplePanNukeDataset(\n",
    "            image_dir=paths['image_dir'],\n",
    "            mask_dir=paths['mask_dir'],\n",
    "            image_size=CONFIG['image_size'],\n",
    "            split=\"val\",\n",
    "            fold=0,\n",
    "        )\n",
    "        \n",
    "        train_ext_loader = DataLoader(\n",
    "            train_ext_dataset, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "            num_workers=0, pin_memory=True, drop_last=True,\n",
    "        )\n",
    "        \n",
    "        val_ext_loader = DataLoader(\n",
    "            val_ext_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
    "            num_workers=0, pin_memory=True,\n",
    "        )\n",
    "        \n",
    "        print(f\"Train: {len(train_ext_dataset)}, Val: {len(val_ext_dataset)}\")\n",
    "        \n",
    "        # Create fresh model initialized from PanNuke-pretrained weights\n",
    "        ft_model = get_model(\n",
    "            MODEL_NAME,\n",
    "            num_classes=CONFIG['num_classes'],\n",
    "            image_size=CONFIG['image_size'],\n",
    "            clip_model=CONFIG['clip_model'],\n",
    "            freeze_clip=True,\n",
    "            device=CONFIG['device'],\n",
    "        )\n",
    "        \n",
    "        # Load pretrained weights from PanNuke\n",
    "        pretrain_path = results_dir / \"models\" / f\"best_{MODEL_NAME}_fold0.pth\"\n",
    "        if pretrain_path.exists():\n",
    "            checkpoint = torch.load(pretrain_path, map_location=CONFIG['device'])\n",
    "            ft_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Initialized from PanNuke pretrained model\")\n",
    "        \n",
    "        # Fine-tuning with lower learning rate\n",
    "        ft_optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, ft_model.parameters()),\n",
    "            lr=CONFIG['learning_rate'] * 0.1,  # Lower LR for fine-tuning\n",
    "            weight_decay=CONFIG['weight_decay'],\n",
    "        )\n",
    "        ft_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            ft_optimizer, T_max=20, eta_min=1e-7,  # Fewer epochs for fine-tuning\n",
    "        )\n",
    "        ft_scaler = torch.amp.GradScaler('cuda')\n",
    "        \n",
    "        # Fine-tuning loop (fewer epochs)\n",
    "        best_ft_metric = 0.0\n",
    "        ft_patience_counter = 0\n",
    "        ft_history = defaultdict(list)\n",
    "        \n",
    "        for epoch in range(20):  # 20 epochs for fine-tuning\n",
    "            print(f\"\\n  Epoch {epoch+1}/20\")\n",
    "            \n",
    "            train_loss = train_one_epoch(\n",
    "                ft_model, train_ext_loader, ft_optimizer, criterion, ft_scaler,\n",
    "                TEXT_PROMPTS, CONFIG['device']\n",
    "            )\n",
    "            \n",
    "            val_metrics = validate(\n",
    "                ft_model, val_ext_loader, criterion, TEXT_PROMPTS,\n",
    "                CONFIG['device'], CONFIG['num_classes']\n",
    "            )\n",
    "            \n",
    "            ft_scheduler.step()\n",
    "            \n",
    "            ft_history['train_loss'].append(train_loss)\n",
    "            ft_history['val_iou'].append(val_metrics['mean_iou'])\n",
    "            ft_history['val_dice'].append(val_metrics['mean_dice'])\n",
    "            \n",
    "            print(f\"    Loss: {train_loss:.4f} | mIoU: {val_metrics['mean_iou']:.4f}\")\n",
    "            \n",
    "            if val_metrics['mean_iou'] > best_ft_metric:\n",
    "                best_ft_metric = val_metrics['mean_iou']\n",
    "                ft_patience_counter = 0\n",
    "                \n",
    "                # Save fine-tuned model\n",
    "                ft_save_path = results_dir / \"fine_tuned\" / f\"best_{MODEL_NAME}_{dataset_name.lower()}.pth\"\n",
    "                torch.save({\n",
    "                    'model_state_dict': ft_model.state_dict(),\n",
    "                    'best_metric': best_ft_metric,\n",
    "                }, ft_save_path)\n",
    "            else:\n",
    "                ft_patience_counter += 1\n",
    "                if ft_patience_counter >= 5:\n",
    "                    print(f\"    Early stopping after {epoch+1} epochs\")\n",
    "                    break\n",
    "        \n",
    "        # Evaluate fine-tuned model with all text variants\n",
    "        ft_model.eval()\n",
    "        dataset_ft_results = {\n",
    "            'best_iou': best_ft_metric,\n",
    "            'text_variants': {},\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n  Text variant evaluation (fine-tuned):\")\n",
    "        for variant_name, prompts in TEXT_PROMPTS_VARIANTS.items():\n",
    "            metrics = validate(\n",
    "                ft_model, val_ext_loader, criterion, prompts,\n",
    "                CONFIG['device'], CONFIG['num_classes']\n",
    "            )\n",
    "            \n",
    "            dataset_ft_results['text_variants'][variant_name] = {\n",
    "                'mean_iou': metrics['mean_iou'],\n",
    "                'mean_dice': metrics['mean_dice'],\n",
    "            }\n",
    "            print(f\"    {variant_name}: mIoU={metrics['mean_iou']:.4f}\")\n",
    "        \n",
    "        fine_tune_results[dataset_name] = dataset_ft_results\n",
    "        \n",
    "        # Cleanup\n",
    "        del ft_model, ft_optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fine-tuning on {dataset_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        fine_tune_results[dataset_name] = {'status': 'error', 'message': str(e)}\n",
    "\n",
    "# Save fine-tuning results\n",
    "ft_results_path = results_dir / \"fine_tuned\" / \"fine_tuning_results.json\"\n",
    "with open(ft_results_path, 'w') as f:\n",
    "    json.dump(fine_tune_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Fine-tuning results saved to: {ft_results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f963a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 5: COMPREHENSIVE RESULTS SUMMARY (CIPS-NET FORMAT with mPQ)\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Timestamp: {EXPERIMENT_TIMESTAMP}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PanNuke 3-Fold Cross-Validation Results\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"1. PanNuke 3-Fold Cross-Validation Results\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fold_results_summary = []\n",
    "for fold_num in range(3):\n",
    "    fold_key = f'fold_{fold_num}'\n",
    "    if fold_key in all_fold_results:\n",
    "        metrics = all_fold_results[fold_key]\n",
    "        fold_results_summary.append({\n",
    "            'Fold': fold_num,\n",
    "            'Best mIoU': metrics['best_iou'],\n",
    "            'Final mDice': metrics['final_dice'],\n",
    "            'Epochs': metrics['epochs_trained'],\n",
    "        })\n",
    "        print(f\"  Fold {fold_num}: mIoU = {metrics['best_iou']:.4f}, mDice = {metrics['final_dice']:.4f}\")\n",
    "\n",
    "if fold_results_summary:\n",
    "    avg_iou = np.mean([r['Best mIoU'] for r in fold_results_summary])\n",
    "    std_iou = np.std([r['Best mIoU'] for r in fold_results_summary])\n",
    "    avg_dice = np.mean([r['Final mDice'] for r in fold_results_summary])\n",
    "    std_dice = np.std([r['Final mDice'] for r in fold_results_summary])\n",
    "    print(f\"\\n  Average: mIoU = {avg_iou:.4f} ± {std_iou:.4f}\")\n",
    "    print(f\"           mDice = {avg_dice:.4f} ± {std_dice:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Text Variants Results (CIPS-Net Format with mPQ)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2. Text Variants Results (CIPS-Net Format)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Dataset':<12} {'Variant':<18} {'Setting':<12} {'mDice':<12} {'mPQ':<12}\")\n",
    "print(\"-\" * 66)\n",
    "\n",
    "csv_rows = []\n",
    "\n",
    "# PanNuke text variants\n",
    "if 'text_variant_results' in dir() and text_variant_results:\n",
    "    for variant_name, result in text_variant_results.items():\n",
    "        display_name = result.get('display_name', variant_name)\n",
    "        row = {\n",
    "            'Dataset': 'PanNuke',\n",
    "            'Variant': display_name,\n",
    "            'Setting': 'Zero-Shot',\n",
    "            'mDice': result['mean_dice'],\n",
    "            'mPQ': result['mean_pq'],\n",
    "        }\n",
    "        csv_rows.append(row)\n",
    "        print(f\"{row['Dataset']:<12} {row['Variant']:<18} {row['Setting']:<12} \"\n",
    "              f\"{row['mDice']:<12.4f} {row['mPQ']:<12.4f}\")\n",
    "\n",
    "print(\"-\" * 66)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Save Results in CIPS-Net Format\n",
    "# ============================================================================\n",
    "\n",
    "# Save CSV (matching CIPS-Net output format)\n",
    "if csv_rows:\n",
    "    results_df = pd.DataFrame(csv_rows)\n",
    "    csv_path = results_dir / \"text_variants\" / \"results_cipsnet_format.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n✓ CSV results saved to: {csv_path}\")\n",
    "\n",
    "# Compile all results into summary JSON\n",
    "summary = {\n",
    "    'experiment': {\n",
    "        'name': EXPERIMENT_NAME,\n",
    "        'model': MODEL_NAME,\n",
    "        'timestamp': EXPERIMENT_TIMESTAMP,\n",
    "        'config': CONFIG,\n",
    "    },\n",
    "    'pannuke_3fold_cv': {\n",
    "        'fold_results': all_fold_results if 'all_fold_results' in dir() else {},\n",
    "        'mean_iou': avg_iou if 'avg_iou' in dir() else None,\n",
    "        'std_iou': std_iou if 'std_iou' in dir() else None,\n",
    "        'mean_dice': avg_dice if 'avg_dice' in dir() else None,\n",
    "        'std_dice': std_dice if 'std_dice' in dir() else None,\n",
    "    },\n",
    "    'text_variants': text_variant_results if 'text_variant_results' in dir() else {},\n",
    "    'zero_shot': zero_shot_results if 'zero_shot_results' in dir() else {},\n",
    "}\n",
    "\n",
    "summary_path = results_dir / \"full_experiment_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Full summary saved to: {summary_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Create Markdown Summary (with mPQ)\n",
    "# ============================================================================\n",
    "md_summary = f\"\"\"# Experiment Results: {EXPERIMENT_NAME}\n",
    "\n",
    "## Model: {MODEL_NAME}\n",
    "- **Timestamp**: {EXPERIMENT_TIMESTAMP}\n",
    "- **Epochs**: {CONFIG['num_epochs']} (Early Stopping: {CONFIG['early_stopping_patience']})\n",
    "- **Image Size**: {CONFIG['image_size']}\n",
    "- **Batch Size**: {CONFIG['batch_size']}\n",
    "\n",
    "## PanNuke 3-Fold Cross-Validation\n",
    "| Fold | mIoU | mDice | Epochs |\n",
    "|------|------|-------|--------|\n",
    "\"\"\"\n",
    "\n",
    "for r in fold_results_summary:\n",
    "    md_summary += f\"| {r['Fold']} | {r['Best mIoU']:.4f} | {r['Final mDice']:.4f} | {r['Epochs']} |\\n\"\n",
    "\n",
    "if 'avg_iou' in dir():\n",
    "    md_summary += f\"| **Avg** | **{avg_iou:.4f}±{std_iou:.4f}** | **{avg_dice:.4f}±{std_dice:.4f}** | - |\\n\"\n",
    "\n",
    "md_summary += f\"\"\"\n",
    "## Text Variants Evaluation (CIPS-Net Protocol)\n",
    "| Dataset | Variant | Setting | mDice | mPQ |\n",
    "|---------|---------|---------|-------|-----|\n",
    "\"\"\"\n",
    "\n",
    "for row in csv_rows:\n",
    "    md_summary += f\"| {row['Dataset']} | {row['Variant']} | {row['Setting']} | {row['mDice']:.4f} | {row['mPQ']:.4f} |\\n\"\n",
    "\n",
    "md_summary += f\"\"\"\n",
    "## Comparison with CIPS-Net (Reference)\n",
    "| Dataset | Variant | CIPS-Net mDice | CIPS-Net mPQ | {MODEL_NAME} mDice | {MODEL_NAME} mPQ |\n",
    "|---------|---------|----------------|--------------|-----------------|---------------|\n",
    "| PanNuke | Per-Image Text | 0.7661 | 0.5356 | {csv_rows[0]['mDice']:.4f} | {csv_rows[0]['mPQ']:.4f} |\n",
    "| PanNuke | Common Text | 0.3558 | 0.1091 | {csv_rows[1]['mDice']:.4f} | {csv_rows[1]['mPQ']:.4f} |\n",
    "| PanNuke | No Text | 0.6910 | 0.4998 | {csv_rows[2]['mDice']:.4f} | {csv_rows[2]['mPQ']:.4f} |\n",
    "\"\"\"\n",
    "\n",
    "md_path = results_dir / \"RESULTS.md\"\n",
    "with open(md_path, 'w') as f:\n",
    "    f.write(md_summary)\n",
    "\n",
    "print(f\"✓ Markdown summary saved to: {md_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Final Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n📁 Results saved to: {results_dir}\")\n",
    "print(f\"\\nFiles:\")\n",
    "print(f\"  • models/best_{MODEL_NAME}_fold*.pth - Model checkpoints\")\n",
    "print(f\"  • pannuke_3fold/cv_results.json - 3-fold CV results\")\n",
    "print(f\"  • text_variants/results_cipsnet_format.csv - Text variant results (mDice + mPQ)\")\n",
    "print(f\"  • RESULTS.md - Markdown summary\")\n",
    "print(f\"  • full_experiment_summary.json - Complete results\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7e55c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Run all folds**: Change `CONFIG['current_fold']` to 1 and 2 to complete 3-fold CV\n",
    "2. **Try different models**: Change `MODEL_NAME` to train other models\n",
    "3. **Compare with CIPS-Net**: Load CIPS-Net results from `results_comparison.txt`\n",
    "4. **Zero-shot evaluation**: Test trained models on CoNSeP and MoNuSAC without fine-tuning\n",
    "5. **Fine-tuning**: Optionally fine-tune on target datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
