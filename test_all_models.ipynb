{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8556945",
   "metadata": {},
   "source": [
    "# Test All Text-Guided Segmentation Models\n",
    "\n",
    "This notebook tests all 14 text-guided segmentation models to verify they:\n",
    "1. Load correctly\n",
    "2. Accept the expected inputs (images + text prompts)\n",
    "3. Produce the expected outputs (logits with correct shape)\n",
    "\n",
    "## Models to test:\n",
    "1. CLIPSeg (CVPR 2022)\n",
    "2. LSeg (ICLR 2022)\n",
    "3. GroupViT (CVPR 2022)\n",
    "4. SAN (CVPR 2023)\n",
    "5. FC-CLIP (NeurIPS 2023)\n",
    "6. OVSeg (CVPR 2023)\n",
    "7. CAT-Seg (CVPR 2024)\n",
    "8. SED (CVPR 2024)\n",
    "9. MAFT+ (ECCV 2024 Oral)\n",
    "10. X-Decoder (CVPR 2023)\n",
    "11. OpenSeeD (ICCV 2023)\n",
    "12. ODISE (CVPR 2023)\n",
    "13. TagAlign (arXiv 2023)\n",
    "14. Semantic-SAM (ECCV 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdaac384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "WORKSPACE = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work\"\n",
    "sys.path.insert(0, WORKSPACE)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fce2f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Text-Guided Segmentation Models for Histopathology\n",
      "================================================================================\n",
      "\n",
      "#   Model           Venue                Description                             \n",
      "--------------------------------------------------------------------------------\n",
      "1   CLIPSeg         CVPR 2022            Uses CLIP features with FiLM condition..\n",
      "2   LSeg            ICLR 2022            Dense Prediction Transformer with CLIP..\n",
      "3   GroupViT        CVPR 2022            Hierarchical grouping mechanism with c..\n",
      "4   SAN             CVPR 2023            Side adapter network preserving CLIP c..\n",
      "5   FC-CLIP         NeurIPS 2023         Fully convolutional CLIP for dense pre..\n",
      "6   OVSeg           CVPR 2023            Mask-adapted CLIP with region-level cl..\n",
      "7   CAT-Seg         CVPR 2024            Cost aggregation with spatial semantic..\n",
      "8   SED             CVPR 2024            Simple encoder-decoder with category-g..\n",
      "9   MAFT+           ECCV 2024 Oral       Multi-modal adapters with cross-modal ..\n",
      "10  X-Decoder       CVPR 2023            Unified decoder supporting multiple vi..\n",
      "11  OpenSeeD        ICCV 2023            Unified framework with decoupled seman..\n",
      "12  ODISE           CVPR 2023            Leverages diffusion model features for..\n",
      "13  TagAlign        arXiv 2023           Tag-guided alignment with multi-granul..\n",
      "14  Semantic-SAM    ECCV 2024            Multi-granularity segmentation with se..\n",
      "--------------------------------------------------------------------------------\n",
      "Total: 14 models\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import TextGuidedSegmentation package\n",
    "# First, clear any cached imports\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove cached TextGuidedSegmentation modules\n",
    "modules_to_remove = [m for m in sys.modules.keys() if 'TextGuidedSegmentation' in m]\n",
    "for m in modules_to_remove:\n",
    "    del sys.modules[m]\n",
    "\n",
    "# Now import fresh\n",
    "from TextGuidedSegmentation import (\n",
    "    get_model,\n",
    "    list_models,\n",
    "    print_model_summary,\n",
    "    MODEL_INFO,\n",
    "    DEFAULT_TEXT_PROMPTS,\n",
    ")\n",
    "\n",
    "# Print available models\n",
    "print_model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cd0edbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy input shape: torch.Size([2, 3, 256, 256])\n",
      "Text prompts: ['neoplastic cells', 'inflammatory cells', 'connective tissue cells', 'dead cells', 'epithelial cells']\n"
     ]
    }
   ],
   "source": [
    "# Test configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 2\n",
    "IMAGE_SIZE = 256\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# Default text prompts for histopathology\n",
    "TEXT_PROMPTS = [\n",
    "    \"neoplastic cells\",\n",
    "    \"inflammatory cells\",\n",
    "    \"connective tissue cells\",\n",
    "    \"dead cells\",\n",
    "    \"epithelial cells\",\n",
    "]\n",
    "\n",
    "# Create dummy input\n",
    "dummy_images = torch.randn(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)\n",
    "print(f\"Dummy input shape: {dummy_images.shape}\")\n",
    "print(f\"Text prompts: {TEXT_PROMPTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3b8a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name: str, images: torch.Tensor, text_prompts: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Test a single model.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with test results\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'model_name': model_name,\n",
    "        'status': 'unknown',\n",
    "        'error': None,\n",
    "        'output_shape': None,\n",
    "        'num_params': None,\n",
    "        'inference_time': None,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        model = get_model(\n",
    "            model_name,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            image_size=IMAGE_SIZE,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"✓ Model loaded in {load_time:.2f}s\")\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        result['num_params'] = total_params\n",
    "        print(f\"✓ Total params: {total_params:,}\")\n",
    "        print(f\"✓ Trainable params: {trainable_params:,}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            outputs = model(images, text_prompts)\n",
    "            inference_time = time.time() - start_time\n",
    "        \n",
    "        result['inference_time'] = inference_time\n",
    "        print(f\"✓ Inference time: {inference_time:.3f}s\")\n",
    "        \n",
    "        # Check outputs\n",
    "        assert 'logits' in outputs, \"Output must contain 'logits'\"\n",
    "        logits = outputs['logits']\n",
    "        result['output_shape'] = tuple(logits.shape)\n",
    "        \n",
    "        expected_shape = (BATCH_SIZE, NUM_CLASSES, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        print(f\"✓ Output shape: {logits.shape}\")\n",
    "        print(f\"  Expected shape: {expected_shape}\")\n",
    "        \n",
    "        assert logits.shape == expected_shape, f\"Shape mismatch: {logits.shape} != {expected_shape}\"\n",
    "        print(f\"✓ Shape matches expected!\")\n",
    "        \n",
    "        # Check for NaN/Inf\n",
    "        assert not torch.isnan(logits).any(), \"Output contains NaN values\"\n",
    "        assert not torch.isinf(logits).any(), \"Output contains Inf values\"\n",
    "        print(f\"✓ No NaN/Inf values\")\n",
    "        \n",
    "        # Check predicted mask\n",
    "        if 'pred_mask' in outputs:\n",
    "            pred_mask = outputs['pred_mask']\n",
    "            print(f\"✓ Predicted mask shape: {pred_mask.shape}\")\n",
    "            print(f\"  Unique classes: {torch.unique(pred_mask).tolist()}\")\n",
    "        \n",
    "        result['status'] = 'PASSED'\n",
    "        print(f\"\\n✅ {model_name}: PASSED\")\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['status'] = 'FAILED'\n",
    "        result['error'] = str(e)\n",
    "        print(f\"\\n❌ {model_name}: FAILED\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aba3d109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models to test: 18\n",
      "Models: ['clipseg', 'clipseg_rd64', 'clipseg_rd128', 'lseg', 'lseg_vit_l', 'groupvit', 'san', 'fc_clip', 'fc_clip_convnext', 'ovseg', 'cat_seg', 'sed', 'maft_plus', 'x_decoder', 'openseed', 'odise', 'tagalign', 'semantic_sam']\n"
     ]
    }
   ],
   "source": [
    "# Get all model names\n",
    "all_models = list_models()\n",
    "print(f\"Total models to test: {len(all_models)}\")\n",
    "print(f\"Models: {all_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a3e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing: clipseg\n",
      "============================================================\n",
      "✓ Model loaded in 1.42s\n",
      "✓ Total params: 150,078,466\n",
      "✓ Trainable params: 457,729\n",
      "✓ Inference time: 0.030s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [0, 1, 2, 3, 4]\n",
      "\n",
      "✅ clipseg: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: clipseg_rd64\n",
      "============================================================\n",
      "✓ Model loaded in 1.26s\n",
      "✓ Total params: 150,078,466\n",
      "✓ Trainable params: 457,729\n",
      "✓ Inference time: 0.022s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [0, 1, 2, 3, 4]\n",
      "\n",
      "✅ clipseg_rd64: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: clipseg_rd128\n",
      "============================================================\n",
      "✓ Model loaded in 1.39s\n",
      "✓ Total params: 150,761,474\n",
      "✓ Trainable params: 1,140,737\n",
      "✓ Inference time: 0.031s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [0, 1, 2, 3, 4]\n",
      "\n",
      "✅ clipseg_rd128: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: lseg\n",
      "============================================================\n",
      "✓ Model loaded in 1.52s\n",
      "✓ Total params: 178,928,897\n",
      "✓ Trainable params: 29,308,160\n",
      "✓ Inference time: 0.032s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [2]\n",
      "\n",
      "✅ lseg: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: lseg_vit_l\n",
      "============================================================\n",
      "✓ Model loaded in 3.36s\n",
      "✓ Total params: 456,990,465\n",
      "✓ Trainable params: 29,373,952\n",
      "\n",
      "❌ lseg_vit_l: FAILED\n",
      "   Error: mat1 and mat2 shapes cannot be multiplied (648x2048 and 1536x768)\n",
      "\n",
      "============================================================\n",
      "Testing: groupvit\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3306561/2823249585.py\", line 44, in test_model\n",
      "    outputs = model(images, text_prompts)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/lseg.py\", line 356, in forward\n",
      "    visual_features = self.encode_image(image)  # (B, D, H', W')\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/lseg.py\", line 327, in encode_image\n",
      "    dense_features = self.dpt_head(layer_features, cls_token, patch_h, patch_w)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/lseg.py\", line 197, in forward\n",
      "    feat = readout_op(feat, cls_token)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/lseg.py\", line 43, in forward\n",
      "    x = self.readout_proj(x)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: mat1 and mat2 shapes cannot be multiplied (648x2048 and 1536x768)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded in 1.50s\n",
      "✓ Total params: 175,964,802\n",
      "✓ Trainable params: 26,344,065\n",
      "✓ Inference time: 0.011s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [0, 4]\n",
      "\n",
      "✅ groupvit: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: san\n",
      "============================================================\n",
      "✓ Model loaded in 1.47s\n",
      "✓ Total params: 181,669,121\n",
      "✓ Trainable params: 32,048,384\n",
      "✓ Inference time: 0.034s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [0, 1, 2, 3, 4]\n",
      "\n",
      "✅ san: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: fc_clip\n",
      "============================================================\n",
      "✓ Model loaded in 1.37s\n",
      "✓ Total params: 154,019,586\n",
      "✓ Trainable params: 4,398,849\n",
      "✓ Inference time: 0.037s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [2]\n",
      "\n",
      "✅ fc_clip: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: fc_clip_convnext\n",
      "============================================================\n",
      "✓ Model loaded in 1.39s\n",
      "✓ Total params: 155,200,770\n",
      "✓ Trainable params: 5,580,033\n",
      "✓ Inference time: 0.037s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [0]\n",
      "\n",
      "✅ fc_clip_convnext: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: ovseg\n",
      "============================================================\n",
      "✓ Model loaded in 1.40s\n",
      "✓ Total params: 155,276,034\n",
      "✓ Trainable params: 5,655,297\n",
      "✓ Inference time: 0.024s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [2]\n",
      "\n",
      "✅ ovseg: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: cat_seg\n",
      "============================================================\n",
      "✓ Model loaded in 1.51s\n",
      "✓ Total params: 151,498,888\n",
      "✓ Trainable params: 1,878,151\n",
      "✓ Inference time: 0.144s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [0, 1, 2, 3, 4]\n",
      "\n",
      "✅ cat_seg: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: sed\n",
      "============================================================\n",
      "✓ Model loaded in 1.39s\n",
      "✓ Total params: 156,639,491\n",
      "✓ Trainable params: 7,018,754\n",
      "✓ Inference time: 0.022s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [1, 2, 4]\n",
      "\n",
      "✅ sed: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: maft_plus\n",
      "============================================================\n",
      "✓ Model loaded in 1.26s\n",
      "✓ Total params: 158,623,311\n",
      "✓ Trainable params: 9,002,574\n",
      "\n",
      "❌ maft_plus: FAILED\n",
      "   Error: mat1 and mat2 shapes cannot be multiplied (131072x512 and 768x512)\n",
      "\n",
      "============================================================\n",
      "Testing: x_decoder\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3306561/2823249585.py\", line 44, in test_model\n",
      "    outputs = model(images, text_prompts)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/maft_plus.py\", line 355, in forward\n",
      "    visual_enhanced, text_enhanced = self.cross_modal(visual_flat, text_features)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/maft_plus.py\", line 126, in forward\n",
      "    v_proj = self.v2t_proj(visual_features)  # (B, N, D_t)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: mat1 and mat2 shapes cannot be multiplied (131072x512 and 768x512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded in 1.25s\n",
      "✓ Total params: 162,475,521\n",
      "✓ Trainable params: 12,854,784\n",
      "\n",
      "❌ x_decoder: FAILED\n",
      "   Error: The size of tensor a (256) must match the size of tensor b (65536) at non-singleton dimension 1\n",
      "\n",
      "============================================================\n",
      "Testing: openseed\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3306561/2823249585.py\", line 44, in test_model\n",
      "    outputs = model(images, text_prompts)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/x_decoder.py\", line 356, in forward\n",
      "    queries = self.decoder(decoder_feats, text_feats, pos_flat)  # (B, Q, d_model)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/x_decoder.py\", line 165, in forward\n",
      "    queries = layer(queries, visual_feats, text_feats, pos_embed)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/x_decoder.py\", line 104, in forward\n",
      "    k = visual_feats + pos_embed\n",
      "RuntimeError: The size of tensor a (256) must match the size of tensor b (65536) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded in 1.26s\n",
      "✓ Total params: 158,622,721\n",
      "✓ Trainable params: 9,001,984\n",
      "✓ Inference time: 0.025s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [4]\n",
      "\n",
      "✅ openseed: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: odise\n",
      "============================================================\n",
      "✓ Model loaded in 1.32s\n",
      "✓ Total params: 162,532,417\n",
      "✓ Trainable params: 12,911,680\n",
      "✓ Inference time: 0.046s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [0, 2, 3, 4]\n",
      "\n",
      "✅ odise: PASSED\n",
      "\n",
      "============================================================\n",
      "Testing: tagalign\n",
      "============================================================\n",
      "✓ Model loaded in 1.24s\n",
      "✓ Total params: 151,934,210\n",
      "✓ Trainable params: 2,313,473\n",
      "\n",
      "❌ tagalign: FAILED\n",
      "   Error: Sizes of tensors must match except in dimension 1. Expected size 15 but got size 16 for tensor number 1 in the list.\n",
      "\n",
      "============================================================\n",
      "Testing: semantic_sam\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3306561/2823249585.py\", line 44, in test_model\n",
      "    outputs = model(images, text_prompts)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/tagalign.py\", line 305, in forward\n",
      "    visual_features = self.encode_image(image, text_features)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/tagalign.py\", line 277, in encode_image\n",
      "    multi_scale = self.multi_granular(aligned)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/miglab/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/Datasets/Nikhil/Histopathology_Work/TextGuidedSegmentation/models/tagalign.py\", line 147, in forward\n",
      "    concat = torch.cat(scale_features, dim=1)\n",
      "RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 15 but got size 16 for tensor number 1 in the list.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded in 1.25s\n",
      "✓ Total params: 155,403,522\n",
      "✓ Trainable params: 5,782,785\n",
      "✓ Inference time: 0.026s\n",
      "✓ Output shape: torch.Size([2, 5, 256, 256])\n",
      "  Expected shape: (2, 5, 256, 256)\n",
      "✓ Shape matches expected!\n",
      "✓ No NaN/Inf values\n",
      "✓ Predicted mask shape: torch.Size([2, 256, 256])\n",
      "  Unique classes: [3]\n",
      "\n",
      "✅ semantic_sam: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Test all models\n",
    "results = []\n",
    "\n",
    "for model_name in all_models:\n",
    "    result = test_model(model_name, dummy_images, TEXT_PROMPTS)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Clear CUDA cache between models\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fefdf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✅ Passed: 14/18\n",
      "❌ Failed: 4/18\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model                Status     Params (M)   Time (s)  \n",
      "--------------------------------------------------------------------------------\n",
      "clipseg              ✅ PASS         150.08M      0.030s\n",
      "clipseg_rd64         ✅ PASS         150.08M      0.022s\n",
      "clipseg_rd128        ✅ PASS         150.76M      0.031s\n",
      "lseg                 ✅ PASS         178.93M      0.032s\n",
      "lseg_vit_l           ❌ FAIL         456.99M      0.000s\n",
      "groupvit             ✅ PASS         175.96M      0.011s\n",
      "san                  ✅ PASS         181.67M      0.034s\n",
      "fc_clip              ✅ PASS         154.02M      0.037s\n",
      "fc_clip_convnext     ✅ PASS         155.20M      0.037s\n",
      "ovseg                ✅ PASS         155.28M      0.024s\n",
      "cat_seg              ✅ PASS         151.50M      0.144s\n",
      "sed                  ✅ PASS         156.64M      0.022s\n",
      "maft_plus            ❌ FAIL         158.62M      0.000s\n",
      "x_decoder            ❌ FAIL         162.48M      0.000s\n",
      "openseed             ✅ PASS         158.62M      0.025s\n",
      "odise                ✅ PASS         162.53M      0.046s\n",
      "tagalign             ❌ FAIL         151.93M      0.000s\n",
      "semantic_sam         ✅ PASS         155.40M      0.026s\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Failed models:\n",
      "  - lseg_vit_l: mat1 and mat2 shapes cannot be multiplied (648x2048 and 1536x768)\n",
      "  - maft_plus: mat1 and mat2 shapes cannot be multiplied (131072x512 and 768x512)\n",
      "  - x_decoder: The size of tensor a (256) must match the size of tensor b (65536) at non-singleton dimension 1\n",
      "  - tagalign: Sizes of tensors must match except in dimension 1. Expected size 15 but got size 16 for tensor number 1 in the list.\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "passed = [r for r in results if r['status'] == 'PASSED']\n",
    "failed = [r for r in results if r['status'] == 'FAILED']\n",
    "\n",
    "print(f\"\\n✅ Passed: {len(passed)}/{len(results)}\")\n",
    "print(f\"❌ Failed: {len(failed)}/{len(results)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"{'Model':<20} {'Status':<10} {'Params (M)':<12} {'Time (s)':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for r in results:\n",
    "    params_m = r['num_params'] / 1e6 if r['num_params'] else 0\n",
    "    time_s = r['inference_time'] if r['inference_time'] else 0\n",
    "    status = \"✅ PASS\" if r['status'] == 'PASSED' else \"❌ FAIL\"\n",
    "    print(f\"{r['model_name']:<20} {status:<10} {params_m:>10.2f}M {time_s:>10.3f}s\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "if failed:\n",
    "    print(\"\\nFailed models:\")\n",
    "    for r in failed:\n",
    "        print(f\"  - {r['model_name']}: {r['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73850915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a single model in detail (for debugging)\n",
    "# Change MODEL_TO_TEST to test a specific model\n",
    "\n",
    "MODEL_TO_TEST = \"clipseg\"  # Change this to test different models\n",
    "\n",
    "print(f\"\\nDetailed test for: {MODEL_TO_TEST}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = get_model(\n",
    "    MODEL_TO_TEST,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(dummy_images, TEXT_PROMPTS)\n",
    "\n",
    "print(\"\\nOutput keys:\", outputs.keys())\n",
    "for key, value in outputs.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7755e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = get_model(\"clipseg\", num_classes=NUM_CLASSES, image_size=IMAGE_SIZE, device=DEVICE)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(dummy_images, TEXT_PROMPTS)\n",
    "\n",
    "logits = outputs['logits']\n",
    "pred_mask = logits.argmax(dim=1)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(BATCH_SIZE, 3, figsize=(12, 4*BATCH_SIZE))\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Input image (denormalized for visualization)\n",
    "    img = dummy_images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0, 1]\n",
    "    \n",
    "    # Prediction\n",
    "    pred = pred_mask[i].cpu().numpy()\n",
    "    \n",
    "    # Logits heatmap for first class\n",
    "    heatmap = logits[i, 0].cpu().numpy()\n",
    "    \n",
    "    if BATCH_SIZE > 1:\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title(f'Input {i}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(pred, cmap='tab10', vmin=0, vmax=NUM_CLASSES-1)\n",
    "        axes[i, 1].set_title(f'Prediction {i}')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        im = axes[i, 2].imshow(heatmap, cmap='hot')\n",
    "        axes[i, 2].set_title(f'Neoplastic logits {i}')\n",
    "        axes[i, 2].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i, 2])\n",
    "    else:\n",
    "        axes[0].imshow(img)\n",
    "        axes[0].set_title('Input')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(pred, cmap='tab10', vmin=0, vmax=NUM_CLASSES-1)\n",
    "        axes[1].set_title('Prediction')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        im = axes[2].imshow(heatmap, cmap='hot')\n",
    "        axes[2].set_title('Neoplastic logits')\n",
    "        axes[2].axis('off')\n",
    "        plt.colorbar(im, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b846b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage comparison\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nGPU Memory Usage Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    memory_results = []\n",
    "    \n",
    "    for model_name in all_models:\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            model = get_model(model_name, num_classes=NUM_CLASSES, image_size=IMAGE_SIZE, device=DEVICE)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                _ = model(dummy_images, TEXT_PROMPTS)\n",
    "            \n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "            memory_results.append((model_name, peak_memory))\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            memory_results.append((model_name, -1))\n",
    "    \n",
    "    # Sort by memory\n",
    "    memory_results.sort(key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\n{'Model':<20} {'Peak Memory (GB)':<15}\")\n",
    "    print(\"-\"*35)\n",
    "    for name, mem in memory_results:\n",
    "        if mem >= 0:\n",
    "            print(f\"{name:<20} {mem:>10.2f} GB\")\n",
    "        else:\n",
    "            print(f\"{name:<20} {'FAILED':>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfde0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All tests completed!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTo use these models in training, see: train_text_guided_unified.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
